{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6466ff74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "054c5671",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f014cd21",
   "metadata": {},
   "source": [
    "Of course. This is the perfect time to architect a scalable and robust pipeline. Moving from a single-table proof-of-concept to a multi-document, multi-table system requires careful planning around data flow, context management, and graph modeling.\n",
    "\n",
    "Here is a robust, phased plan of action that addresses your requirements. We will focus on solving the multi-table/multi-document problem first, laying a strong foundation for integrating unstructured text later.\n",
    "\n",
    "---\n",
    "\n",
    "### **Phase 1: The Scalable Multi-Document & Multi-Table Pipeline**\n",
    "\n",
    "The goal of this phase is to process a directory of PDF documents, extract all tables from each, convert them into a single, unified knowledge graph in Neo4j, and critically, **preserve the origin (provenance)** of every piece of data.\n",
    "\n",
    "#### **Plan of Action**\n",
    "\n",
    "**Step 1: Design a Hierarchical Data Model for Provenance**\n",
    "\n",
    "Before writing any code, we must update our graph model. We'll introduce `Document` and `Table` nodes to track where every fact comes from.\n",
    "\n",
    "*   **`(:Document)` Node**: Represents a single PDF file.\n",
    "    *   **ID**: A unique identifier, e.g., a hash of the filename or the filename itself (`doc_id`).\n",
    "    *   **Properties**: `{filePath: \"...\", fileName: \"...\", url: \"...\", processedAt: datetime()}`\n",
    "*   **`(:Table)` Node**: Represents a single table within a document.\n",
    "    *   **ID**: A composite ID to ensure uniqueness, e.g., `{doc_id}_table_{table_index}`.\n",
    "    *   **Properties**: `{title: \"...\", pageNumber: 1, tableIndex: 0}`\n",
    "*   **`(:FinancialMetric)` Node**: Your existing metric nodes (e.g., \"Total Operating Revenues\").\n",
    "    *   **ID**: A normalized version of the metric's label (e.g., `total_operating_revenue`).\n",
    "*   **Relationships to Connect the Hierarchy**:\n",
    "    *   `(:Document)-[:CONTAINS_TABLE]->(:Table)`\n",
    "    *   `(:Table)-[:REPORTS_ON]->(:FinancialMetric)`\n",
    "    *   `(:FinancialMetric)-[r:HAS_VALUE_FOR_PERIOD]->(:TimePeriod)` (with properties like `amount` on the relationship `r`).\n",
    "\n",
    "This model allows you to query your graph for data provenance, e.g., \"Show me the exact table and document where this financial figure originated.\"\n",
    "\n",
    "**Step 2: Implement the Main Processing Loop**\n",
    "\n",
    "Your main script will orchestrate the entire pipeline. It will no longer work on a single hardcoded file but on a directory.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f362bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Assume you have these classes/functions defined from previous steps\n",
    "# from your_code import PDFProcessor, TableReconstructionAgent, GraphExtractor, Neo4jGraphConstructor\n",
    "\n",
    "# --- Configuration ---\n",
    "PDF_DIRECTORY = Path(\"i:/My Drive/M. Tech AI ML/AIML SEM 4/Dissertation/Project/downloaded_verizon_financial_pdfs/downloaded_verizon_quarterly_pdfs/2025/1Q\")\n",
    "NEO4J_URI = \"bolt://localhost:7687\"\n",
    "NEO4J_USER = \"neo4j\"\n",
    "NEO4J_PASSWORD = \"your_password\"\n",
    "\n",
    "# --- Initialization ---\n",
    "pdf_processor = PDFProcessor()\n",
    "table_agent = TableReconstructionAgent()\n",
    "graph_extractor = GraphExtractor()\n",
    "graph_constructor = Neo4jGraphConstructor(NEO4J_URI, NEO4J_USER, NEO4J_PASSWORD)\n",
    "\n",
    "# --- Main Loop ---\n",
    "def process_all_documents():\n",
    "    pdf_files = list(PDF_DIRECTORY.glob(\"*.pdf\"))\n",
    "    print(f\"Found {len(pdf_files)} PDF documents to process.\")\n",
    "\n",
    "    for pdf_path in pdf_files:\n",
    "        doc_id = pdf_path.stem # e.g., \"Download Financial statements PDF_5a9b6fa4\"\n",
    "        print(f\"\\n--- Processing Document: {doc_id} ---\")\n",
    "        \n",
    "        # Add document node to the graph\n",
    "        graph_constructor.add_document_node(doc_id, str(pdf_path))\n",
    "\n",
    "        # 1. Parse the PDF to get tables and text\n",
    "        docling_data = pdf_processor.parse(pdf_path)\n",
    "        \n",
    "        # 2. Loop through each table found in the document\n",
    "        for table_index, table_data in enumerate(docling_data.get('tables', [])):\n",
    "            table_id = f\"{doc_id}_table_{table_index}\"\n",
    "            print(f\"  -> Processing Table #{table_index}\")\n",
    "\n",
    "            # Add table node and link it to the document\n",
    "            graph_constructor.add_table_node(table_id, doc_id, table_data.get('metadata'))\n",
    "\n",
    "            # 3. Reconstruct the table into markdown (your existing logic)\n",
    "            reconstructed_table = table_agent.reconstruct_table(table_data)\n",
    "            \n",
    "            # 4. Extract graph data (nodes and relationships) from the markdown\n",
    "            # This now needs the document and table context\n",
    "            graph_json = graph_extractor.extract_graph_from_table(\n",
    "                reconstructed_table, \n",
    "                doc_id, \n",
    "                table_id\n",
    "            )\n",
    "            \n",
    "            # 5. Add the extracted data to Neo4j\n",
    "            graph_constructor.add_data_to_graph(graph_json, table_id)\n",
    "\n",
    "    graph_constructor.close()\n",
    "    print(\"\\n✅ Pipeline finished. All documents processed.\")\n",
    "\n",
    "# process_all_documents()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf68b8e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Step 3: Update the Graph Extraction Prompt**\n",
    "\n",
    "This is a critical change. The prompt must now receive the `doc_id` and `table_id` and be instructed to use them.\n",
    "\n",
    "**New `graph_prompt` Template:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd94772",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"You are an expert financial analyst AI creating a knowledge graph.\n",
    "The input contains a markdown table extracted from a specific table within a source document.\n",
    "\n",
    "**CONTEXT:**\n",
    "- **Document ID**: `{doc_id}`\n",
    "- **Table ID**: `{table_id}`\n",
    "- **Financial Context**: `{financial_context}`\n",
    "- **Markdown Table**: `{markdown_table}`\n",
    "\n",
    "**YOUR TASK:**\n",
    "Create a JSON object with \"nodes\" and \"relationships\" representing the information.\n",
    "1.  Create a `Document` node with the id `{doc_id}`.\n",
    "2.  Create a `Table` node with the id `{table_id}`.\n",
    "3.  For each financial metric (e.g., \"Total Operating Revenues\"), create a `FinancialMetric` node.\n",
    "4.  Create a relationship from the `Document` node to the `Table` node.\n",
    "5.  Create a relationship from the `Table` node to each `FinancialMetric` node it contains.\n",
    "6.  Extract all financial values as relationships between `FinancialMetric` and `TimePeriod` nodes, as previously instructed.\n",
    "\n",
    "**Output MUST be a single, clean JSON object.**\n",
    "\n",
    "---\n",
    "TEXT TO ANALYZE:\n",
    "{input} \n",
    "---\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70c399a",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Step 4: Create a `Neo4jGraphConstructor` Class**\n",
    "\n",
    "This class will abstract away the Cypher queries, making your main loop clean and readable. It will use idempotent `MERGE` queries to prevent data duplication on re-runs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f64bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "class Neo4jGraphConstructor:\n",
    "    def __init__(self, uri, user, password):\n",
    "        self.driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "    def close(self):\n",
    "        self.driver.close()\n",
    "\n",
    "    def execute_query(self, query, parameters=None):\n",
    "        with self.driver.session() as session:\n",
    "            return session.run(query, parameters)\n",
    "\n",
    "    def add_document_node(self, doc_id, file_path):\n",
    "        query = \"\"\"\n",
    "        MERGE (d:Document {id: $doc_id})\n",
    "        ON CREATE SET d.filePath = $file_path, d.processedAt = datetime()\n",
    "        \"\"\"\n",
    "        self.execute_query(query, {'doc_id': doc_id, 'file_path': file_path})\n",
    "\n",
    "    def add_table_node(self, table_id, doc_id, metadata):\n",
    "        query = \"\"\"\n",
    "        MERGE (t:Table {id: $table_id})\n",
    "        ON CREATE SET t.title = $title, t.page = $page\n",
    "        WITH t\n",
    "        MATCH (d:Document {id: $doc_id})\n",
    "        MERGE (d)-[:CONTAINS_TABLE]->(t)\n",
    "        \"\"\"\n",
    "        self.execute_query(query, {\n",
    "            'table_id': table_id,\n",
    "            'doc_id': doc_id,\n",
    "            'title': metadata.get('title'),\n",
    "            'page': metadata.get('page')\n",
    "        })\n",
    "\n",
    "    def add_data_to_graph(self, graph_json, table_id):\n",
    "        # This method will contain loops to iterate through nodes and relationships\n",
    "        # from the LLM's output and generate the appropriate MERGE queries.\n",
    "        # It will link FinancialMetric nodes to the current table_id.\n",
    "        pass # Implementation would follow here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90181778",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **Phase 2: Integrating Unstructured Text (The Next Step)**\n",
    "\n",
    "Once Phase 1 is working reliably, integrating text is straightforward because the foundation is already built.\n",
    "\n",
    "1.  **Add a Text Extraction Step**: In your main loop, after processing tables, you'll process the text fragments from `docling_data.get('body', [])`.\n",
    "2.  **Use a General Extraction Prompt**: Create a new prompt for unstructured text. This prompt will be more general, asking the LLM to identify any entities (`Person`, `Organization`, `Location`, `Product`) and their relationships.\n",
    "3.  **Feed to the Same Constructor**: The nodes and relationships extracted from the text will be fed into the *same* `graph_constructor.add_data_to_graph()` method. The power of `MERGE` in Neo4j will automatically link new information to existing nodes. For example, if the text mentions \"Verizon\" and a table created a `(:Company {name: \"Verizon\"})` node, Neo4j will merge them, enriching the existing node with new relationships from the text.\n",
    "\n",
    "This plan provides a clear, step-by-step path to building the robust, multi-source knowledge graph you envision.\n",
    "\n",
    "Similar code found with 1 license type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3889466",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5359a7f4",
   "metadata": {},
   "source": [
    "##### 1) Ingest & Parse (pyMuPDF4LLM + Docling), preserve provenance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3dd311",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, json, uuid\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "# We only need Docling for this refined pipeline\n",
    "try:\n",
    "    from docling.document_converter import DocumentConverter\n",
    "    DOCLING = True\n",
    "except ImportError:\n",
    "    print(\"⚠️ Docling library not found. Please install it to proceed.\")\n",
    "    DocumentConverter = None\n",
    "    DOCLING = False\n",
    "\n",
    "@dataclass\n",
    "class IngestConfig:\n",
    "    # The root output directory\n",
    "    out_dir: str = \"output/parsed_pdfs\"\n",
    "\n",
    "# --- UTILITY FUNCTIONS ---\n",
    "def _ensure_dir(p: str):\n",
    "    \"\"\"Ensures that a directory exists, creating it if necessary.\"\"\"\n",
    "    os.makedirs(p, exist_ok=True)\n",
    "\n",
    "def _slug(s: str) -> str:\n",
    "    \"\"\"Creates a clean, URL-friendly slug from a file path.\"\"\"\n",
    "    basename = os.path.splitext(os.path.basename(s))[0]\n",
    "    return re.sub(r\"[^A-Za-z0-9]+\", \"-\", basename).strip(\"-\").lower()\n",
    "\n",
    "# --- CORE EXTRACTION LOGIC ---\n",
    "def extract_with_docling(pdf_path: str) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Extracts structured JSON from a PDF using Docling.\n",
    "    Returns the JSON data or None if extraction fails.\n",
    "    \"\"\"\n",
    "    if not DOCLING:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        conv = DocumentConverter()\n",
    "        res = conv.convert(pdf_path)\n",
    "        doc = getattr(res, \"document\", None) or res\n",
    "        \n",
    "        # Prioritize dictionary methods, then fall back to JSON string methods\n",
    "        for method_name in (\"to_dict\", \"export_to_dict\", \"as_dict\"):\n",
    "            if hasattr(doc, method_name):\n",
    "                return getattr(doc, method_name)()\n",
    "        \n",
    "        for method_name in (\"to_json\", \"export_to_json\"):\n",
    "            if hasattr(doc, method_name):\n",
    "                return json.loads(getattr(doc, method_name)())\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"  ❌ Error processing with Docling: {e}\")\n",
    "        return None\n",
    "    \n",
    "    return None\n",
    "\n",
    "# --- REFINED INGESTION PIPELINE ---\n",
    "def run_ingest_refined(pdf_path: str, root_input_dir: str, cfg: IngestConfig) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    A simplified pipeline that extracts Docling JSON and saves it in a mirrored directory structure.\n",
    "    \"\"\"\n",
    "    doc_id = _slug(pdf_path)\n",
    "    \n",
    "    # Determine the relative path from the root input to the PDF's directory\n",
    "    relative_dir = os.path.dirname(os.path.relpath(pdf_path, root_input_dir))\n",
    "    \n",
    "    # Create the mirrored output directory structure\n",
    "    # e.g., output/parsed_pdfs/2025/1Q/download-financial-statements-pdf-5a9b6fa4/\n",
    "    final_out_dir = os.path.join(cfg.out_dir, relative_dir, doc_id)\n",
    "    \n",
    "    # --- Caching Check ---\n",
    "    summary_path = os.path.join(final_out_dir, f\"{doc_id}.summary.json\")\n",
    "    if os.path.exists(summary_path):\n",
    "        print(f\"SKIP (already processed): {doc_id}\")\n",
    "        with open(summary_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            return json.load(f)\n",
    "    \n",
    "    print(f\"  -> Processing with Docling...\")\n",
    "    _ensure_dir(final_out_dir)\n",
    "    \n",
    "    # --- Extraction ---\n",
    "    docling_json = extract_with_docling(pdf_path)\n",
    "    \n",
    "    if not docling_json:\n",
    "        print(f\"  ❌ Failed to extract Docling JSON for {doc_id}.\")\n",
    "        return None\n",
    "        \n",
    "    # --- Save Artifacts ---\n",
    "    docl_json_path = os.path.join(final_out_dir, f\"{doc_id}.docling.json\")\n",
    "    with open(docl_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(docling_json, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"  ✅ Saved Docling JSON to: {docl_json_path}\")\n",
    "\n",
    "    # --- Create Final Summary ---\n",
    "    summary = {\n",
    "        \"doc_id\": doc_id,\n",
    "        \"pdf_path\": pdf_path,\n",
    "        \"out_dir\": final_out_dir,\n",
    "        \"artifacts\": {\n",
    "            \"docling_json\": docl_json_path,\n",
    "        },\n",
    "        \"stats\": {\n",
    "            \"docling_available\": True\n",
    "        }\n",
    "    }\n",
    "    with open(summary_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(summary, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79ae22ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Searching for all PDF files in: i:\\My Drive\\M. Tech AI ML\\AIML SEM 4\\Dissertation\\Project\\downloaded_verizon_financial_pdfs\\downloaded_verizon_quarterly_pdfs\n",
      "  -> Found 209 total PDF files.\n",
      "🎯 Filtering for years: ['2024', '2025']\n",
      "  -> Found 44 files to process from the specified years.\n",
      "\n",
      "Processing Ingest for: Download Webcast Transcript PDF_0773ea78.pdf\n",
      "SKIP (already processed): download-webcast-transcript-pdf-0773ea78\n",
      "  ✅ Ingest successful for: download-webcast-transcript-pdf-0773ea78\n",
      "\n",
      "Processing Ingest for: Download Formal Remarks PDF_e2835efd.pdf\n",
      "SKIP (already processed): download-formal-remarks-pdf-e2835efd\n",
      "  ✅ Ingest successful for: download-formal-remarks-pdf-e2835efd\n",
      "\n",
      "Processing Ingest for: Download Webcast Presentation PDF_a1b3e2e1.pdf\n",
      "SKIP (already processed): download-webcast-presentation-pdf-a1b3e2e1\n",
      "  ✅ Ingest successful for: download-webcast-presentation-pdf-a1b3e2e1\n",
      "\n",
      "Processing Ingest for: Download Infographic PDF_88d85bda.pdf\n",
      "SKIP (already processed): download-infographic-pdf-88d85bda\n",
      "  ✅ Ingest successful for: download-infographic-pdf-88d85bda\n",
      "\n",
      "Processing Ingest for: Download Financial statements PDF_ad7baa45.pdf\n",
      "SKIP (already processed): download-financial-statements-pdf-ad7baa45\n",
      "  ✅ Ingest successful for: download-financial-statements-pdf-ad7baa45\n",
      "\n",
      "Processing Ingest for: Download Financial & Operating information PDF_8c62471e.pdf\n",
      "SKIP (already processed): download-financial-operating-information-pdf-8c62471e\n",
      "  ✅ Ingest successful for: download-financial-operating-information-pdf-8c62471e\n",
      "\n",
      "Processing Ingest for: Download Non-GAAP reconciliations PDF_9669cf61.pdf\n",
      "SKIP (already processed): download-non-gaap-reconciliations-pdf-9669cf61\n",
      "  ✅ Ingest successful for: download-non-gaap-reconciliations-pdf-9669cf61\n",
      "\n",
      "Processing Ingest for: Download Webcast Transcript PDF_2283b88e.pdf\n",
      "SKIP (already processed): download-webcast-transcript-pdf-2283b88e\n",
      "  ✅ Ingest successful for: download-webcast-transcript-pdf-2283b88e\n",
      "\n",
      "Processing Ingest for: Download Formal Remarks PDF_4c40b0bf.pdf\n",
      "SKIP (already processed): download-formal-remarks-pdf-4c40b0bf\n",
      "  ✅ Ingest successful for: download-formal-remarks-pdf-4c40b0bf\n",
      "\n",
      "Processing Ingest for: Download Webcast Presentation PDF_ceaa87f0.pdf\n",
      "SKIP (already processed): download-webcast-presentation-pdf-ceaa87f0\n",
      "  ✅ Ingest successful for: download-webcast-presentation-pdf-ceaa87f0\n",
      "\n",
      "Processing Ingest for: Download Infographic PDF_733d33e1.pdf\n",
      "SKIP (already processed): download-infographic-pdf-733d33e1\n",
      "  ✅ Ingest successful for: download-infographic-pdf-733d33e1\n",
      "\n",
      "Processing Ingest for: Download Financial statements PDF_de2bf44c.pdf\n",
      "SKIP (already processed): download-financial-statements-pdf-de2bf44c\n",
      "  ✅ Ingest successful for: download-financial-statements-pdf-de2bf44c\n",
      "\n",
      "Processing Ingest for: Download Financial & Operating information PDF_56695261.pdf\n",
      "SKIP (already processed): download-financial-operating-information-pdf-56695261\n",
      "  ✅ Ingest successful for: download-financial-operating-information-pdf-56695261\n",
      "\n",
      "Processing Ingest for: Download Non-GAAP reconciliations PDF_06cc31ee.pdf\n",
      "SKIP (already processed): download-non-gaap-reconciliations-pdf-06cc31ee\n",
      "  ✅ Ingest successful for: download-non-gaap-reconciliations-pdf-06cc31ee\n",
      "\n",
      "Processing Ingest for: Download Webcast Transcript PDF_4d61943a.pdf\n",
      "SKIP (already processed): download-webcast-transcript-pdf-4d61943a\n",
      "  ✅ Ingest successful for: download-webcast-transcript-pdf-4d61943a\n",
      "\n",
      "Processing Ingest for: Download Formal Remarks PDF_3f10e07d.pdf\n",
      "SKIP (already processed): download-formal-remarks-pdf-3f10e07d\n",
      "  ✅ Ingest successful for: download-formal-remarks-pdf-3f10e07d\n",
      "\n",
      "Processing Ingest for: Download Presentation PDF_876a8141.pdf\n",
      "SKIP (already processed): download-presentation-pdf-876a8141\n",
      "  ✅ Ingest successful for: download-presentation-pdf-876a8141\n",
      "\n",
      "Processing Ingest for: Download Infographic PDF_20100392.pdf\n",
      "SKIP (already processed): download-infographic-pdf-20100392\n",
      "  ✅ Ingest successful for: download-infographic-pdf-20100392\n",
      "\n",
      "Processing Ingest for: Download Financial statements PDF_23b3bcbf.pdf\n",
      "SKIP (already processed): download-financial-statements-pdf-23b3bcbf\n",
      "  ✅ Ingest successful for: download-financial-statements-pdf-23b3bcbf\n",
      "\n",
      "Processing Ingest for: Download Financial & Operating information PDF_548e5fff.pdf\n",
      "SKIP (already processed): download-financial-operating-information-pdf-548e5fff\n",
      "  ✅ Ingest successful for: download-financial-operating-information-pdf-548e5fff\n",
      "\n",
      "Processing Ingest for: Download Non-GAAP reconciliations PDF_26969289.pdf\n",
      "SKIP (already processed): download-non-gaap-reconciliations-pdf-26969289\n",
      "  ✅ Ingest successful for: download-non-gaap-reconciliations-pdf-26969289\n",
      "\n",
      "Processing Ingest for: Download Webcast Transcript PDF_791d532a.pdf\n",
      "SKIP (already processed): download-webcast-transcript-pdf-791d532a\n",
      "  ✅ Ingest successful for: download-webcast-transcript-pdf-791d532a\n",
      "\n",
      "Processing Ingest for: Download Webcast Presentation PDF_cf490e3b.pdf\n",
      "SKIP (already processed): download-webcast-presentation-pdf-cf490e3b\n",
      "  ✅ Ingest successful for: download-webcast-presentation-pdf-cf490e3b\n",
      "\n",
      "Processing Ingest for: Download Earnings Presentation PDF_828709d1.pdf\n",
      "SKIP (already processed): download-earnings-presentation-pdf-828709d1\n",
      "  ✅ Ingest successful for: download-earnings-presentation-pdf-828709d1\n",
      "\n",
      "Processing Ingest for: Download Prepared Earnings Transcript PDF_daaf83b5.pdf\n",
      "SKIP (already processed): download-prepared-earnings-transcript-pdf-daaf83b5\n",
      "  ✅ Ingest successful for: download-prepared-earnings-transcript-pdf-daaf83b5\n",
      "\n",
      "Processing Ingest for: Download Formal Remarks PDF_1a7008fb.pdf\n",
      "SKIP (already processed): download-formal-remarks-pdf-1a7008fb\n",
      "  ✅ Ingest successful for: download-formal-remarks-pdf-1a7008fb\n",
      "\n",
      "Processing Ingest for: Download Financial statements PDF_56712f58.pdf\n",
      "SKIP (already processed): download-financial-statements-pdf-56712f58\n",
      "  ✅ Ingest successful for: download-financial-statements-pdf-56712f58\n",
      "\n",
      "Processing Ingest for: Download Financial & Operating information PDF_2f954363.pdf\n",
      "SKIP (already processed): download-financial-operating-information-pdf-2f954363\n",
      "  ✅ Ingest successful for: download-financial-operating-information-pdf-2f954363\n",
      "\n",
      "Processing Ingest for: Download Non-GAAP reconciliations PDF_a98978fa.pdf\n",
      "SKIP (already processed): download-non-gaap-reconciliations-pdf-a98978fa\n",
      "  ✅ Ingest successful for: download-non-gaap-reconciliations-pdf-a98978fa\n",
      "\n",
      "Processing Ingest for: Download Non-GAAP reconciliations PDF_de6a19a8.pdf\n",
      "SKIP (already processed): download-non-gaap-reconciliations-pdf-de6a19a8\n",
      "  ✅ Ingest successful for: download-non-gaap-reconciliations-pdf-de6a19a8\n",
      "\n",
      "Processing Ingest for: Download Webcast Transcript PDF_f011dc37.pdf\n",
      "SKIP (already processed): download-webcast-transcript-pdf-f011dc37\n",
      "  ✅ Ingest successful for: download-webcast-transcript-pdf-f011dc37\n",
      "\n",
      "Processing Ingest for: Download Webcast Presentation PDF_da5ae256.pdf\n",
      "SKIP (already processed): download-webcast-presentation-pdf-da5ae256\n",
      "  ✅ Ingest successful for: download-webcast-presentation-pdf-da5ae256\n",
      "\n",
      "Processing Ingest for: Download Earnings Presentation PDF_be525a5d.pdf\n",
      "SKIP (already processed): download-earnings-presentation-pdf-be525a5d\n",
      "  ✅ Ingest successful for: download-earnings-presentation-pdf-be525a5d\n",
      "\n",
      "Processing Ingest for: Download Prepared Earnings Transcript PDF_15df53b4.pdf\n",
      "SKIP (already processed): download-prepared-earnings-transcript-pdf-15df53b4\n",
      "  ✅ Ingest successful for: download-prepared-earnings-transcript-pdf-15df53b4\n",
      "\n",
      "Processing Ingest for: Download Infographic PDF_30c3d8e4.pdf\n",
      "SKIP (already processed): download-infographic-pdf-30c3d8e4\n",
      "  ✅ Ingest successful for: download-infographic-pdf-30c3d8e4\n",
      "\n",
      "Processing Ingest for: Download Financial statements PDF_5a9b6fa4.pdf\n",
      "SKIP (already processed): download-financial-statements-pdf-5a9b6fa4\n",
      "  ✅ Ingest successful for: download-financial-statements-pdf-5a9b6fa4\n",
      "\n",
      "Processing Ingest for: Download Financial & Operating information PDF_3dedf514.pdf\n",
      "SKIP (already processed): download-financial-operating-information-pdf-3dedf514\n",
      "  ✅ Ingest successful for: download-financial-operating-information-pdf-3dedf514\n",
      "\n",
      "Processing Ingest for: Download Non-GAAP reconciliations PDF_d419afb0.pdf\n",
      "SKIP (already processed): download-non-gaap-reconciliations-pdf-d419afb0\n",
      "  ✅ Ingest successful for: download-non-gaap-reconciliations-pdf-d419afb0\n",
      "\n",
      "Processing Ingest for: Download Webcast Transcript PDF_c1bdcc0f.pdf\n",
      "SKIP (already processed): download-webcast-transcript-pdf-c1bdcc0f\n",
      "  ✅ Ingest successful for: download-webcast-transcript-pdf-c1bdcc0f\n",
      "\n",
      "Processing Ingest for: Download Presentation PDF_90cbcc8c.pdf\n",
      "SKIP (already processed): download-presentation-pdf-90cbcc8c\n",
      "  ✅ Ingest successful for: download-presentation-pdf-90cbcc8c\n",
      "\n",
      "Processing Ingest for: Download Infographic PDF_5450fea0.pdf\n",
      "SKIP (already processed): download-infographic-pdf-5450fea0\n",
      "  ✅ Ingest successful for: download-infographic-pdf-5450fea0\n",
      "\n",
      "Processing Ingest for: Download Financial statements PDF_7b4b0746.pdf\n",
      "SKIP (already processed): download-financial-statements-pdf-7b4b0746\n",
      "  ✅ Ingest successful for: download-financial-statements-pdf-7b4b0746\n",
      "\n",
      "Processing Ingest for: Download Financial & Operating information PDF_e1c5ab85.pdf\n",
      "SKIP (already processed): download-financial-operating-information-pdf-e1c5ab85\n",
      "  ✅ Ingest successful for: download-financial-operating-information-pdf-e1c5ab85\n",
      "\n",
      "Processing Ingest for: Download Non-GAAP reconciliations PDF_eb924252.pdf\n",
      "SKIP (already processed): download-non-gaap-reconciliations-pdf-eb924252\n",
      "  ✅ Ingest successful for: download-non-gaap-reconciliations-pdf-eb924252\n",
      "\n",
      "\n",
      "--- PIPELINE COMPLETE ---\n",
      "Successfully processed 44 out of 44 documents.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# --- Configuration ---\n",
    "ROOT = r\"i:\\My Drive\\M. Tech AI ML\\AIML SEM 4\\Dissertation\\Project\"\n",
    "# Define the base directory for all input PDFs. This is used to create the mirrored output structure.\n",
    "ROOT_INPUT_DIR = os.path.join(ROOT, \"downloaded_verizon_financial_pdfs\")\n",
    "# Define the single output directory for all parsed results.\n",
    "OUT_DIR = os.path.join(ROOT, \"output\", \"parsed_pdfs\")\n",
    "\n",
    "# --- Dynamic File Discovery ---\n",
    "# The target directory to search for all quarterly reports.\n",
    "QUARTERLY_REPORTS_DIR = os.path.join(ROOT_INPUT_DIR, \"downloaded_verizon_quarterly_pdfs\")\n",
    "\n",
    "print(f\"🔍 Searching for all PDF files in: {QUARTERLY_REPORTS_DIR}\")\n",
    "# Use glob to recursively find all files ending with .pdf\n",
    "all_pdf_files = glob.glob(os.path.join(QUARTERLY_REPORTS_DIR, '**', '*.pdf'), recursive=True)\n",
    "print(f\"  -> Found {len(all_pdf_files)} total PDF files.\")\n",
    "\n",
    "# --- Filter by Year ---\n",
    "# allowed_years = {'2023', '2024', '2025'}\n",
    "allowed_years = {'2024', '2025'}\n",
    "print(f\"🎯 Filtering for years: {sorted(list(allowed_years))}\")\n",
    "\n",
    "filtered_pdf_files = []\n",
    "for path in all_pdf_files:\n",
    "    # Check if any part of the path is one of the allowed years\n",
    "    path_parts = set(path.split(os.path.sep))\n",
    "    if not allowed_years.isdisjoint(path_parts):\n",
    "        filtered_pdf_files.append(path)\n",
    "\n",
    "print(f\"  -> Found {len(filtered_pdf_files)} files to process from the specified years.\")\n",
    "\n",
    "\n",
    "# --- Run the Ingestion Pipeline ---\n",
    "# The config is now much simpler.\n",
    "cfg = IngestConfig(out_dir=OUT_DIR)\n",
    "\n",
    "all_results = []\n",
    "# Loop through the dynamically discovered and filtered list of files.\n",
    "for pdf_path in filtered_pdf_files:\n",
    "    if not os.path.exists(pdf_path):\n",
    "        print(f\"⚠️  File not found, skipping: {pdf_path}\")\n",
    "        continue\n",
    "        \n",
    "    print(f\"\\nProcessing Ingest for: {os.path.basename(pdf_path)}\")\n",
    "    # Call the refined function with the root input directory\n",
    "    result = run_ingest_refined(pdf_path, ROOT_INPUT_DIR, cfg)\n",
    "    \n",
    "    if result:\n",
    "        print(f\"  ✅ Ingest successful for: {result['doc_id']}\")\n",
    "        all_results.append(result)\n",
    "    else:\n",
    "        print(f\"  ❌ Ingest failed for: {pdf_path}\")\n",
    "\n",
    "print(f\"\\n\\n--- PIPELINE COMPLETE ---\")\n",
    "print(f\"Successfully processed {len(all_results)} out of {len(filtered_pdf_files)} documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465c3a7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88777728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final corrected TableReconstructionAgent with rate limit handling\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import json\n",
    "import re\n",
    "import time  # Import the time module\n",
    "import random # Import the random module\n",
    "from typing import Dict, Any, List, Optional\n",
    "\n",
    "def save_prepared_data(prepared_data: Dict[str, Any], ingest_summary: Dict[str, Any]) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Saves the prepared document data (with reconstructed tables and text chunks)\n",
    "    to a JSON file in the document's specific output directory.\n",
    "    \"\"\"\n",
    "    doc_id = prepared_data.get(\"doc_id\")\n",
    "    out_dir = ingest_summary.get(\"out_dir\")\n",
    "\n",
    "    if not doc_id or not out_dir:\n",
    "        print(\"❌ ERROR: Cannot save prepared data. Missing 'doc_id' or 'out_dir'.\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # Ensure the output directory from the ingest summary exists\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "        \n",
    "        # Define the output path for the prepared data JSON file\n",
    "        output_path = os.path.join(out_dir, f\"{doc_id}.prepared.json\")\n",
    "        \n",
    "        # Write the dictionary to the JSON file\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(prepared_data, f, ensure_ascii=False, indent=2)\n",
    "            \n",
    "        print(f\"  -> ✅ Saved prepared data to: {output_path}\")\n",
    "        return output_path\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  -> ❌ ERROR: Failed to save prepared data for doc_id '{doc_id}'. Reason: {e}\")\n",
    "        return None\n",
    "\n",
    "class DocumentProcessor:\n",
    "    \"\"\"\n",
    "    Orchestrates the processing of a single document's parsed data.\n",
    "    *** This version is hardened against empty 'prov' lists. ***\n",
    "    \"\"\"\n",
    "    def __init__(self, table_agent: \"TableReconstructionAgent\"):\n",
    "        self.table_agent = table_agent\n",
    "        print(\"✅ DocumentProcessor initialized.\")\n",
    "\n",
    "    def process_document(self, ingest_summary: Dict[str, Any]) -> Optional[Dict[str, Any]]:\n",
    "        doc_id = ingest_summary.get(\"doc_id\")\n",
    "        docling_json_path = ingest_summary.get(\"artifacts\", {}).get(\"docling_json\")\n",
    "\n",
    "        if not doc_id or not docling_json_path or not os.path.exists(docling_json_path):\n",
    "            print(f\"❌ ERROR: Invalid ingest summary for doc_id '{doc_id}'.\")\n",
    "            return None\n",
    "\n",
    "        print(f\"\\n--- Preparing Document for Graph Extraction: {doc_id} ---\")\n",
    "        \n",
    "        with open(docling_json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            docling_data = json.load(f)\n",
    "\n",
    "        # 1. Process tables using 'prov' for metadata\n",
    "        reconstructed_tables = []\n",
    "        raw_tables = docling_data.get(\"tables\", [])\n",
    "        if raw_tables:\n",
    "            print(f\"  -> Found {len(raw_tables)} tables to process.\")\n",
    "            for i, raw_table in enumerate(raw_tables):\n",
    "                page_no = \"N/A\"\n",
    "                prov_list = raw_table.get(\"prov\", [])\n",
    "                # *** HARDENING STEP ***: Check if the list is not empty before accessing it.\n",
    "                if prov_list and isinstance(prov_list[0], dict):\n",
    "                    page_no = prov_list[0].get(\"page_no\", \"N/A\")\n",
    "                \n",
    "                table_id = f\"{doc_id}_p{page_no}_t{i}\"\n",
    "                \n",
    "                reconstructed_result = self.table_agent.reconstruct_table(raw_table, i)\n",
    "                reconstructed_result[\"table_id\"] = table_id\n",
    "                reconstructed_tables.append(reconstructed_result)\n",
    "        else:\n",
    "            print(\"  -> No tables found in this document.\")\n",
    "\n",
    "        # 2. Process text using 'prov' for metadata\n",
    "        text_chunks_with_meta = []\n",
    "        text_elements = docling_data.get(\"texts\", [])\n",
    "        if text_elements:\n",
    "            for element in text_elements:\n",
    "                if isinstance(element, dict) and \"text\" in element:\n",
    "                    page_no = \"N/A\"\n",
    "                    prov_list = element.get(\"prov\", [])\n",
    "                    # *** HARDENING STEP ***: Check if the list is not empty here as well.\n",
    "                    if prov_list and isinstance(prov_list[0], dict):\n",
    "                        page_no = prov_list[0].get(\"page_no\", \"N/A\")\n",
    "                    \n",
    "                    text_chunks_with_meta.append({\n",
    "                        \"text\": element[\"text\"],\n",
    "                        \"page_number\": page_no,\n",
    "                        \"original\": element.get(\"orig\"),\n",
    "                        \"label\": element.get(\"label\")\n",
    "                    })\n",
    "            print(f\"  -> Extracted {len(text_chunks_with_meta)} text chunks with metadata.\")\n",
    "        else:\n",
    "            print(\"  -> No text elements found in this document.\")\n",
    "\n",
    "        # 3. Return the final prepared data package\n",
    "        prepared_data = {\n",
    "            \"doc_id\": doc_id,\n",
    "            \"source_pdf\": ingest_summary.get(\"pdf_path\"),\n",
    "            \"reconstructed_tables\": reconstructed_tables,\n",
    "            \"text_chunks\": text_chunks_with_meta\n",
    "        }\n",
    "        \n",
    "        print(f\"  -> ✅ Document '{doc_id}' prepared successfully.\")\n",
    "        return prepared_data\n",
    "\n",
    "\n",
    "class TableReconstructionAgent:\n",
    "    \"\"\"\n",
    "    An agent that intelligently reconstructs tables from raw Docling JSON\n",
    "    by iteratively processing chunks of data and preserving financial context.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name: str = \"gemma-3-27b-it\"):\n",
    "        self.model_name = model_name\n",
    "        self.llm = ChatGoogleGenerativeAI(\n",
    "            model=self.model_name, \n",
    "            temperature=0.0,\n",
    "            max_output_tokens=8192,\n",
    "            max_retries=1 \n",
    "        )\n",
    "        self.prompt = self._create_prompt()\n",
    "        print(f\"✅ Agent initialized with Gemini model: {self.model_name}\")\n",
    "\n",
    "    def _create_prompt(self):\n",
    "        \"\"\"\n",
    "        Creates a prompt that works with models without system instruction support\n",
    "        by combining system instructions into the human prompt.\n",
    "        \"\"\"\n",
    "        system_instructions = \"\"\"You are an expert financial data analyst reconstructing tables from raw data.  \n",
    "\n",
    "Your task:\n",
    "1. Process raw table cell data chunks iteratively\n",
    "2. Build a clean, structured table in MARKDOWN format\n",
    "3. Preserve financial context (units, time periods, etc.)\n",
    "4. Maintain proper headers and data alignment\n",
    "\n",
    "Return JSON with this structure:\n",
    "{{\n",
    "  \"markdown_table\": \"| Header 1 | Header 2 |\\\\n|----------|----------|\\\\n| Data 1   | Data 2   |\",\n",
    "  \"financial_context\": {{\n",
    "    \"units\": \"millions USD\",\n",
    "    \"time_periods\": [\"Q1 2025\", \"Q1 2024\"],\n",
    "    \"currency\": \"USD\",\n",
    "    \"notes\": [\"(dollars in millions, except per share amounts)\"]\n",
    "  }},\n",
    "  \"table_metadata\": {{\n",
    "    \"title\": \"Condensed Consolidated Statements of Income\",\n",
    "    \"page\": 1,\n",
    "    \"sections\": [\"Operating Revenues\", \"Operating Expenses\"],\n",
    "    \"total_rows\": 28,\n",
    "    \"total_cols\": 4\n",
    "  }}\n",
    "}}\n",
    "\n",
    "Focus on:\n",
    "- Clear markdown table structure\n",
    "- Proper header identification\n",
    "- Financial section groupings\n",
    "- Unit and time period preservation\n",
    "- Data type recognition (amounts, percentages, text)\"\"\"\n",
    "\n",
    "        human_prompt = f\"\"\"{system_instructions}\n",
    "\n",
    "Current table state: {{existing_table}}\n",
    "\n",
    "Next raw data chunk: {{raw_chunk}}\n",
    "\n",
    "Process this chunk and update the markdown table structure.\"\"\"\n",
    "\n",
    "        return ChatPromptTemplate.from_template(human_prompt)\n",
    "\n",
    "    def _safe_json_loads(self, json_content: str) -> Dict[str, Any]:\n",
    "        \"\"\"Safely parse JSON, cleaning it if necessary.\"\"\"\n",
    "        print(f\"🔍 DEBUG: Attempting to parse JSON content...\")\n",
    "        try:\n",
    "            match = re.search(r'```json\\s*(\\{.*?\\})\\s*```', json_content, re.DOTALL)\n",
    "            if match:\n",
    "                print(\"🔍 DEBUG: Found JSON block inside markdown.\")\n",
    "                json_content = match.group(1)\n",
    "            \n",
    "            return json.loads(json_content)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"❌ ERROR: JSON parsing failed: {e}\")\n",
    "            print(f\"Corrupted JSON content preview: {json_content[:500]}\")\n",
    "            return {\"markdown_table\": \"PARSING_ERROR\"}\n",
    "\n",
    "    def _chunk_table(self, table_data: Dict[str, Any], chunk_size: int = 20) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Splits the raw table cells into manageable chunks.\"\"\"\n",
    "        cells = table_data.get(\"data\", {}).get(\"table_cells\", [])\n",
    "        if not cells:\n",
    "            return []\n",
    "        \n",
    "        chunks = []\n",
    "        for i in range(0, len(cells), chunk_size):\n",
    "            chunks.append({\"cells\": cells[i:i + chunk_size]})\n",
    "        return chunks\n",
    "\n",
    "    def reconstruct_table(self, raw_table: Dict[str, Any], table_index: int) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Processes a single raw table iteratively to build a markdown representation.\n",
    "        *** This version uses a proactive, fixed-wait strategy after each chunk. ***\n",
    "        \"\"\"\n",
    "        print(f\"\\n--- Reconstructing Table # {table_index} (Markdown Format) ---\")\n",
    "        \n",
    "        # --- METADATA ENHANCEMENT using 'prov' ---\n",
    "        page_no = \"N/A\"\n",
    "        prov_list = raw_table.get(\"prov\", [])\n",
    "        if prov_list and isinstance(prov_list[0], dict):\n",
    "            page_no = prov_list[0].get(\"page_no\", \"N/A\")\n",
    "\n",
    "        source_metadata = raw_table.get(\"metadata\", {})\n",
    "        \n",
    "        reconstructed_table = {\n",
    "            \"markdown_table\": \"\",\n",
    "            \"financial_context\": {},\n",
    "            \"table_metadata\": {\n",
    "                \"page\": page_no, # Prioritize page number from 'prov'\n",
    "                \"title\": source_metadata.get(\"title\")\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        table_chunks = self._chunk_table(raw_table)\n",
    "        if not table_chunks:\n",
    "            print(f\"  -> No cells found in table. Skipping.\")\n",
    "            return reconstructed_table\n",
    "\n",
    "        print(f\"  -> Divided into {len(table_chunks)} chunks for processing.\")\n",
    "\n",
    "        for i, chunk in enumerate(table_chunks, 1):\n",
    "            print(f\"    -> Processing chunk {i}/{len(table_chunks)}...\")\n",
    "            \n",
    "            try:\n",
    "                existing_table_str = json.dumps(reconstructed_table, indent=2)\n",
    "                raw_chunk_str = json.dumps(chunk, indent=2)\n",
    "                \n",
    "                prompt_value = self.prompt.invoke({\n",
    "                    \"existing_table\": existing_table_str,\n",
    "                    \"raw_chunk\": raw_chunk_str\n",
    "                })\n",
    "                \n",
    "                resp = self.llm.invoke(prompt_value)\n",
    "                llm_output = self._safe_json_loads(resp.content.strip())\n",
    "                \n",
    "                if \"markdown_table\" in llm_output and llm_output[\"markdown_table\"] != \"PARSING_ERROR\":\n",
    "                    reconstructed_table = llm_output\n",
    "                    reconstructed_table[\"table_metadata\"][\"page\"] = page_no\n",
    "                    if \"title\" not in reconstructed_table[\"table_metadata\"]:\n",
    "                         reconstructed_table[\"table_metadata\"][\"title\"] = source_metadata.get(\"title\")\n",
    "                    \n",
    "                    rows = len(reconstructed_table.get(\"markdown_table\", \"\").split('\\n'))\n",
    "                    print(f\"      -> ✅ Chunk merged. Table now has ~{rows} rows.\")\n",
    "                else:\n",
    "                    print(f\"      -> ⚠️ Skipping update due to invalid response from LLM.\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"      -> ❌ Unhandled error on chunk {i}: {e}\")\n",
    "                print(f\"      -> Moving to the next chunk.\")\n",
    "\n",
    "            # --- Proactive Wait Logic ---\n",
    "            # Wait after every chunk attempt (success or failure) to avoid rate limits.\n",
    "            # Do not wait after the very last chunk.\n",
    "            if i < len(table_chunks):\n",
    "                wait_time = 33 + random.uniform(0, 2)\n",
    "                print(f\"      -> ⏱️ Waiting for {wait_time:.2f} seconds before next chunk...\")\n",
    "                time.sleep(wait_time)\n",
    "            \n",
    "        print(f\"  -> ✅ Table #{table_index} reconstruction completed!\")\n",
    "        return reconstructed_table\n",
    "\n",
    "    def process_first_table_only(self, docling_data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Process only the first table in the Docling document for debugging.\"\"\"\n",
    "        tables = docling_data.get(\"tables\", [])\n",
    "        if not tables:\n",
    "            print(\"❌ No tables found in docling_data\")\n",
    "            return {}\n",
    "        \n",
    "        first_table = tables[0]\n",
    "        print(f\"🔍 Found first table with {len(first_table.get('data', {}).get('table_cells', []))} cells and {first_table.get('data', {}).get('num_rows', 0)} rows.\")\n",
    "        \n",
    "        reconstructed = self.reconstruct_table(first_table, 0)\n",
    "        \n",
    "        if reconstructed and reconstructed.get(\"markdown_table\"):\n",
    "            markdown_table = reconstructed.get(\"markdown_table\", \"\")\n",
    "            print(\"\\n\" + \"=\"*50)\n",
    "            print(\"📊 FINAL RECONSTRUCTED TABLE\")\n",
    "            print(\"=\"*50)\n",
    "            print(markdown_table)\n",
    "            print(\"=\"*50)\n",
    "            print(f\"\\n📋 Final financial context: {reconstructed.get('financial_context', {})}\")\n",
    "            print(f\"📋 Final metadata: {reconstructed.get('table_metadata', {})}\")\n",
    "            return reconstructed\n",
    "        else:\n",
    "            print(\"❌ Failed to reconstruct first table.\")\n",
    "            return {}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8b33a86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'doc_id': 'download-webcast-transcript-pdf-0773ea78',\n",
       "  'pdf_path': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\downloaded_verizon_financial_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2024\\\\1Q\\\\Download Webcast Transcript PDF_0773ea78.pdf',\n",
       "  'out_dir': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\output\\\\parsed_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2024\\\\1Q\\\\download-webcast-transcript-pdf-0773ea78',\n",
       "  'artifacts': {'docling_json': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\output\\\\parsed_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2024\\\\1Q\\\\download-webcast-transcript-pdf-0773ea78\\\\download-webcast-transcript-pdf-0773ea78.docling.json'},\n",
       "  'stats': {'docling_available': True}},\n",
       " {'doc_id': 'download-formal-remarks-pdf-e2835efd',\n",
       "  'pdf_path': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\downloaded_verizon_financial_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2024\\\\1Q\\\\Download Formal Remarks PDF_e2835efd.pdf',\n",
       "  'out_dir': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\output\\\\parsed_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2024\\\\1Q\\\\download-formal-remarks-pdf-e2835efd',\n",
       "  'artifacts': {'docling_json': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\output\\\\parsed_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2024\\\\1Q\\\\download-formal-remarks-pdf-e2835efd\\\\download-formal-remarks-pdf-e2835efd.docling.json'},\n",
       "  'stats': {'docling_available': True}},\n",
       " {'doc_id': 'download-webcast-presentation-pdf-a1b3e2e1',\n",
       "  'pdf_path': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\downloaded_verizon_financial_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2024\\\\1Q\\\\Download Webcast Presentation PDF_a1b3e2e1.pdf',\n",
       "  'out_dir': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\output\\\\parsed_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2024\\\\1Q\\\\download-webcast-presentation-pdf-a1b3e2e1',\n",
       "  'artifacts': {'docling_json': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\output\\\\parsed_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2024\\\\1Q\\\\download-webcast-presentation-pdf-a1b3e2e1\\\\download-webcast-presentation-pdf-a1b3e2e1.docling.json'},\n",
       "  'stats': {'docling_available': True}},\n",
       " {'doc_id': 'download-infographic-pdf-88d85bda',\n",
       "  'pdf_path': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\downloaded_verizon_financial_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2024\\\\1Q\\\\Download Infographic PDF_88d85bda.pdf',\n",
       "  'out_dir': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\output\\\\parsed_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2024\\\\1Q\\\\download-infographic-pdf-88d85bda',\n",
       "  'artifacts': {'docling_json': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\output\\\\parsed_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2024\\\\1Q\\\\download-infographic-pdf-88d85bda\\\\download-infographic-pdf-88d85bda.docling.json'},\n",
       "  'stats': {'docling_available': True}},\n",
       " {'doc_id': 'download-financial-statements-pdf-ad7baa45',\n",
       "  'pdf_path': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\downloaded_verizon_financial_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2024\\\\1Q\\\\Download Financial statements PDF_ad7baa45.pdf',\n",
       "  'out_dir': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\output\\\\parsed_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2024\\\\1Q\\\\download-financial-statements-pdf-ad7baa45',\n",
       "  'artifacts': {'docling_json': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\output\\\\parsed_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2024\\\\1Q\\\\download-financial-statements-pdf-ad7baa45\\\\download-financial-statements-pdf-ad7baa45.docling.json'},\n",
       "  'stats': {'docling_available': True}},\n",
       " {'doc_id': 'download-financial-operating-information-pdf-8c62471e',\n",
       "  'pdf_path': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\downloaded_verizon_financial_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2024\\\\1Q\\\\Download Financial & Operating information PDF_8c62471e.pdf',\n",
       "  'out_dir': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\output\\\\parsed_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2024\\\\1Q\\\\download-financial-operating-information-pdf-8c62471e',\n",
       "  'artifacts': {'docling_json': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\output\\\\parsed_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2024\\\\1Q\\\\download-financial-operating-information-pdf-8c62471e\\\\download-financial-operating-information-pdf-8c62471e.docling.json'},\n",
       "  'stats': {'docling_available': True}},\n",
       " {'doc_id': 'download-non-gaap-reconciliations-pdf-9669cf61',\n",
       "  'pdf_path': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\downloaded_verizon_financial_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2024\\\\1Q\\\\Download Non-GAAP reconciliations PDF_9669cf61.pdf',\n",
       "  'out_dir': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\output\\\\parsed_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2024\\\\1Q\\\\download-non-gaap-reconciliations-pdf-9669cf61',\n",
       "  'artifacts': {'docling_json': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\output\\\\parsed_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2024\\\\1Q\\\\download-non-gaap-reconciliations-pdf-9669cf61\\\\download-non-gaap-reconciliations-pdf-9669cf61.docling.json'},\n",
       "  'stats': {'docling_available': True}},\n",
       " {'doc_id': 'download-webcast-transcript-pdf-2283b88e',\n",
       "  'pdf_path': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\downloaded_verizon_financial_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2024\\\\2Q\\\\Download Webcast Transcript PDF_2283b88e.pdf',\n",
       "  'out_dir': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\output\\\\parsed_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2024\\\\2Q\\\\download-webcast-transcript-pdf-2283b88e',\n",
       "  'artifacts': {'docling_json': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\output\\\\parsed_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2024\\\\2Q\\\\download-webcast-transcript-pdf-2283b88e\\\\download-webcast-transcript-pdf-2283b88e.docling.json'},\n",
       "  'stats': {'docling_available': True}},\n",
       " {'doc_id': 'download-formal-remarks-pdf-4c40b0bf',\n",
       "  'pdf_path': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\downloaded_verizon_financial_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2024\\\\2Q\\\\Download Formal Remarks PDF_4c40b0bf.pdf',\n",
       "  'out_dir': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\output\\\\parsed_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2024\\\\2Q\\\\download-formal-remarks-pdf-4c40b0bf',\n",
       "  'artifacts': {'docling_json': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\output\\\\parsed_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2024\\\\2Q\\\\download-formal-remarks-pdf-4c40b0bf\\\\download-formal-remarks-pdf-4c40b0bf.docling.json'},\n",
       "  'stats': {'docling_available': True}},\n",
       " {'doc_id': 'download-webcast-presentation-pdf-ceaa87f0',\n",
       "  'pdf_path': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\downloaded_verizon_financial_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2024\\\\2Q\\\\Download Webcast Presentation PDF_ceaa87f0.pdf',\n",
       "  'out_dir': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\output\\\\parsed_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2024\\\\2Q\\\\download-webcast-presentation-pdf-ceaa87f0',\n",
       "  'artifacts': {'docling_json': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\output\\\\parsed_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2024\\\\2Q\\\\download-webcast-presentation-pdf-ceaa87f0\\\\download-webcast-presentation-pdf-ceaa87f0.docling.json'},\n",
       "  'stats': {'docling_available': True}},\n",
       " {'doc_id': 'download-infographic-pdf-733d33e1',\n",
       "  'pdf_path': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\downloaded_verizon_financial_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2024\\\\2Q\\\\Download Infographic PDF_733d33e1.pdf',\n",
       "  'out_dir': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\output\\\\parsed_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2024\\\\2Q\\\\download-infographic-pdf-733d33e1',\n",
       "  'artifacts': {'docling_json': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\output\\\\parsed_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2024\\\\2Q\\\\download-infographic-pdf-733d33e1\\\\download-infographic-pdf-733d33e1.docling.json'},\n",
       "  'stats': {'docling_available': True}},\n",
       " {'doc_id': 'download-financial-statements-pdf-de2bf44c',\n",
       "  'pdf_path': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\downloaded_verizon_financial_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2024\\\\2Q\\\\Download Financial statements PDF_de2bf44c.pdf',\n",
       "  'out_dir': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\output\\\\parsed_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2024\\\\2Q\\\\download-financial-statements-pdf-de2bf44c',\n",
       "  'artifacts': {'docling_json': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\output\\\\parsed_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2024\\\\2Q\\\\download-financial-statements-pdf-de2bf44c\\\\download-financial-statements-pdf-de2bf44c.docling.json'},\n",
       "  'stats': {'docling_available': True}},\n",
       " {'doc_id': 'download-financial-operating-information-pdf-56695261',\n",
       "  'pdf_path': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\downloaded_verizon_financial_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2024\\\\2Q\\\\Download Financial & Operating information PDF_56695261.pdf',\n",
       "  'out_dir': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\output\\\\parsed_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2024\\\\2Q\\\\download-financial-operating-information-pdf-56695261',\n",
       "  'artifacts': {'docling_json': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\output\\\\parsed_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2024\\\\2Q\\\\download-financial-operating-information-pdf-56695261\\\\download-financial-operating-information-pdf-56695261.docling.json'},\n",
       "  'stats': {'docling_available': True}},\n",
       " {'doc_id': 'download-non-gaap-reconciliations-pdf-06cc31ee',\n",
       "  'pdf_path': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\downloaded_verizon_financial_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2024\\\\2Q\\\\Download Non-GAAP reconciliations PDF_06cc31ee.pdf',\n",
       "  'out_dir': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\output\\\\parsed_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2024\\\\2Q\\\\download-non-gaap-reconciliations-pdf-06cc31ee',\n",
       "  'artifacts': {'docling_json': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\output\\\\parsed_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2024\\\\2Q\\\\download-non-gaap-reconciliations-pdf-06cc31ee\\\\download-non-gaap-reconciliations-pdf-06cc31ee.docling.json'},\n",
       "  'stats': {'docling_available': True}},\n",
       " {'doc_id': 'download-webcast-transcript-pdf-4d61943a',\n",
       "  'pdf_path': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\downloaded_verizon_financial_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2024\\\\3Q\\\\Download Webcast Transcript PDF_4d61943a.pdf',\n",
       "  'out_dir': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\output\\\\parsed_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2024\\\\3Q\\\\download-webcast-transcript-pdf-4d61943a',\n",
       "  'artifacts': {'docling_json': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\output\\\\parsed_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2024\\\\3Q\\\\download-webcast-transcript-pdf-4d61943a\\\\download-webcast-transcript-pdf-4d61943a.docling.json'},\n",
       "  'stats': {'docling_available': True}},\n",
       " {'doc_id': 'download-formal-remarks-pdf-3f10e07d',\n",
       "  'pdf_path': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\downloaded_verizon_financial_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2024\\\\3Q\\\\Download Formal Remarks PDF_3f10e07d.pdf',\n",
       "  'out_dir': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\output\\\\parsed_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2024\\\\3Q\\\\download-formal-remarks-pdf-3f10e07d',\n",
       "  'artifacts': {'docling_json': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\output\\\\parsed_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2024\\\\3Q\\\\download-formal-remarks-pdf-3f10e07d\\\\download-formal-remarks-pdf-3f10e07d.docling.json'},\n",
       "  'stats': {'docling_available': True}},\n",
       " {'doc_id': 'download-presentation-pdf-876a8141',\n",
       "  'pdf_path': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\downloaded_verizon_financial_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2024\\\\3Q\\\\Download Presentation PDF_876a8141.pdf',\n",
       "  'out_dir': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\output\\\\parsed_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2024\\\\3Q\\\\download-presentation-pdf-876a8141',\n",
       "  'artifacts': {'docling_json': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\output\\\\parsed_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2024\\\\3Q\\\\download-presentation-pdf-876a8141\\\\download-presentation-pdf-876a8141.docling.json'},\n",
       "  'stats': {'docling_available': True}},\n",
       " {'doc_id': 'download-infographic-pdf-20100392',\n",
       "  'pdf_path': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\downloaded_verizon_financial_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2024\\\\3Q\\\\Download Infographic PDF_20100392.pdf',\n",
       "  'out_dir': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\output\\\\parsed_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2024\\\\3Q\\\\download-infographic-pdf-20100392',\n",
       "  'artifacts': {'docling_json': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\output\\\\parsed_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2024\\\\3Q\\\\download-infographic-pdf-20100392\\\\download-infographic-pdf-20100392.docling.json'},\n",
       "  'stats': {'docling_available': True}},\n",
       " {'doc_id': 'download-financial-statements-pdf-23b3bcbf',\n",
       "  'pdf_path': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\downloaded_verizon_financial_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2024\\\\3Q\\\\Download Financial statements PDF_23b3bcbf.pdf',\n",
       "  'out_dir': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\output\\\\parsed_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2024\\\\3Q\\\\download-financial-statements-pdf-23b3bcbf',\n",
       "  'artifacts': {'docling_json': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\output\\\\parsed_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2024\\\\3Q\\\\download-financial-statements-pdf-23b3bcbf\\\\download-financial-statements-pdf-23b3bcbf.docling.json'},\n",
       "  'stats': {'docling_available': True}},\n",
       " {'doc_id': 'download-financial-operating-information-pdf-548e5fff',\n",
       "  'pdf_path': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\downloaded_verizon_financial_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2024\\\\3Q\\\\Download Financial & Operating information PDF_548e5fff.pdf',\n",
       "  'out_dir': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\output\\\\parsed_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2024\\\\3Q\\\\download-financial-operating-information-pdf-548e5fff',\n",
       "  'artifacts': {'docling_json': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\output\\\\parsed_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2024\\\\3Q\\\\download-financial-operating-information-pdf-548e5fff\\\\download-financial-operating-information-pdf-548e5fff.docling.json'},\n",
       "  'stats': {'docling_available': True}},\n",
       " {'doc_id': 'download-non-gaap-reconciliations-pdf-26969289',\n",
       "  'pdf_path': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\downloaded_verizon_financial_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2024\\\\3Q\\\\Download Non-GAAP reconciliations PDF_26969289.pdf',\n",
       "  'out_dir': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\output\\\\parsed_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2024\\\\3Q\\\\download-non-gaap-reconciliations-pdf-26969289',\n",
       "  'artifacts': {'docling_json': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\output\\\\parsed_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2024\\\\3Q\\\\download-non-gaap-reconciliations-pdf-26969289\\\\download-non-gaap-reconciliations-pdf-26969289.docling.json'},\n",
       "  'stats': {'docling_available': True}},\n",
       " {'doc_id': 'download-webcast-transcript-pdf-791d532a',\n",
       "  'pdf_path': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\downloaded_verizon_financial_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2024\\\\4Q\\\\Download Webcast Transcript PDF_791d532a.pdf',\n",
       "  'out_dir': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\output\\\\parsed_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2024\\\\4Q\\\\download-webcast-transcript-pdf-791d532a',\n",
       "  'artifacts': {'docling_json': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\output\\\\parsed_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2024\\\\4Q\\\\download-webcast-transcript-pdf-791d532a\\\\download-webcast-transcript-pdf-791d532a.docling.json'},\n",
       "  'stats': {'docling_available': True}},\n",
       " {'doc_id': 'download-webcast-presentation-pdf-cf490e3b',\n",
       "  'pdf_path': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\downloaded_verizon_financial_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2024\\\\4Q\\\\Download Webcast Presentation PDF_cf490e3b.pdf',\n",
       "  'out_dir': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\output\\\\parsed_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2024\\\\4Q\\\\download-webcast-presentation-pdf-cf490e3b',\n",
       "  'artifacts': {'docling_json': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\output\\\\parsed_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2024\\\\4Q\\\\download-webcast-presentation-pdf-cf490e3b\\\\download-webcast-presentation-pdf-cf490e3b.docling.json'},\n",
       "  'stats': {'docling_available': True}},\n",
       " {'doc_id': 'download-earnings-presentation-pdf-828709d1',\n",
       "  'pdf_path': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\downloaded_verizon_financial_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2024\\\\4Q\\\\Download Earnings Presentation PDF_828709d1.pdf',\n",
       "  'out_dir': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\output\\\\parsed_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2024\\\\4Q\\\\download-earnings-presentation-pdf-828709d1',\n",
       "  'artifacts': {'docling_json': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\output\\\\parsed_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2024\\\\4Q\\\\download-earnings-presentation-pdf-828709d1\\\\download-earnings-presentation-pdf-828709d1.docling.json'},\n",
       "  'stats': {'docling_available': True}},\n",
       " {'doc_id': 'download-prepared-earnings-transcript-pdf-daaf83b5',\n",
       "  'pdf_path': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\downloaded_verizon_financial_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2024\\\\4Q\\\\Download Prepared Earnings Transcript PDF_daaf83b5.pdf',\n",
       "  'out_dir': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\output\\\\parsed_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2024\\\\4Q\\\\download-prepared-earnings-transcript-pdf-daaf83b5',\n",
       "  'artifacts': {'docling_json': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\output\\\\parsed_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2024\\\\4Q\\\\download-prepared-earnings-transcript-pdf-daaf83b5\\\\download-prepared-earnings-transcript-pdf-daaf83b5.docling.json'},\n",
       "  'stats': {'docling_available': True}},\n",
       " {'doc_id': 'download-formal-remarks-pdf-1a7008fb',\n",
       "  'pdf_path': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\downloaded_verizon_financial_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2024\\\\4Q\\\\Download Formal Remarks PDF_1a7008fb.pdf',\n",
       "  'out_dir': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\output\\\\parsed_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2024\\\\4Q\\\\download-formal-remarks-pdf-1a7008fb',\n",
       "  'artifacts': {'docling_json': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\output\\\\parsed_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2024\\\\4Q\\\\download-formal-remarks-pdf-1a7008fb\\\\download-formal-remarks-pdf-1a7008fb.docling.json'},\n",
       "  'stats': {'docling_available': True}},\n",
       " {'doc_id': 'download-financial-statements-pdf-56712f58',\n",
       "  'pdf_path': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\downloaded_verizon_financial_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2024\\\\4Q\\\\Download Financial statements PDF_56712f58.pdf',\n",
       "  'out_dir': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\output\\\\parsed_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2024\\\\4Q\\\\download-financial-statements-pdf-56712f58',\n",
       "  'artifacts': {'docling_json': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\output\\\\parsed_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2024\\\\4Q\\\\download-financial-statements-pdf-56712f58\\\\download-financial-statements-pdf-56712f58.docling.json'},\n",
       "  'stats': {'docling_available': True}},\n",
       " {'doc_id': 'download-financial-operating-information-pdf-2f954363',\n",
       "  'pdf_path': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\downloaded_verizon_financial_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2024\\\\4Q\\\\Download Financial & Operating information PDF_2f954363.pdf',\n",
       "  'out_dir': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\output\\\\parsed_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2024\\\\4Q\\\\download-financial-operating-information-pdf-2f954363',\n",
       "  'artifacts': {'docling_json': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\output\\\\parsed_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2024\\\\4Q\\\\download-financial-operating-information-pdf-2f954363\\\\download-financial-operating-information-pdf-2f954363.docling.json'},\n",
       "  'stats': {'docling_available': True}},\n",
       " {'doc_id': 'download-non-gaap-reconciliations-pdf-a98978fa',\n",
       "  'pdf_path': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\downloaded_verizon_financial_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2024\\\\4Q\\\\Download Non-GAAP reconciliations PDF_a98978fa.pdf',\n",
       "  'out_dir': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\output\\\\parsed_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2024\\\\4Q\\\\download-non-gaap-reconciliations-pdf-a98978fa',\n",
       "  'artifacts': {'docling_json': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\output\\\\parsed_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2024\\\\4Q\\\\download-non-gaap-reconciliations-pdf-a98978fa\\\\download-non-gaap-reconciliations-pdf-a98978fa.docling.json'},\n",
       "  'stats': {'docling_available': True}},\n",
       " {'doc_id': 'download-non-gaap-reconciliations-pdf-de6a19a8',\n",
       "  'pdf_path': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\downloaded_verizon_financial_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2024\\\\4Q\\\\Download Non-GAAP reconciliations PDF_de6a19a8.pdf',\n",
       "  'out_dir': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\output\\\\parsed_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2024\\\\4Q\\\\download-non-gaap-reconciliations-pdf-de6a19a8',\n",
       "  'artifacts': {'docling_json': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\output\\\\parsed_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2024\\\\4Q\\\\download-non-gaap-reconciliations-pdf-de6a19a8\\\\download-non-gaap-reconciliations-pdf-de6a19a8.docling.json'},\n",
       "  'stats': {'docling_available': True}},\n",
       " {'doc_id': 'download-webcast-transcript-pdf-f011dc37',\n",
       "  'pdf_path': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\downloaded_verizon_financial_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2025\\\\1Q\\\\Download Webcast Transcript PDF_f011dc37.pdf',\n",
       "  'out_dir': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\output\\\\parsed_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2025\\\\1Q\\\\download-webcast-transcript-pdf-f011dc37',\n",
       "  'artifacts': {'docling_json': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\output\\\\parsed_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2025\\\\1Q\\\\download-webcast-transcript-pdf-f011dc37\\\\download-webcast-transcript-pdf-f011dc37.docling.json'},\n",
       "  'stats': {'docling_available': True}},\n",
       " {'doc_id': 'download-webcast-presentation-pdf-da5ae256',\n",
       "  'pdf_path': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\downloaded_verizon_financial_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2025\\\\1Q\\\\Download Webcast Presentation PDF_da5ae256.pdf',\n",
       "  'out_dir': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\output\\\\parsed_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2025\\\\1Q\\\\download-webcast-presentation-pdf-da5ae256',\n",
       "  'artifacts': {'docling_json': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\output\\\\parsed_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2025\\\\1Q\\\\download-webcast-presentation-pdf-da5ae256\\\\download-webcast-presentation-pdf-da5ae256.docling.json'},\n",
       "  'stats': {'docling_available': True}},\n",
       " {'doc_id': 'download-earnings-presentation-pdf-be525a5d',\n",
       "  'pdf_path': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\downloaded_verizon_financial_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2025\\\\1Q\\\\Download Earnings Presentation PDF_be525a5d.pdf',\n",
       "  'out_dir': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\output\\\\parsed_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2025\\\\1Q\\\\download-earnings-presentation-pdf-be525a5d',\n",
       "  'artifacts': {'docling_json': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\output\\\\parsed_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2025\\\\1Q\\\\download-earnings-presentation-pdf-be525a5d\\\\download-earnings-presentation-pdf-be525a5d.docling.json'},\n",
       "  'stats': {'docling_available': True}},\n",
       " {'doc_id': 'download-prepared-earnings-transcript-pdf-15df53b4',\n",
       "  'pdf_path': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\downloaded_verizon_financial_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2025\\\\1Q\\\\Download Prepared Earnings Transcript PDF_15df53b4.pdf',\n",
       "  'out_dir': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\output\\\\parsed_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2025\\\\1Q\\\\download-prepared-earnings-transcript-pdf-15df53b4',\n",
       "  'artifacts': {'docling_json': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\output\\\\parsed_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2025\\\\1Q\\\\download-prepared-earnings-transcript-pdf-15df53b4\\\\download-prepared-earnings-transcript-pdf-15df53b4.docling.json'},\n",
       "  'stats': {'docling_available': True}},\n",
       " {'doc_id': 'download-infographic-pdf-30c3d8e4',\n",
       "  'pdf_path': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\downloaded_verizon_financial_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2025\\\\1Q\\\\Download Infographic PDF_30c3d8e4.pdf',\n",
       "  'out_dir': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\output\\\\parsed_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2025\\\\1Q\\\\download-infographic-pdf-30c3d8e4',\n",
       "  'artifacts': {'docling_json': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\output\\\\parsed_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2025\\\\1Q\\\\download-infographic-pdf-30c3d8e4\\\\download-infographic-pdf-30c3d8e4.docling.json'},\n",
       "  'stats': {'docling_available': True}},\n",
       " {'doc_id': 'download-financial-statements-pdf-5a9b6fa4',\n",
       "  'pdf_path': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\downloaded_verizon_financial_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2025\\\\1Q\\\\Download Financial statements PDF_5a9b6fa4.pdf',\n",
       "  'out_dir': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\output\\\\parsed_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2025\\\\1Q\\\\download-financial-statements-pdf-5a9b6fa4',\n",
       "  'artifacts': {'docling_json': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\output\\\\parsed_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2025\\\\1Q\\\\download-financial-statements-pdf-5a9b6fa4\\\\download-financial-statements-pdf-5a9b6fa4.docling.json'},\n",
       "  'stats': {'docling_available': True}},\n",
       " {'doc_id': 'download-financial-operating-information-pdf-3dedf514',\n",
       "  'pdf_path': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\downloaded_verizon_financial_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2025\\\\1Q\\\\Download Financial & Operating information PDF_3dedf514.pdf',\n",
       "  'out_dir': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\output\\\\parsed_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2025\\\\1Q\\\\download-financial-operating-information-pdf-3dedf514',\n",
       "  'artifacts': {'docling_json': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\output\\\\parsed_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2025\\\\1Q\\\\download-financial-operating-information-pdf-3dedf514\\\\download-financial-operating-information-pdf-3dedf514.docling.json'},\n",
       "  'stats': {'docling_available': True}},\n",
       " {'doc_id': 'download-non-gaap-reconciliations-pdf-d419afb0',\n",
       "  'pdf_path': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\downloaded_verizon_financial_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2025\\\\1Q\\\\Download Non-GAAP reconciliations PDF_d419afb0.pdf',\n",
       "  'out_dir': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\output\\\\parsed_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2025\\\\1Q\\\\download-non-gaap-reconciliations-pdf-d419afb0',\n",
       "  'artifacts': {'docling_json': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\output\\\\parsed_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2025\\\\1Q\\\\download-non-gaap-reconciliations-pdf-d419afb0\\\\download-non-gaap-reconciliations-pdf-d419afb0.docling.json'},\n",
       "  'stats': {'docling_available': True}},\n",
       " {'doc_id': 'download-webcast-transcript-pdf-c1bdcc0f',\n",
       "  'pdf_path': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\downloaded_verizon_financial_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2025\\\\2Q\\\\Download Webcast Transcript PDF_c1bdcc0f.pdf',\n",
       "  'out_dir': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\output\\\\parsed_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2025\\\\2Q\\\\download-webcast-transcript-pdf-c1bdcc0f',\n",
       "  'artifacts': {'docling_json': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\output\\\\parsed_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2025\\\\2Q\\\\download-webcast-transcript-pdf-c1bdcc0f\\\\download-webcast-transcript-pdf-c1bdcc0f.docling.json'},\n",
       "  'stats': {'docling_available': True}},\n",
       " {'doc_id': 'download-presentation-pdf-90cbcc8c',\n",
       "  'pdf_path': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\downloaded_verizon_financial_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2025\\\\2Q\\\\Download Presentation PDF_90cbcc8c.pdf',\n",
       "  'out_dir': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\output\\\\parsed_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2025\\\\2Q\\\\download-presentation-pdf-90cbcc8c',\n",
       "  'artifacts': {'docling_json': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\output\\\\parsed_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2025\\\\2Q\\\\download-presentation-pdf-90cbcc8c\\\\download-presentation-pdf-90cbcc8c.docling.json'},\n",
       "  'stats': {'docling_available': True}},\n",
       " {'doc_id': 'download-infographic-pdf-5450fea0',\n",
       "  'pdf_path': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\downloaded_verizon_financial_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2025\\\\2Q\\\\Download Infographic PDF_5450fea0.pdf',\n",
       "  'out_dir': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\output\\\\parsed_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2025\\\\2Q\\\\download-infographic-pdf-5450fea0',\n",
       "  'artifacts': {'docling_json': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\output\\\\parsed_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2025\\\\2Q\\\\download-infographic-pdf-5450fea0\\\\download-infographic-pdf-5450fea0.docling.json'},\n",
       "  'stats': {'docling_available': True}},\n",
       " {'doc_id': 'download-financial-statements-pdf-7b4b0746',\n",
       "  'pdf_path': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\downloaded_verizon_financial_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2025\\\\2Q\\\\Download Financial statements PDF_7b4b0746.pdf',\n",
       "  'out_dir': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\output\\\\parsed_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2025\\\\2Q\\\\download-financial-statements-pdf-7b4b0746',\n",
       "  'artifacts': {'docling_json': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\output\\\\parsed_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2025\\\\2Q\\\\download-financial-statements-pdf-7b4b0746\\\\download-financial-statements-pdf-7b4b0746.docling.json'},\n",
       "  'stats': {'docling_available': True}},\n",
       " {'doc_id': 'download-financial-operating-information-pdf-e1c5ab85',\n",
       "  'pdf_path': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\downloaded_verizon_financial_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2025\\\\2Q\\\\Download Financial & Operating information PDF_e1c5ab85.pdf',\n",
       "  'out_dir': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\output\\\\parsed_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2025\\\\2Q\\\\download-financial-operating-information-pdf-e1c5ab85',\n",
       "  'artifacts': {'docling_json': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\output\\\\parsed_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2025\\\\2Q\\\\download-financial-operating-information-pdf-e1c5ab85\\\\download-financial-operating-information-pdf-e1c5ab85.docling.json'},\n",
       "  'stats': {'docling_available': True}},\n",
       " {'doc_id': 'download-non-gaap-reconciliations-pdf-eb924252',\n",
       "  'pdf_path': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\downloaded_verizon_financial_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2025\\\\2Q\\\\Download Non-GAAP reconciliations PDF_eb924252.pdf',\n",
       "  'out_dir': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\output\\\\parsed_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2025\\\\2Q\\\\download-non-gaap-reconciliations-pdf-eb924252',\n",
       "  'artifacts': {'docling_json': 'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\output\\\\parsed_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2025\\\\2Q\\\\download-non-gaap-reconciliations-pdf-eb924252\\\\download-non-gaap-reconciliations-pdf-eb924252.docling.json'},\n",
       "  'stats': {'docling_available': True}}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e71fe6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 DEBUG: Loading Docling data...\n",
      "🔍 DEBUG: Initializing agent...\n",
      "✅ Agent initialized with Gemini model: gemma-3-27b-it\n",
      "🔍 DEBUG: Processing first table...\n",
      "🔍 Found first table with 95 cells and 28 rows.\n",
      "\n",
      "--- Reconstructing Table # 0 (Markdown Format) ---\n",
      "  -> Divided into 5 chunks for processing.\n",
      "    -> Processing chunk 1/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~9 rows.\n",
      "    -> ⏱️  Waiting for 40 seconds to avoid rate limits...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~9 rows.\n",
      "    -> ⏱️  Waiting for 40 seconds to avoid rate limits...\n",
      "    -> Processing chunk 2/5...\n",
      "    -> Processing chunk 2/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~14 rows.\n",
      "    -> ⏱️  Waiting for 40 seconds to avoid rate limits...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~14 rows.\n",
      "    -> ⏱️  Waiting for 40 seconds to avoid rate limits...\n",
      "    -> Processing chunk 3/5...\n",
      "    -> Processing chunk 3/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~19 rows.\n",
      "    -> ⏱️  Waiting for 40 seconds to avoid rate limits...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~19 rows.\n",
      "    -> ⏱️  Waiting for 40 seconds to avoid rate limits...\n",
      "    -> Processing chunk 4/5...\n",
      "    -> Processing chunk 4/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~23 rows.\n",
      "    -> ⏱️  Waiting for 40 seconds to avoid rate limits...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~23 rows.\n",
      "    -> ⏱️  Waiting for 40 seconds to avoid rate limits...\n",
      "    -> Processing chunk 5/5...\n",
      "    -> Processing chunk 5/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~28 rows.\n",
      "  -> ✅ Table #0 reconstruction completed!\n",
      "\n",
      "==================================================\n",
      "📊 FINAL RECONSTRUCTED TABLE\n",
      "==================================================\n",
      "|               | 3 Mos. Ended 3/31/25 | 3 Mos. Ended 3/31/24 | % Change |\n",
      "|----------------|-----------------------|-----------------------|----------|\n",
      "| Unaudited      |                       |                       |          |\n",
      "| Operating Revenues |                       |                       |          |\n",
      "| Service revenues and other | $ 28,087              | $ 27,620              | 1.7      |\n",
      "| Wireless equipment revenues | 5,398                | 5,361                | 0.7      |\n",
      "| Total Operating Revenues | 33,485               | 32,981               | 1.5      |\n",
      "| Operating Expenses |                       |                       |          |\n",
      "| Cost of services | 6,950                | 6,967                | (0.2)    |\n",
      "| Cost of wireless equipment | 6,106                | 5,905                | 3.4      |\n",
      "| Selling, general and administrative expense | 7,874                | 8,143                | (3.3)    |\n",
      "| Depreciation and amortization expense | 4,577                | 4,445                | 3.0      |\n",
      "| Total Operating Expenses | 25,507               | 25,460               | 0.2      |\n",
      "| Operating Income |                       |                       |          |\n",
      "| Equity in earnings (losses) of unconsolidated businesses | 7,978                | 7,521                | 6.1      |\n",
      "| Other income, net | 121                   | 198                   | (38.9)   |\n",
      "| Interest expense | (1,632)               | (1,635)               | (0.2)    |\n",
      "| Income Before Provision For Income Taxes | 6,473                | 6,075                | 6.6      |\n",
      "| Provision for income taxes |                       |                       |          |\n",
      "| Net Income |                       |                       |          |\n",
      "| Net income attributable to noncontrolling interests | $ 104                | $ 120                | (13.3)   |\n",
      "| Net income attributable to Verizon | $ 4,879              | $ 4,602              | 6.0      |\n",
      "| Basic Earnings Per Common Share |                       |                       |          |\n",
      "| Net income attributable to Verizon | $ 1.16               | $ 1.09               | 6.4      |\n",
      "| Weighted-average shares outstanding (in millions) | 4,222                | 4,215                |          |\n",
      "| Diluted Earnings Per Common Share (1) |                       |                       |          |\n",
      "| Net income attributable to Verizon | $ 1.15               | $ 1.09               | 5.5      |\n",
      "| Weighted-average shares outstanding (in millions) | 4,226                | 4,219                |          |\n",
      "==================================================\n",
      "\n",
      "📋 Final financial context: {'units': 'millions USD', 'time_periods': ['Q1 2025', 'Q1 2024'], 'currency': 'USD', 'notes': ['(dollars in millions, except per share amounts)']}\n",
      "📋 Final metadata: {'title': 'Condensed Consolidated Statements of Income', 'page': 1, 'sections': ['Operating Revenues', 'Operating Expenses'], 'total_rows': 28, 'total_cols': 4}\n",
      "\n",
      "✅ SUCCESS: First table processed successfully!\n",
      "📊 Final markdown table has 28 lines.\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~28 rows.\n",
      "  -> ✅ Table #0 reconstruction completed!\n",
      "\n",
      "==================================================\n",
      "📊 FINAL RECONSTRUCTED TABLE\n",
      "==================================================\n",
      "|               | 3 Mos. Ended 3/31/25 | 3 Mos. Ended 3/31/24 | % Change |\n",
      "|----------------|-----------------------|-----------------------|----------|\n",
      "| Unaudited      |                       |                       |          |\n",
      "| Operating Revenues |                       |                       |          |\n",
      "| Service revenues and other | $ 28,087              | $ 27,620              | 1.7      |\n",
      "| Wireless equipment revenues | 5,398                | 5,361                | 0.7      |\n",
      "| Total Operating Revenues | 33,485               | 32,981               | 1.5      |\n",
      "| Operating Expenses |                       |                       |          |\n",
      "| Cost of services | 6,950                | 6,967                | (0.2)    |\n",
      "| Cost of wireless equipment | 6,106                | 5,905                | 3.4      |\n",
      "| Selling, general and administrative expense | 7,874                | 8,143                | (3.3)    |\n",
      "| Depreciation and amortization expense | 4,577                | 4,445                | 3.0      |\n",
      "| Total Operating Expenses | 25,507               | 25,460               | 0.2      |\n",
      "| Operating Income |                       |                       |          |\n",
      "| Equity in earnings (losses) of unconsolidated businesses | 7,978                | 7,521                | 6.1      |\n",
      "| Other income, net | 121                   | 198                   | (38.9)   |\n",
      "| Interest expense | (1,632)               | (1,635)               | (0.2)    |\n",
      "| Income Before Provision For Income Taxes | 6,473                | 6,075                | 6.6      |\n",
      "| Provision for income taxes |                       |                       |          |\n",
      "| Net Income |                       |                       |          |\n",
      "| Net income attributable to noncontrolling interests | $ 104                | $ 120                | (13.3)   |\n",
      "| Net income attributable to Verizon | $ 4,879              | $ 4,602              | 6.0      |\n",
      "| Basic Earnings Per Common Share |                       |                       |          |\n",
      "| Net income attributable to Verizon | $ 1.16               | $ 1.09               | 6.4      |\n",
      "| Weighted-average shares outstanding (in millions) | 4,222                | 4,215                |          |\n",
      "| Diluted Earnings Per Common Share (1) |                       |                       |          |\n",
      "| Net income attributable to Verizon | $ 1.15               | $ 1.09               | 5.5      |\n",
      "| Weighted-average shares outstanding (in millions) | 4,226                | 4,219                |          |\n",
      "==================================================\n",
      "\n",
      "📋 Final financial context: {'units': 'millions USD', 'time_periods': ['Q1 2025', 'Q1 2024'], 'currency': 'USD', 'notes': ['(dollars in millions, except per share amounts)']}\n",
      "📋 Final metadata: {'title': 'Condensed Consolidated Statements of Income', 'page': 1, 'sections': ['Operating Revenues', 'Operating Expenses'], 'total_rows': 28, 'total_cols': 4}\n",
      "\n",
      "✅ SUCCESS: First table processed successfully!\n",
      "📊 Final markdown table has 28 lines.\n"
     ]
    }
   ],
   "source": [
    "# Test with rate limit handling\n",
    "try:\n",
    "    print(\"🔍 DEBUG: Loading Docling data...\")\n",
    "    DOC_JSON_PATH = r\"I:\\My Drive\\M. Tech AI ML\\AIML SEM 4\\Dissertation\\Project\\output\\parsed_pdfs\\download-financial-statements-pdf-5a9b6fa4\\download-financial-statements-pdf-5a9b6fa4.docling.json\"\n",
    "    \n",
    "    with open(DOC_JSON_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        docling_data = json.load(f)\n",
    "    \n",
    "    print(\"🔍 DEBUG: Initializing agent...\")\n",
    "    agent = TableReconstructionAgent()\n",
    "    \n",
    "    print(\"🔍 DEBUG: Processing first table...\")\n",
    "    first_table_result = agent.process_first_table_only(docling_data)\n",
    "    \n",
    "    if first_table_result and first_table_result.get(\"markdown_table\"):\n",
    "        print(f\"\\n✅ SUCCESS: First table processed successfully!\")\n",
    "        final_rows = len(first_table_result.get('markdown_table', '').split('\\n'))\n",
    "        print(f\"📊 Final markdown table has {final_rows} lines.\")\n",
    "    else:\n",
    "        print(f\"\\n❌ FAILED: Could not process first table\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Error during table reconstruction: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd508f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Agent initialized with Gemini model: gemma-3-27b-it\n",
      "✅ DocumentProcessor initialized.\n",
      "\n",
      "--- Preparing Document for Graph Extraction: download-webcast-transcript-pdf-0773ea78 ---\n",
      "  -> No tables found in this document.\n",
      "  -> Extracted 339 text chunks with metadata.\n",
      "  -> ✅ Document 'download-webcast-transcript-pdf-0773ea78' prepared successfully.\n",
      "\n",
      "--- Example Prepared Data Output ---\n",
      "Document ID: download-webcast-transcript-pdf-0773ea78\n",
      "Tables Reconstructed: 0\n",
      "Text Chunks Found: 339\n"
     ]
    }
   ],
   "source": [
    "# --- Example Usage ---\n",
    "# (Assuming 'all_results' from the previous ingestion cell is available)\n",
    "\n",
    "# 1. Initialize the agent that the processor will use\n",
    "# Note: This can be time-consuming if you run it for every document,\n",
    "# so we initialize it once and pass it to the processor.\n",
    "table_recon_agent = TableReconstructionAgent()\n",
    "\n",
    "# 2. Initialize the processor\n",
    "doc_processor = DocumentProcessor(table_agent=table_recon_agent)\n",
    "\n",
    "# 3. Process the first document from our ingestion results as a test\n",
    "if all_results:\n",
    "    first_doc_summary = all_results[0]\n",
    "    prepared_document_data = doc_processor.process_document(first_doc_summary)\n",
    "\n",
    "    # Print a summary of the prepared data\n",
    "    if prepared_document_data:\n",
    "        print(\"\\n--- Example Prepared Data Output ---\")\n",
    "        print(f\"Document ID: {prepared_document_data['doc_id']}\")\n",
    "        print(f\"Tables Reconstructed: {len(prepared_document_data['reconstructed_tables'])}\")\n",
    "        print(f\"Text Chunks Found: {len(prepared_document_data['text_chunks'])}\")\n",
    "        \n",
    "        # --- SAVE THE PREPARED DATA ---\n",
    "        save_prepared_data(prepared_document_data, first_doc_summary)\n",
    "\n",
    "        # Preview the first table's data\n",
    "        if prepared_document_data['reconstructed_tables']:\n",
    "            first_table = prepared_document_data['reconstructed_tables'][0]\n",
    "            print(\"\\nPreview of first table:\")\n",
    "            print(f\"  Table ID: {first_table['table_id']}\")\n",
    "            print(f\"  Markdown Preview: {first_table.get('markdown_table', 'N/A')[:100]}...\")\n",
    "else:\n",
    "    print(\"\\n⚠️ Skipping example usage because 'all_results' is empty. Please run the ingestion cell first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3af7c4a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Agent initialized with Gemini model: gemma-3-27b-it\n",
      "✅ DocumentProcessor initialized.\n",
      "🎯 Searching for and processing specific document: download-financial-statements-pdf-ad7baa45\n",
      "🎯 Found document summary for ID: download-financial-statements-pdf-ad7baa45\n",
      "\n",
      "--- Preparing Document for Graph Extraction: download-financial-statements-pdf-ad7baa45 ---\n",
      "  -> Found 10 tables to process.\n",
      "\n",
      "--- Reconstructing Table # 0 (Markdown Format) ---\n",
      "  -> Divided into 5 chunks for processing.\n",
      "    -> Processing chunk 1/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~8 rows.\n",
      "      -> ⏱️ Waiting for 20.03 seconds before next chunk...\n",
      "    -> Processing chunk 2/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~13 rows.\n",
      "      -> ⏱️ Waiting for 20.99 seconds before next chunk...\n",
      "    -> Processing chunk 3/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~18 rows.\n",
      "      -> ⏱️ Waiting for 20.78 seconds before next chunk...\n",
      "    -> Processing chunk 4/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~22 rows.\n",
      "      -> ⏱️ Waiting for 21.47 seconds before next chunk...\n",
      "    -> Processing chunk 5/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~27 rows.\n",
      "  -> ✅ Table #0 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 1 (Markdown Format) ---\n",
      "  -> Divided into 8 chunks for processing.\n",
      "    -> Processing chunk 1/8...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~9 rows.\n",
      "      -> ⏱️ Waiting for 20.98 seconds before next chunk...\n",
      "    -> Processing chunk 2/8...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~14 rows.\n",
      "      -> ⏱️ Waiting for 20.75 seconds before next chunk...\n",
      "    -> Processing chunk 3/8...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~19 rows.\n",
      "      -> ⏱️ Waiting for 20.58 seconds before next chunk...\n",
      "    -> Processing chunk 4/8...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~25 rows.\n",
      "      -> ⏱️ Waiting for 20.26 seconds before next chunk...\n",
      "    -> Processing chunk 5/8...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~30 rows.\n",
      "      -> ⏱️ Waiting for 21.19 seconds before next chunk...\n",
      "    -> Processing chunk 6/8...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~35 rows.\n",
      "      -> ⏱️ Waiting for 20.21 seconds before next chunk...\n",
      "    -> Processing chunk 7/8...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~41 rows.\n",
      "      -> ⏱️ Waiting for 21.16 seconds before next chunk...\n",
      "    -> Processing chunk 8/8...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~45 rows.\n",
      "  -> ✅ Table #1 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 2 (Markdown Format) ---\n",
      "  -> Divided into 2 chunks for processing.\n",
      "    -> Processing chunk 1/2...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~9 rows.\n",
      "      -> ⏱️ Waiting for 20.61 seconds before next chunk...\n",
      "    -> Processing chunk 2/2...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~11 rows.\n",
      "  -> ✅ Table #2 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 3 (Markdown Format) ---\n",
      "  -> Divided into 6 chunks for processing.\n",
      "    -> Processing chunk 1/6...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~9 rows.\n",
      "      -> ⏱️ Waiting for 21.06 seconds before next chunk...\n",
      "    -> Processing chunk 2/6...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~14 rows.\n",
      "      -> ⏱️ Waiting for 20.42 seconds before next chunk...\n",
      "    -> Processing chunk 3/6...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~20 rows.\n",
      "      -> ⏱️ Waiting for 20.16 seconds before next chunk...\n",
      "    -> Processing chunk 4/6...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~26 rows.\n",
      "      -> ⏱️ Waiting for 21.62 seconds before next chunk...\n",
      "    -> Processing chunk 5/6...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~30 rows.\n",
      "      -> ⏱️ Waiting for 20.14 seconds before next chunk...\n",
      "    -> Processing chunk 6/6...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~32 rows.\n",
      "  -> ✅ Table #3 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 4 (Markdown Format) ---\n",
      "  -> Divided into 3 chunks for processing.\n",
      "    -> Processing chunk 1/3...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~8 rows.\n",
      "      -> ⏱️ Waiting for 21.83 seconds before next chunk...\n",
      "    -> Processing chunk 2/3...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~14 rows.\n",
      "      -> ⏱️ Waiting for 20.64 seconds before next chunk...\n",
      "    -> Processing chunk 3/3...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~18 rows.\n",
      "  -> ✅ Table #4 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 5 (Markdown Format) ---\n",
      "  -> Divided into 7 chunks for processing.\n",
      "    -> Processing chunk 1/7...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~7 rows.\n",
      "      -> ⏱️ Waiting for 21.11 seconds before next chunk...\n",
      "    -> Processing chunk 2/7...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~12 rows.\n",
      "      -> ⏱️ Waiting for 21.05 seconds before next chunk...\n",
      "    -> Processing chunk 3/7...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~19 rows.\n",
      "      -> ⏱️ Waiting for 20.52 seconds before next chunk...\n",
      "    -> Processing chunk 4/7...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~24 rows.\n",
      "      -> ⏱️ Waiting for 21.23 seconds before next chunk...\n",
      "    -> Processing chunk 5/7...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~29 rows.\n",
      "      -> ⏱️ Waiting for 20.75 seconds before next chunk...\n",
      "    -> Processing chunk 6/7...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~37 rows.\n",
      "      -> ⏱️ Waiting for 21.80 seconds before next chunk...\n",
      "    -> Processing chunk 7/7...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~38 rows.\n",
      "  -> ✅ Table #5 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 6 (Markdown Format) ---\n",
      "  -> Divided into 2 chunks for processing.\n",
      "    -> Processing chunk 1/2...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~7 rows.\n",
      "      -> ⏱️ Waiting for 20.67 seconds before next chunk...\n",
      "    -> Processing chunk 2/2...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~9 rows.\n",
      "  -> ✅ Table #6 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 7 (Markdown Format) ---\n",
      "  -> Divided into 3 chunks for processing.\n",
      "    -> Processing chunk 1/3...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~7 rows.\n",
      "      -> ⏱️ Waiting for 21.74 seconds before next chunk...\n",
      "    -> Processing chunk 2/3...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~13 rows.\n",
      "      -> ⏱️ Waiting for 20.86 seconds before next chunk...\n",
      "    -> Processing chunk 3/3...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~17 rows.\n",
      "  -> ✅ Table #7 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 8 (Markdown Format) ---\n",
      "  -> Divided into 5 chunks for processing.\n",
      "    -> Processing chunk 1/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~7 rows.\n",
      "      -> ⏱️ Waiting for 20.13 seconds before next chunk...\n",
      "    -> Processing chunk 2/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~12 rows.\n",
      "      -> ⏱️ Waiting for 21.62 seconds before next chunk...\n",
      "    -> Processing chunk 3/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~19 rows.\n",
      "      -> ⏱️ Waiting for 21.47 seconds before next chunk...\n",
      "    -> Processing chunk 4/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~25 rows.\n",
      "      -> ⏱️ Waiting for 21.34 seconds before next chunk...\n",
      "    -> Processing chunk 5/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~30 rows.\n",
      "  -> ✅ Table #8 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 9 (Markdown Format) ---\n",
      "  -> Divided into 6 chunks for processing.\n",
      "    -> Processing chunk 1/6...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~7 rows.\n",
      "      -> ⏱️ Waiting for 21.83 seconds before next chunk...\n",
      "    -> Processing chunk 2/6...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~12 rows.\n",
      "      -> ⏱️ Waiting for 20.43 seconds before next chunk...\n",
      "    -> Processing chunk 3/6...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~18 rows.\n",
      "      -> ⏱️ Waiting for 21.36 seconds before next chunk...\n",
      "    -> Processing chunk 4/6...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~25 rows.\n",
      "      -> ⏱️ Waiting for 21.15 seconds before next chunk...\n",
      "    -> Processing chunk 5/6...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~36 rows.\n",
      "      -> ⏱️ Waiting for 20.68 seconds before next chunk...\n",
      "    -> Processing chunk 6/6...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~39 rows.\n",
      "  -> ✅ Table #9 reconstruction completed!\n",
      "  -> Extracted 53 text chunks with metadata.\n",
      "  -> ✅ Document 'download-financial-statements-pdf-ad7baa45' prepared successfully.\n",
      "\n",
      "--- ✅ Processing Complete: Prepared Data Output ---\n",
      "Document ID: download-financial-statements-pdf-ad7baa45\n",
      "Tables Reconstructed: 10\n",
      "Text Chunks Found: 53\n",
      "  -> ✅ Saved prepared data to: i:\\My Drive\\M. Tech AI ML\\AIML SEM 4\\Dissertation\\Project\\output\\parsed_pdfs\\downloaded_verizon_quarterly_pdfs\\2024\\1Q\\download-financial-statements-pdf-ad7baa45\\download-financial-statements-pdf-ad7baa45.prepared.json\n",
      "\n",
      "Preview of first reconstructed table:\n",
      "  Table ID: download-financial-statements-pdf-ad7baa45_p1_t0\n",
      "  Page: 1\n",
      "  Markdown Preview: \n",
      "| Header 1 | 3 Mos. Ended 3/31/24 | 3 Mos. Ended 3/31/23 | % Change |\n",
      "|----------|-----------------------|-----------------------|----------|\n",
      "| Operating Revenues |                       |                       |          |\n",
      "| Service revenues and other | $ 27,620              | $ 27,152              | 1.7      |\n",
      "| Wireless equipment revenues | 5,361                 | 5,760                 | (6.9) ...\n"
     ]
    }
   ],
   "source": [
    "# 1. Initialize the agent that the processor will use\n",
    "table_recon_agent = TableReconstructionAgent()\n",
    "\n",
    "# 2. Initialize the processor\n",
    "doc_processor = DocumentProcessor(table_agent=table_recon_agent)\n",
    "\n",
    "# 3. Define the specific document to find and process\n",
    "TARGET_DOC_ID = \"download-financial-statements-pdf-ad7baa45\"\n",
    "print(f\"🎯 Searching for and processing specific document: {TARGET_DOC_ID}\")\n",
    "\n",
    "target_doc_summary = None\n",
    "if all_results:\n",
    "    # Find the specific document summary from the ingestion results\n",
    "    for summary in all_results:\n",
    "        if summary.get(\"doc_id\") == TARGET_DOC_ID:\n",
    "            target_doc_summary = summary\n",
    "            print(f\"🎯 Found document summary for ID: {TARGET_DOC_ID}\")\n",
    "            break # Found it, no need to search further\n",
    "\n",
    "    if target_doc_summary:\n",
    "        # Process the specific document we found\n",
    "        prepared_document_data = doc_processor.process_document(target_doc_summary)\n",
    "\n",
    "        # Print a summary of the prepared data\n",
    "        if prepared_document_data:\n",
    "            print(\"\\n--- ✅ Processing Complete: Prepared Data Output ---\")\n",
    "            print(f\"Document ID: {prepared_document_data['doc_id']}\")\n",
    "            print(f\"Tables Reconstructed: {len(prepared_document_data['reconstructed_tables'])}\")\n",
    "            print(f\"Text Chunks Found: {len(prepared_document_data['text_chunks'])}\")\n",
    "            \n",
    "            save_prepared_data(prepared_document_data, target_doc_summary)\n",
    "            \n",
    "            # Preview the first table's data\n",
    "            if prepared_document_data.get('reconstructed_tables'):\n",
    "                first_table = prepared_document_data['reconstructed_tables'][0]\n",
    "                print(\"\\nPreview of first reconstructed table:\")\n",
    "                print(f\"  Table ID: {first_table.get('table_id', 'N/A')}\")\n",
    "                print(f\"  Page: {first_table.get('table_metadata', {}).get('page', 'N/A')}\")\n",
    "                print(f\"  Markdown Preview: \\n{first_table.get('markdown_table', 'N/A')[:400]}...\")\n",
    "            else:\n",
    "                print(\"\\n⚠️ No tables were reconstructed for this document.\")\n",
    "    else:\n",
    "        print(f\"\\n❌ ERROR: Could not find document with ID '{TARGET_DOC_ID}' in 'all_results'.\")\n",
    "else:\n",
    "    print(\"\\n⚠️ Skipping example usage because 'all_results' is empty. Please run the ingestion cell first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7bf363fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 📊 Displaying Reconstructed Markdown Table ---\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "|               | 3/31/24  | 12/31/23 | $ Change | \n",
       "|----------------|----------|----------|----------|\n",
       "| Unaudited      |          |          |          |\n",
       "| Assets          |          |          |          |\n",
       "| Current assets  |          |          |          |\n",
       "| Cash and cash equivalents | $ 2,365  | $ 2,065  | $ 300   |\n",
       "| Accounts receivable | 26,380   | 26,102   | 278     |\n",
       "| Less Allowance for credit losses | 1,061    | 1,017    | 44      |\n",
       "| Accounts receivable, net | 25,319   | 25,085   | 234     |\n",
       "| Inventories     | 2,076    | 2,057    | 19      |\n",
       "| Prepaid expenses and other | 8,197    | 7,607    | 590     |\n",
       "| Total current assets | 37,957   | 36,814   | 1,143   |\n",
       "| Property, plant and equipment | 322,266  | 320,108  | 2,158   |\n",
       "| Less Accumulated depreciation | 322,266  | 320,108  | 2,158   |\n",
       "| Property, plant and equipment, net | 214,403  | 211,798  | 2,605   |\n",
       "| Investments in unconsolidated businesses | 941      | 953      | (12)    |\n",
       "| Wireless licenses | 156,111  | 155,667  | 444     |\n",
       "| Goodwill | 22,842   | 22,843   | (1)     |\n",
       "| Other intangible assets, net | 10,835   | 11,057   | (222)   |\n",
       "| Operating lease right-of-use assets | 24,351   | 24,726   | (375)   |\n",
       "| Other assets | 19,258   | 19,885   | (627)   |\n",
       "| Total assets | $ 380,158 | $ 380,255 | $ (97)   |\n",
       "| Liabilities and Equity |          |          |          |\n",
       "| Current liabilities |          |          |          |\n",
       "| Debt maturing within one year | $ 15,594 | $ 12,973 | $ 2,621   |\n",
       "| Accounts payable and accrued liabilities | 20,139   | 23,453   | (3,314)   |\n",
       "| Current operating lease liabilities | 4,282    | 4,266    | 16      |\n",
       "| Other current liabilities | 13,616   | 12,531   | 1,085   |\n",
       "| Total current liabilities | 53,631   | 53,223   | 408     |\n",
       "| Long-term debt | 136,104  | 137,701  | (1,597)   |\n",
       "| Employee benefit obligations | 12,805   | 13,189   | (384)   |\n",
       "| Deferred income taxes | 45,980   | 45,781   | 199     |\n",
       "| Non-current operating lease liabilities | 19,654   | 20,002   | (348)   |\n",
       "| Other liabilities | 16,258   | 16,560   | (302)   |\n",
       "| Total long-term liabilities | 230,801  | 233,233  | (2,432)   |\n",
       "| Equity |          |          |          |\n",
       "| Common stock | 429      | 429      | -       |\n",
       "| Additional paid in capital | 13,571   | 13,631   | (60)    |\n",
       "| Retained earnings | 84,714   | 82,915   | 1,799   |\n",
       "| Accumulated other comprehensive loss | (1,199)  | (1,380)  | 181     |\n",
       "| Common stock in treasury, at cost | (3,602)  |          |          |\n",
       "| Deferred compensation - employee stock ownership plans and other | 421      | 656      | (235)   |\n",
       "| Noncontrolling interests | 1,392    | 1,369    | 23      |\n",
       "| Total equity | 95,726   | 93,799   | 1,927   |\n",
       "| Total liabilities and equity | $ 380,158 | $ 380,255 | $ (97)   |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Assuming 'prepared_document_data' is available from the previous cell run\n",
    "if prepared_document_data and prepared_document_data.get('reconstructed_tables'):\n",
    "    first_table = prepared_document_data['reconstructed_tables'][1]\n",
    "    first_table_markdown = first_table.get('markdown_table', '*Markdown table not found.*')\n",
    "    \n",
    "    print(\"--- 📊 Displaying Reconstructed Markdown Table ---\")\n",
    "    # This will render the string as a visual markdown table\n",
    "    display(Markdown(first_table_markdown))\n",
    "else:\n",
    "    print(\"⚠️ No reconstructed table available to display.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbf2569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> ✅ Saved prepared data to: i:\\My Drive\\M. Tech AI ML\\AIML SEM 4\\Dissertation\\Project\\output\\parsed_pdfs\\downloaded_verizon_quarterly_pdfs\\2024\\1Q\\download-webcast-transcript-pdf-0773ea78\\download-financial-statements-pdf-ad7baa45.prepared.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'i:\\\\My Drive\\\\M. Tech AI ML\\\\AIML SEM 4\\\\Dissertation\\\\Project\\\\output\\\\parsed_pdfs\\\\downloaded_verizon_quarterly_pdfs\\\\2024\\\\1Q\\\\download-webcast-transcript-pdf-0773ea78\\\\download-financial-statements-pdf-ad7baa45.prepared.json'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_prepared_data(prepared_document_data, target_doc_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a96ce5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Agent initialized with Gemini model: gemma-3-27b-it\n",
      "✅ DocumentProcessor initialized.\n",
      "\n",
      "--- Starting Batch Processing for 44 Documents ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2657d7bbe37c4049a1d14a8731db0c77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Documents:   0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing document: download-financial-operating-information-pdf-2f954363\n",
      "\n",
      "--- Preparing Document for Graph Extraction: download-financial-operating-information-pdf-2f954363 ---\n",
      "  -> Found 20 tables to process.\n",
      "\n",
      "--- Reconstructing Table # 0 (Markdown Format) ---\n",
      "  -> Divided into 1 chunks for processing.\n",
      "    -> Processing chunk 1/1...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~1 rows.\n",
      "  -> ✅ Table #0 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 1 (Markdown Format) ---\n",
      "  -> Divided into 15 chunks for processing.\n",
      "    -> Processing chunk 1/15...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~6 rows.\n",
      "      -> ⏱️ Waiting for 34.93 seconds before next chunk...\n",
      "    -> Processing chunk 2/15...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~14 rows.\n",
      "      -> ⏱️ Waiting for 33.07 seconds before next chunk...\n",
      "    -> Processing chunk 3/15...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~18 rows.\n",
      "      -> ⏱️ Waiting for 34.87 seconds before next chunk...\n",
      "    -> Processing chunk 4/15...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~20 rows.\n",
      "      -> ⏱️ Waiting for 34.69 seconds before next chunk...\n",
      "    -> Processing chunk 5/15...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~24 rows.\n",
      "      -> ⏱️ Waiting for 34.00 seconds before next chunk...\n",
      "    -> Processing chunk 6/15...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~26 rows.\n",
      "      -> ⏱️ Waiting for 34.50 seconds before next chunk...\n",
      "    -> Processing chunk 7/15...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~28 rows.\n",
      "      -> ⏱️ Waiting for 34.10 seconds before next chunk...\n",
      "    -> Processing chunk 8/15...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~31 rows.\n",
      "      -> ⏱️ Waiting for 34.39 seconds before next chunk...\n",
      "    -> Processing chunk 9/15...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~39 rows.\n",
      "      -> ⏱️ Waiting for 34.82 seconds before next chunk...\n",
      "    -> Processing chunk 10/15...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~41 rows.\n",
      "      -> ⏱️ Waiting for 33.49 seconds before next chunk...\n",
      "    -> Processing chunk 11/15...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~54 rows.\n",
      "      -> ⏱️ Waiting for 33.79 seconds before next chunk...\n",
      "    -> Processing chunk 12/15...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~61 rows.\n",
      "      -> ⏱️ Waiting for 34.61 seconds before next chunk...\n",
      "    -> Processing chunk 13/15...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~61 rows.\n",
      "      -> ⏱️ Waiting for 33.78 seconds before next chunk...\n",
      "    -> Processing chunk 14/15...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~67 rows.\n",
      "      -> ⏱️ Waiting for 33.17 seconds before next chunk...\n",
      "    -> Processing chunk 15/15...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~73 rows.\n",
      "  -> ✅ Table #1 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 2 (Markdown Format) ---\n",
      "  -> Divided into 20 chunks for processing.\n",
      "    -> Processing chunk 1/20...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~5 rows.\n",
      "      -> ⏱️ Waiting for 34.03 seconds before next chunk...\n",
      "    -> Processing chunk 2/20...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~7 rows.\n",
      "      -> ⏱️ Waiting for 33.87 seconds before next chunk...\n",
      "    -> Processing chunk 3/20...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~9 rows.\n",
      "      -> ⏱️ Waiting for 33.70 seconds before next chunk...\n",
      "    -> Processing chunk 4/20...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~11 rows.\n",
      "      -> ⏱️ Waiting for 33.83 seconds before next chunk...\n",
      "    -> Processing chunk 5/20...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~13 rows.\n",
      "      -> ⏱️ Waiting for 34.03 seconds before next chunk...\n",
      "    -> Processing chunk 6/20...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~15 rows.\n",
      "      -> ⏱️ Waiting for 33.93 seconds before next chunk...\n",
      "    -> Processing chunk 7/20...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~17 rows.\n",
      "      -> ⏱️ Waiting for 34.07 seconds before next chunk...\n",
      "    -> Processing chunk 8/20...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~19 rows.\n",
      "      -> ⏱️ Waiting for 34.83 seconds before next chunk...\n",
      "    -> Processing chunk 9/20...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~21 rows.\n",
      "      -> ⏱️ Waiting for 34.10 seconds before next chunk...\n",
      "    -> Processing chunk 10/20...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~25 rows.\n",
      "      -> ⏱️ Waiting for 33.83 seconds before next chunk...\n",
      "    -> Processing chunk 11/20...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~27 rows.\n",
      "      -> ⏱️ Waiting for 33.33 seconds before next chunk...\n",
      "    -> Processing chunk 12/20...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~29 rows.\n",
      "      -> ⏱️ Waiting for 33.25 seconds before next chunk...\n",
      "    -> Processing chunk 13/20...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~31 rows.\n",
      "      -> ⏱️ Waiting for 33.12 seconds before next chunk...\n",
      "    -> Processing chunk 14/20...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~33 rows.\n",
      "      -> ⏱️ Waiting for 34.40 seconds before next chunk...\n",
      "    -> Processing chunk 15/20...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~37 rows.\n",
      "      -> ⏱️ Waiting for 34.47 seconds before next chunk...\n",
      "    -> Processing chunk 16/20...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~38 rows.\n",
      "      -> ⏱️ Waiting for 33.86 seconds before next chunk...\n",
      "    -> Processing chunk 17/20...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~40 rows.\n",
      "      -> ⏱️ Waiting for 33.57 seconds before next chunk...\n",
      "    -> Processing chunk 18/20...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~43 rows.\n",
      "      -> ⏱️ Waiting for 33.55 seconds before next chunk...\n",
      "    -> Processing chunk 19/20...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~45 rows.\n",
      "      -> ⏱️ Waiting for 33.56 seconds before next chunk...\n",
      "    -> Processing chunk 20/20...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~45 rows.\n",
      "  -> ✅ Table #2 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 3 (Markdown Format) ---\n",
      "  -> Divided into 5 chunks for processing.\n",
      "    -> Processing chunk 1/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~4 rows.\n",
      "      -> ⏱️ Waiting for 34.34 seconds before next chunk...\n",
      "    -> Processing chunk 2/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~6 rows.\n",
      "      -> ⏱️ Waiting for 34.79 seconds before next chunk...\n",
      "    -> Processing chunk 3/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~9 rows.\n",
      "      -> ⏱️ Waiting for 34.50 seconds before next chunk...\n",
      "    -> Processing chunk 4/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~10 rows.\n",
      "      -> ⏱️ Waiting for 34.82 seconds before next chunk...\n",
      "    -> Processing chunk 5/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~11 rows.\n",
      "  -> ✅ Table #3 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 4 (Markdown Format) ---\n",
      "  -> Divided into 15 chunks for processing.\n",
      "    -> Processing chunk 1/15...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~5 rows.\n",
      "      -> ⏱️ Waiting for 33.39 seconds before next chunk...\n",
      "    -> Processing chunk 2/15...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~10 rows.\n",
      "      -> ⏱️ Waiting for 34.13 seconds before next chunk...\n",
      "    -> Processing chunk 3/15...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~12 rows.\n",
      "      -> ⏱️ Waiting for 33.66 seconds before next chunk...\n",
      "    -> Processing chunk 4/15...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~16 rows.\n",
      "      -> ⏱️ Waiting for 33.42 seconds before next chunk...\n",
      "    -> Processing chunk 5/15...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~20 rows.\n",
      "      -> ⏱️ Waiting for 34.11 seconds before next chunk...\n",
      "    -> Processing chunk 6/15...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~22 rows.\n",
      "      -> ⏱️ Waiting for 34.64 seconds before next chunk...\n",
      "    -> Processing chunk 7/15...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~26 rows.\n",
      "      -> ⏱️ Waiting for 34.68 seconds before next chunk...\n",
      "    -> Processing chunk 8/15...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~30 rows.\n",
      "      -> ⏱️ Waiting for 34.96 seconds before next chunk...\n",
      "    -> Processing chunk 9/15...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~34 rows.\n",
      "      -> ⏱️ Waiting for 33.83 seconds before next chunk...\n",
      "    -> Processing chunk 10/15...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~36 rows.\n",
      "      -> ⏱️ Waiting for 34.95 seconds before next chunk...\n",
      "    -> Processing chunk 11/15...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~38 rows.\n",
      "      -> ⏱️ Waiting for 33.60 seconds before next chunk...\n",
      "    -> Processing chunk 12/15...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~41 rows.\n",
      "      -> ⏱️ Waiting for 34.78 seconds before next chunk...\n",
      "    -> Processing chunk 13/15...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~43 rows.\n",
      "      -> ⏱️ Waiting for 33.70 seconds before next chunk...\n",
      "    -> Processing chunk 14/15...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~47 rows.\n",
      "      -> ⏱️ Waiting for 33.35 seconds before next chunk...\n",
      "    -> Processing chunk 15/15...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~48 rows.\n",
      "  -> ✅ Table #4 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 5 (Markdown Format) ---\n",
      "  -> Divided into 7 chunks for processing.\n",
      "    -> Processing chunk 1/7...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~5 rows.\n",
      "      -> ⏱️ Waiting for 34.36 seconds before next chunk...\n",
      "    -> Processing chunk 2/7...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~7 rows.\n",
      "      -> ⏱️ Waiting for 33.54 seconds before next chunk...\n",
      "    -> Processing chunk 3/7...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~10 rows.\n",
      "      -> ⏱️ Waiting for 34.26 seconds before next chunk...\n",
      "    -> Processing chunk 4/7...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~13 rows.\n",
      "      -> ⏱️ Waiting for 33.80 seconds before next chunk...\n",
      "    -> Processing chunk 5/7...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~15 rows.\n",
      "      -> ⏱️ Waiting for 33.22 seconds before next chunk...\n",
      "    -> Processing chunk 6/7...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~18 rows.\n",
      "      -> ⏱️ Waiting for 33.96 seconds before next chunk...\n",
      "    -> Processing chunk 7/7...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~18 rows.\n",
      "  -> ✅ Table #5 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 6 (Markdown Format) ---\n",
      "  -> Divided into 17 chunks for processing.\n",
      "    -> Processing chunk 1/17...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~5 rows.\n",
      "      -> ⏱️ Waiting for 33.97 seconds before next chunk...\n",
      "    -> Processing chunk 2/17...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~8 rows.\n",
      "      -> ⏱️ Waiting for 34.77 seconds before next chunk...\n",
      "    -> Processing chunk 3/17...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~17 rows.\n",
      "      -> ⏱️ Waiting for 33.64 seconds before next chunk...\n",
      "    -> Processing chunk 4/17...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~24 rows.\n",
      "      -> ⏱️ Waiting for 33.50 seconds before next chunk...\n",
      "    -> Processing chunk 5/17...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~30 rows.\n",
      "      -> ⏱️ Waiting for 34.48 seconds before next chunk...\n",
      "    -> Processing chunk 6/17...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~38 rows.\n",
      "      -> ⏱️ Waiting for 33.86 seconds before next chunk...\n",
      "    -> Processing chunk 7/17...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~42 rows.\n",
      "      -> ⏱️ Waiting for 34.97 seconds before next chunk...\n",
      "    -> Processing chunk 8/17...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~45 rows.\n",
      "      -> ⏱️ Waiting for 34.22 seconds before next chunk...\n",
      "    -> Processing chunk 9/17...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~49 rows.\n",
      "      -> ⏱️ Waiting for 34.14 seconds before next chunk...\n",
      "    -> Processing chunk 10/17...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~55 rows.\n",
      "      -> ⏱️ Waiting for 34.58 seconds before next chunk...\n",
      "    -> Processing chunk 11/17...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~62 rows.\n",
      "      -> ⏱️ Waiting for 33.02 seconds before next chunk...\n",
      "    -> Processing chunk 12/17...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~2 rows.\n",
      "      -> ⏱️ Waiting for 34.94 seconds before next chunk...\n",
      "    -> Processing chunk 13/17...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~6 rows.\n",
      "      -> ⏱️ Waiting for 34.08 seconds before next chunk...\n",
      "    -> Processing chunk 14/17...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~9 rows.\n",
      "      -> ⏱️ Waiting for 34.07 seconds before next chunk...\n",
      "    -> Processing chunk 15/17...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~11 rows.\n",
      "      -> ⏱️ Waiting for 34.01 seconds before next chunk...\n",
      "    -> Processing chunk 16/17...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~13 rows.\n",
      "      -> ⏱️ Waiting for 34.40 seconds before next chunk...\n",
      "    -> Processing chunk 17/17...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~15 rows.\n",
      "  -> ✅ Table #6 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 7 (Markdown Format) ---\n",
      "  -> Divided into 7 chunks for processing.\n",
      "    -> Processing chunk 1/7...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~5 rows.\n",
      "      -> ⏱️ Waiting for 33.90 seconds before next chunk...\n",
      "    -> Processing chunk 2/7...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~9 rows.\n",
      "      -> ⏱️ Waiting for 33.99 seconds before next chunk...\n",
      "    -> Processing chunk 3/7...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~11 rows.\n",
      "      -> ⏱️ Waiting for 33.56 seconds before next chunk...\n",
      "    -> Processing chunk 4/7...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~13 rows.\n",
      "      -> ⏱️ Waiting for 34.14 seconds before next chunk...\n",
      "    -> Processing chunk 5/7...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~15 rows.\n",
      "      -> ⏱️ Waiting for 33.65 seconds before next chunk...\n",
      "    -> Processing chunk 6/7...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~17 rows.\n",
      "      -> ⏱️ Waiting for 33.44 seconds before next chunk...\n",
      "    -> Processing chunk 7/7...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~18 rows.\n",
      "  -> ✅ Table #7 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 8 (Markdown Format) ---\n",
      "  -> Divided into 11 chunks for processing.\n",
      "    -> Processing chunk 1/11...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~5 rows.\n",
      "      -> ⏱️ Waiting for 33.32 seconds before next chunk...\n",
      "    -> Processing chunk 2/11...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~8 rows.\n",
      "      -> ⏱️ Waiting for 33.58 seconds before next chunk...\n",
      "    -> Processing chunk 3/11...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~17 rows.\n",
      "      -> ⏱️ Waiting for 33.24 seconds before next chunk...\n",
      "    -> Processing chunk 4/11...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~27 rows.\n",
      "      -> ⏱️ Waiting for 34.29 seconds before next chunk...\n",
      "    -> Processing chunk 5/11...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~36 rows.\n",
      "      -> ⏱️ Waiting for 34.93 seconds before next chunk...\n",
      "    -> Processing chunk 6/11...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~44 rows.\n",
      "      -> ⏱️ Waiting for 33.08 seconds before next chunk...\n",
      "    -> Processing chunk 7/11...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~53 rows.\n",
      "      -> ⏱️ Waiting for 34.06 seconds before next chunk...\n",
      "    -> Processing chunk 8/11...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~62 rows.\n",
      "      -> ⏱️ Waiting for 33.50 seconds before next chunk...\n",
      "    -> Processing chunk 9/11...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~69 rows.\n",
      "      -> ⏱️ Waiting for 33.71 seconds before next chunk...\n",
      "    -> Processing chunk 10/11...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~73 rows.\n",
      "      -> ⏱️ Waiting for 33.95 seconds before next chunk...\n",
      "    -> Processing chunk 11/11...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~76 rows.\n",
      "  -> ✅ Table #8 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 9 (Markdown Format) ---\n",
      "  -> Divided into 13 chunks for processing.\n",
      "    -> Processing chunk 1/13...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~5 rows.\n",
      "      -> ⏱️ Waiting for 34.36 seconds before next chunk...\n",
      "    -> Processing chunk 2/13...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~8 rows.\n",
      "      -> ⏱️ Waiting for 34.00 seconds before next chunk...\n",
      "    -> Processing chunk 3/13...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~11 rows.\n",
      "      -> ⏱️ Waiting for 33.22 seconds before next chunk...\n",
      "    -> Processing chunk 4/13...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~14 rows.\n",
      "      -> ⏱️ Waiting for 34.48 seconds before next chunk...\n",
      "    -> Processing chunk 5/13...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~16 rows.\n",
      "      -> ⏱️ Waiting for 34.87 seconds before next chunk...\n",
      "    -> Processing chunk 6/13...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~19 rows.\n",
      "      -> ⏱️ Waiting for 33.82 seconds before next chunk...\n",
      "    -> Processing chunk 7/13...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~22 rows.\n",
      "      -> ⏱️ Waiting for 34.01 seconds before next chunk...\n",
      "    -> Processing chunk 8/13...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~26 rows.\n",
      "      -> ⏱️ Waiting for 34.80 seconds before next chunk...\n",
      "    -> Processing chunk 9/13...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~28 rows.\n",
      "      -> ⏱️ Waiting for 34.37 seconds before next chunk...\n",
      "    -> Processing chunk 10/13...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~32 rows.\n",
      "      -> ⏱️ Waiting for 33.08 seconds before next chunk...\n",
      "    -> Processing chunk 11/13...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~35 rows.\n",
      "      -> ⏱️ Waiting for 33.16 seconds before next chunk...\n",
      "    -> Processing chunk 12/13...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~37 rows.\n",
      "      -> ⏱️ Waiting for 34.15 seconds before next chunk...\n",
      "    -> Processing chunk 13/13...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~38 rows.\n",
      "  -> ✅ Table #9 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 10 (Markdown Format) ---\n",
      "  -> Divided into 11 chunks for processing.\n",
      "    -> Processing chunk 1/11...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~3 rows.\n",
      "      -> ⏱️ Waiting for 34.51 seconds before next chunk...\n",
      "    -> Processing chunk 2/11...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~7 rows.\n",
      "      -> ⏱️ Waiting for 34.84 seconds before next chunk...\n",
      "    -> Processing chunk 3/11...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~9 rows.\n",
      "      -> ⏱️ Waiting for 33.69 seconds before next chunk...\n",
      "    -> Processing chunk 4/11...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~13 rows.\n",
      "      -> ⏱️ Waiting for 34.59 seconds before next chunk...\n",
      "    -> Processing chunk 5/11...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~15 rows.\n",
      "      -> ⏱️ Waiting for 34.86 seconds before next chunk...\n",
      "    -> Processing chunk 6/11...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~17 rows.\n",
      "      -> ⏱️ Waiting for 33.33 seconds before next chunk...\n",
      "    -> Processing chunk 7/11...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~20 rows.\n",
      "      -> ⏱️ Waiting for 34.98 seconds before next chunk...\n",
      "    -> Processing chunk 8/11...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~23 rows.\n",
      "      -> ⏱️ Waiting for 34.78 seconds before next chunk...\n",
      "    -> Processing chunk 9/11...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~27 rows.\n",
      "      -> ⏱️ Waiting for 34.93 seconds before next chunk...\n",
      "    -> Processing chunk 10/11...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~33 rows.\n",
      "      -> ⏱️ Waiting for 33.88 seconds before next chunk...\n",
      "    -> Processing chunk 11/11...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~33 rows.\n",
      "  -> ✅ Table #10 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 11 (Markdown Format) ---\n",
      "  -> Divided into 5 chunks for processing.\n",
      "    -> Processing chunk 1/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~7 rows.\n",
      "      -> ⏱️ Waiting for 34.30 seconds before next chunk...\n",
      "    -> Processing chunk 2/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~11 rows.\n",
      "      -> ⏱️ Waiting for 33.07 seconds before next chunk...\n",
      "    -> Processing chunk 3/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~14 rows.\n",
      "      -> ⏱️ Waiting for 33.26 seconds before next chunk...\n",
      "    -> Processing chunk 4/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~17 rows.\n",
      "      -> ⏱️ Waiting for 33.80 seconds before next chunk...\n",
      "    -> Processing chunk 5/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~20 rows.\n",
      "  -> ✅ Table #11 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 12 (Markdown Format) ---\n",
      "  -> Divided into 6 chunks for processing.\n",
      "    -> Processing chunk 1/6...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~3 rows.\n",
      "      -> ⏱️ Waiting for 33.24 seconds before next chunk...\n",
      "    -> Processing chunk 2/6...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~5 rows.\n",
      "      -> ⏱️ Waiting for 33.77 seconds before next chunk...\n",
      "    -> Processing chunk 3/6...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~7 rows.\n",
      "      -> ⏱️ Waiting for 33.65 seconds before next chunk...\n",
      "    -> Processing chunk 4/6...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~10 rows.\n",
      "      -> ⏱️ Waiting for 34.60 seconds before next chunk...\n",
      "    -> Processing chunk 5/6...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~14 rows.\n",
      "      -> ⏱️ Waiting for 33.47 seconds before next chunk...\n",
      "    -> Processing chunk 6/6...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~18 rows.\n",
      "  -> ✅ Table #12 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 13 (Markdown Format) ---\n",
      "  -> Divided into 4 chunks for processing.\n",
      "    -> Processing chunk 1/4...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~8 rows.\n",
      "      -> ⏱️ Waiting for 34.41 seconds before next chunk...\n",
      "    -> Processing chunk 2/4...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~11 rows.\n",
      "      -> ⏱️ Waiting for 34.93 seconds before next chunk...\n",
      "    -> Processing chunk 3/4...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~20 rows.\n",
      "      -> ⏱️ Waiting for 33.70 seconds before next chunk...\n",
      "    -> Processing chunk 4/4...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~24 rows.\n",
      "  -> ✅ Table #13 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 14 (Markdown Format) ---\n",
      "  -> Divided into 5 chunks for processing.\n",
      "    -> Processing chunk 1/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~6 rows.\n",
      "      -> ⏱️ Waiting for 33.19 seconds before next chunk...\n",
      "    -> Processing chunk 2/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~10 rows.\n",
      "      -> ⏱️ Waiting for 34.27 seconds before next chunk...\n",
      "    -> Processing chunk 3/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~15 rows.\n",
      "      -> ⏱️ Waiting for 33.57 seconds before next chunk...\n",
      "    -> Processing chunk 4/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~19 rows.\n",
      "      -> ⏱️ Waiting for 33.41 seconds before next chunk...\n",
      "    -> Processing chunk 5/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~22 rows.\n",
      "  -> ✅ Table #14 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 15 (Markdown Format) ---\n",
      "  -> Divided into 2 chunks for processing.\n",
      "    -> Processing chunk 1/2...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~6 rows.\n",
      "      -> ⏱️ Waiting for 33.51 seconds before next chunk...\n",
      "    -> Processing chunk 2/2...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~9 rows.\n",
      "  -> ✅ Table #15 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 16 (Markdown Format) ---\n",
      "  -> Divided into 1 chunks for processing.\n",
      "    -> Processing chunk 1/1...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~6 rows.\n",
      "  -> ✅ Table #16 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 17 (Markdown Format) ---\n",
      "  -> Divided into 2 chunks for processing.\n",
      "    -> Processing chunk 1/2...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~9 rows.\n",
      "      -> ⏱️ Waiting for 34.52 seconds before next chunk...\n",
      "    -> Processing chunk 2/2...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~11 rows.\n",
      "  -> ✅ Table #17 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 18 (Markdown Format) ---\n",
      "  -> Divided into 5 chunks for processing.\n",
      "    -> Processing chunk 1/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~5 rows.\n",
      "      -> ⏱️ Waiting for 33.30 seconds before next chunk...\n",
      "    -> Processing chunk 2/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~7 rows.\n",
      "      -> ⏱️ Waiting for 33.24 seconds before next chunk...\n",
      "    -> Processing chunk 3/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~9 rows.\n",
      "      -> ⏱️ Waiting for 33.03 seconds before next chunk...\n",
      "    -> Processing chunk 4/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~10 rows.\n",
      "      -> ⏱️ Waiting for 34.23 seconds before next chunk...\n",
      "    -> Processing chunk 5/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~11 rows.\n",
      "  -> ✅ Table #18 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 19 (Markdown Format) ---\n",
      "  -> Divided into 5 chunks for processing.\n",
      "    -> Processing chunk 1/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~4 rows.\n",
      "      -> ⏱️ Waiting for 34.80 seconds before next chunk...\n",
      "    -> Processing chunk 2/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~6 rows.\n",
      "      -> ⏱️ Waiting for 34.22 seconds before next chunk...\n",
      "    -> Processing chunk 3/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~9 rows.\n",
      "      -> ⏱️ Waiting for 33.99 seconds before next chunk...\n",
      "    -> Processing chunk 4/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~10 rows.\n",
      "      -> ⏱️ Waiting for 33.31 seconds before next chunk...\n",
      "    -> Processing chunk 5/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~12 rows.\n",
      "  -> ✅ Table #19 reconstruction completed!\n",
      "  -> Extracted 135 text chunks with metadata.\n",
      "  -> ✅ Document 'download-financial-operating-information-pdf-2f954363' prepared successfully.\n",
      "  -> ✅ Saved prepared data to: i:\\My Drive\\M. Tech AI ML\\AIML SEM 4\\Dissertation\\Project\\output\\parsed_pdfs\\downloaded_verizon_quarterly_pdfs\\2024\\4Q\\download-financial-operating-information-pdf-2f954363\\download-financial-operating-information-pdf-2f954363.prepared.json\n",
      "\n",
      "Processing document: download-non-gaap-reconciliations-pdf-a98978fa\n",
      "\n",
      "--- Preparing Document for Graph Extraction: download-non-gaap-reconciliations-pdf-a98978fa ---\n",
      "  -> Found 10 tables to process.\n",
      "\n",
      "--- Reconstructing Table # 0 (Markdown Format) ---\n",
      "  -> Divided into 11 chunks for processing.\n",
      "    -> Processing chunk 1/11...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~3 rows.\n",
      "      -> ⏱️ Waiting for 34.38 seconds before next chunk...\n",
      "    -> Processing chunk 2/11...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~7 rows.\n",
      "      -> ⏱️ Waiting for 33.59 seconds before next chunk...\n",
      "    -> Processing chunk 3/11...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~10 rows.\n",
      "      -> ⏱️ Waiting for 33.73 seconds before next chunk...\n",
      "    -> Processing chunk 4/11...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~14 rows.\n",
      "      -> ⏱️ Waiting for 33.27 seconds before next chunk...\n",
      "    -> Processing chunk 5/11...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~16 rows.\n",
      "      -> ⏱️ Waiting for 34.32 seconds before next chunk...\n",
      "    -> Processing chunk 6/11...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~17 rows.\n",
      "      -> ⏱️ Waiting for 33.23 seconds before next chunk...\n",
      "    -> Processing chunk 7/11...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~18 rows.\n",
      "      -> ⏱️ Waiting for 33.31 seconds before next chunk...\n",
      "    -> Processing chunk 8/11...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~22 rows.\n",
      "      -> ⏱️ Waiting for 33.69 seconds before next chunk...\n",
      "    -> Processing chunk 9/11...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~24 rows.\n",
      "      -> ⏱️ Waiting for 34.57 seconds before next chunk...\n",
      "    -> Processing chunk 10/11...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~28 rows.\n",
      "      -> ⏱️ Waiting for 33.91 seconds before next chunk...\n",
      "    -> Processing chunk 11/11...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~32 rows.\n",
      "  -> ✅ Table #0 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 1 (Markdown Format) ---\n",
      "  -> Divided into 5 chunks for processing.\n",
      "    -> Processing chunk 1/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~7 rows.\n",
      "      -> ⏱️ Waiting for 34.23 seconds before next chunk...\n",
      "    -> Processing chunk 2/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~11 rows.\n",
      "      -> ⏱️ Waiting for 33.69 seconds before next chunk...\n",
      "    -> Processing chunk 3/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~14 rows.\n",
      "      -> ⏱️ Waiting for 33.25 seconds before next chunk...\n",
      "    -> Processing chunk 4/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~18 rows.\n",
      "      -> ⏱️ Waiting for 34.61 seconds before next chunk...\n",
      "    -> Processing chunk 5/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~21 rows.\n",
      "  -> ✅ Table #1 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 2 (Markdown Format) ---\n",
      "  -> Divided into 6 chunks for processing.\n",
      "    -> Processing chunk 1/6...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~3 rows.\n",
      "      -> ⏱️ Waiting for 34.75 seconds before next chunk...\n",
      "    -> Processing chunk 2/6...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~5 rows.\n",
      "      -> ⏱️ Waiting for 33.31 seconds before next chunk...\n",
      "    -> Processing chunk 3/6...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~7 rows.\n",
      "      -> ⏱️ Waiting for 33.25 seconds before next chunk...\n",
      "    -> Processing chunk 4/6...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~10 rows.\n",
      "      -> ⏱️ Waiting for 33.73 seconds before next chunk...\n",
      "    -> Processing chunk 5/6...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~14 rows.\n",
      "      -> ⏱️ Waiting for 33.15 seconds before next chunk...\n",
      "    -> Processing chunk 6/6...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~18 rows.\n",
      "  -> ✅ Table #2 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 3 (Markdown Format) ---\n",
      "  -> Divided into 4 chunks for processing.\n",
      "    -> Processing chunk 1/4...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~8 rows.\n",
      "      -> ⏱️ Waiting for 33.57 seconds before next chunk...\n",
      "    -> Processing chunk 2/4...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~14 rows.\n",
      "      -> ⏱️ Waiting for 33.59 seconds before next chunk...\n",
      "    -> Processing chunk 3/4...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~23 rows.\n",
      "      -> ⏱️ Waiting for 33.87 seconds before next chunk...\n",
      "    -> Processing chunk 4/4...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~27 rows.\n",
      "  -> ✅ Table #3 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 4 (Markdown Format) ---\n",
      "  -> Divided into 5 chunks for processing.\n",
      "    -> Processing chunk 1/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~7 rows.\n",
      "      -> ⏱️ Waiting for 34.82 seconds before next chunk...\n",
      "    -> Processing chunk 2/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~17 rows.\n",
      "      -> ⏱️ Waiting for 34.97 seconds before next chunk...\n",
      "    -> Processing chunk 3/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~27 rows.\n",
      "      -> ⏱️ Waiting for 33.48 seconds before next chunk...\n",
      "    -> Processing chunk 4/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~31 rows.\n",
      "      -> ⏱️ Waiting for 34.39 seconds before next chunk...\n",
      "    -> Processing chunk 5/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~34 rows.\n",
      "  -> ✅ Table #4 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 5 (Markdown Format) ---\n",
      "  -> Divided into 2 chunks for processing.\n",
      "    -> Processing chunk 1/2...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~6 rows.\n",
      "      -> ⏱️ Waiting for 33.39 seconds before next chunk...\n",
      "    -> Processing chunk 2/2...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~9 rows.\n",
      "  -> ✅ Table #5 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 6 (Markdown Format) ---\n",
      "  -> Divided into 1 chunks for processing.\n",
      "    -> Processing chunk 1/1...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~6 rows.\n",
      "  -> ✅ Table #6 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 7 (Markdown Format) ---\n",
      "  -> Divided into 2 chunks for processing.\n",
      "    -> Processing chunk 1/2...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~9 rows.\n",
      "      -> ⏱️ Waiting for 34.59 seconds before next chunk...\n",
      "    -> Processing chunk 2/2...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~11 rows.\n",
      "  -> ✅ Table #7 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 8 (Markdown Format) ---\n",
      "  -> Divided into 5 chunks for processing.\n",
      "    -> Processing chunk 1/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~5 rows.\n",
      "      -> ⏱️ Waiting for 33.58 seconds before next chunk...\n",
      "    -> Processing chunk 2/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~7 rows.\n",
      "      -> ⏱️ Waiting for 33.76 seconds before next chunk...\n",
      "    -> Processing chunk 3/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~9 rows.\n",
      "      -> ⏱️ Waiting for 34.25 seconds before next chunk...\n",
      "    -> Processing chunk 4/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~10 rows.\n",
      "      -> ⏱️ Waiting for 34.32 seconds before next chunk...\n",
      "    -> Processing chunk 5/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~11 rows.\n",
      "  -> ✅ Table #8 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 9 (Markdown Format) ---\n",
      "  -> Divided into 5 chunks for processing.\n",
      "    -> Processing chunk 1/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~4 rows.\n",
      "      -> ⏱️ Waiting for 34.25 seconds before next chunk...\n",
      "    -> Processing chunk 2/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~6 rows.\n",
      "      -> ⏱️ Waiting for 34.25 seconds before next chunk...\n",
      "    -> Processing chunk 3/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~8 rows.\n",
      "      -> ⏱️ Waiting for 33.82 seconds before next chunk...\n",
      "    -> Processing chunk 4/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~9 rows.\n",
      "      -> ⏱️ Waiting for 34.21 seconds before next chunk...\n",
      "    -> Processing chunk 5/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~10 rows.\n",
      "  -> ✅ Table #9 reconstruction completed!\n",
      "  -> Extracted 73 text chunks with metadata.\n",
      "  -> ✅ Document 'download-non-gaap-reconciliations-pdf-a98978fa' prepared successfully.\n",
      "  -> ✅ Saved prepared data to: i:\\My Drive\\M. Tech AI ML\\AIML SEM 4\\Dissertation\\Project\\output\\parsed_pdfs\\downloaded_verizon_quarterly_pdfs\\2024\\4Q\\download-non-gaap-reconciliations-pdf-a98978fa\\download-non-gaap-reconciliations-pdf-a98978fa.prepared.json\n",
      "\n",
      "Processing document: download-non-gaap-reconciliations-pdf-de6a19a8\n",
      "\n",
      "--- Preparing Document for Graph Extraction: download-non-gaap-reconciliations-pdf-de6a19a8 ---\n",
      "  -> Found 4 tables to process.\n",
      "\n",
      "--- Reconstructing Table # 0 (Markdown Format) ---\n",
      "  -> Divided into 2 chunks for processing.\n",
      "    -> Processing chunk 1/2...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~7 rows.\n",
      "      -> ⏱️ Waiting for 33.96 seconds before next chunk...\n",
      "    -> Processing chunk 2/2...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~9 rows.\n",
      "  -> ✅ Table #0 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 1 (Markdown Format) ---\n",
      "  -> Divided into 3 chunks for processing.\n",
      "    -> Processing chunk 1/3...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~7 rows.\n",
      "      -> ⏱️ Waiting for 33.28 seconds before next chunk...\n",
      "    -> Processing chunk 2/3...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~13 rows.\n",
      "      -> ⏱️ Waiting for 33.08 seconds before next chunk...\n",
      "    -> Processing chunk 3/3...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~15 rows.\n",
      "  -> ✅ Table #1 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 2 (Markdown Format) ---\n",
      "  -> Divided into 3 chunks for processing.\n",
      "    -> Processing chunk 1/3...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~7 rows.\n",
      "      -> ⏱️ Waiting for 34.00 seconds before next chunk...\n",
      "    -> Processing chunk 2/3...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~13 rows.\n",
      "      -> ⏱️ Waiting for 33.43 seconds before next chunk...\n",
      "    -> Processing chunk 3/3...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~14 rows.\n",
      "  -> ✅ Table #2 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 3 (Markdown Format) ---\n",
      "  -> Divided into 3 chunks for processing.\n",
      "    -> Processing chunk 1/3...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~7 rows.\n",
      "      -> ⏱️ Waiting for 33.34 seconds before next chunk...\n",
      "    -> Processing chunk 2/3...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~13 rows.\n",
      "      -> ⏱️ Waiting for 34.03 seconds before next chunk...\n",
      "    -> Processing chunk 3/3...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~16 rows.\n",
      "  -> ✅ Table #3 reconstruction completed!\n",
      "  -> Extracted 26 text chunks with metadata.\n",
      "  -> ✅ Document 'download-non-gaap-reconciliations-pdf-de6a19a8' prepared successfully.\n",
      "  -> ✅ Saved prepared data to: i:\\My Drive\\M. Tech AI ML\\AIML SEM 4\\Dissertation\\Project\\output\\parsed_pdfs\\downloaded_verizon_quarterly_pdfs\\2024\\4Q\\download-non-gaap-reconciliations-pdf-de6a19a8\\download-non-gaap-reconciliations-pdf-de6a19a8.prepared.json\n",
      "\n",
      "Processing document: download-webcast-transcript-pdf-f011dc37\n",
      "\n",
      "--- Preparing Document for Graph Extraction: download-webcast-transcript-pdf-f011dc37 ---\n",
      "  -> No tables found in this document.\n",
      "  -> Extracted 400 text chunks with metadata.\n",
      "  -> ✅ Document 'download-webcast-transcript-pdf-f011dc37' prepared successfully.\n",
      "  -> ✅ Saved prepared data to: i:\\My Drive\\M. Tech AI ML\\AIML SEM 4\\Dissertation\\Project\\output\\parsed_pdfs\\downloaded_verizon_quarterly_pdfs\\2025\\1Q\\download-webcast-transcript-pdf-f011dc37\\download-webcast-transcript-pdf-f011dc37.prepared.json\n",
      "\n",
      "Processing document: download-webcast-presentation-pdf-da5ae256\n",
      "\n",
      "--- Preparing Document for Graph Extraction: download-webcast-presentation-pdf-da5ae256 ---\n",
      "  -> Found 1 tables to process.\n",
      "\n",
      "--- Reconstructing Table # 0 (Markdown Format) ---\n",
      "  -> Divided into 1 chunks for processing.\n",
      "    -> Processing chunk 1/1...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~7 rows.\n",
      "  -> ✅ Table #0 reconstruction completed!\n",
      "  -> Extracted 216 text chunks with metadata.\n",
      "  -> ✅ Document 'download-webcast-presentation-pdf-da5ae256' prepared successfully.\n",
      "  -> ✅ Saved prepared data to: i:\\My Drive\\M. Tech AI ML\\AIML SEM 4\\Dissertation\\Project\\output\\parsed_pdfs\\downloaded_verizon_quarterly_pdfs\\2025\\1Q\\download-webcast-presentation-pdf-da5ae256\\download-webcast-presentation-pdf-da5ae256.prepared.json\n",
      "\n",
      "Processing document: download-earnings-presentation-pdf-be525a5d\n",
      "\n",
      "--- Preparing Document for Graph Extraction: download-earnings-presentation-pdf-be525a5d ---\n",
      "  -> Found 2 tables to process.\n",
      "\n",
      "--- Reconstructing Table # 0 (Markdown Format) ---\n",
      "  -> Divided into 1 chunks for processing.\n",
      "    -> Processing chunk 1/1...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~7 rows.\n",
      "  -> ✅ Table #0 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 1 (Markdown Format) ---\n",
      "  -> Divided into 2 chunks for processing.\n",
      "    -> Processing chunk 1/2...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~10 rows.\n",
      "      -> ⏱️ Waiting for 34.62 seconds before next chunk...\n",
      "    -> Processing chunk 2/2...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~12 rows.\n",
      "  -> ✅ Table #1 reconstruction completed!\n",
      "  -> Extracted 120 text chunks with metadata.\n",
      "  -> ✅ Document 'download-earnings-presentation-pdf-be525a5d' prepared successfully.\n",
      "  -> ✅ Saved prepared data to: i:\\My Drive\\M. Tech AI ML\\AIML SEM 4\\Dissertation\\Project\\output\\parsed_pdfs\\downloaded_verizon_quarterly_pdfs\\2025\\1Q\\download-earnings-presentation-pdf-be525a5d\\download-earnings-presentation-pdf-be525a5d.prepared.json\n",
      "\n",
      "Processing document: download-prepared-earnings-transcript-pdf-15df53b4\n",
      "\n",
      "--- Preparing Document for Graph Extraction: download-prepared-earnings-transcript-pdf-15df53b4 ---\n",
      "  -> No tables found in this document.\n",
      "  -> Extracted 87 text chunks with metadata.\n",
      "  -> ✅ Document 'download-prepared-earnings-transcript-pdf-15df53b4' prepared successfully.\n",
      "  -> ✅ Saved prepared data to: i:\\My Drive\\M. Tech AI ML\\AIML SEM 4\\Dissertation\\Project\\output\\parsed_pdfs\\downloaded_verizon_quarterly_pdfs\\2025\\1Q\\download-prepared-earnings-transcript-pdf-15df53b4\\download-prepared-earnings-transcript-pdf-15df53b4.prepared.json\n",
      "\n",
      "Processing document: download-infographic-pdf-30c3d8e4\n",
      "\n",
      "--- Preparing Document for Graph Extraction: download-infographic-pdf-30c3d8e4 ---\n",
      "  -> No tables found in this document.\n",
      "  -> Extracted 19 text chunks with metadata.\n",
      "  -> ✅ Document 'download-infographic-pdf-30c3d8e4' prepared successfully.\n",
      "  -> ✅ Saved prepared data to: i:\\My Drive\\M. Tech AI ML\\AIML SEM 4\\Dissertation\\Project\\output\\parsed_pdfs\\downloaded_verizon_quarterly_pdfs\\2025\\1Q\\download-infographic-pdf-30c3d8e4\\download-infographic-pdf-30c3d8e4.prepared.json\n",
      "\n",
      "Processing document: download-financial-statements-pdf-5a9b6fa4\n",
      "\n",
      "--- Preparing Document for Graph Extraction: download-financial-statements-pdf-5a9b6fa4 ---\n",
      "  -> Found 11 tables to process.\n",
      "\n",
      "--- Reconstructing Table # 0 (Markdown Format) ---\n",
      "  -> Divided into 5 chunks for processing.\n",
      "    -> Processing chunk 1/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~9 rows.\n",
      "      -> ⏱️ Waiting for 34.66 seconds before next chunk...\n",
      "    -> Processing chunk 2/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~14 rows.\n",
      "      -> ⏱️ Waiting for 33.22 seconds before next chunk...\n",
      "    -> Processing chunk 3/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~19 rows.\n",
      "      -> ⏱️ Waiting for 34.66 seconds before next chunk...\n",
      "    -> Processing chunk 4/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~23 rows.\n",
      "      -> ⏱️ Waiting for 34.12 seconds before next chunk...\n",
      "    -> Processing chunk 5/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~28 rows.\n",
      "  -> ✅ Table #0 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 1 (Markdown Format) ---\n",
      "  -> Divided into 8 chunks for processing.\n",
      "    -> Processing chunk 1/8...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~9 rows.\n",
      "      -> ⏱️ Waiting for 34.78 seconds before next chunk...\n",
      "    -> Processing chunk 2/8...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~14 rows.\n",
      "      -> ⏱️ Waiting for 33.85 seconds before next chunk...\n",
      "    -> Processing chunk 3/8...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~19 rows.\n",
      "      -> ⏱️ Waiting for 33.89 seconds before next chunk...\n",
      "    -> Processing chunk 4/8...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~25 rows.\n",
      "      -> ⏱️ Waiting for 34.47 seconds before next chunk...\n",
      "    -> Processing chunk 5/8...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~30 rows.\n",
      "      -> ⏱️ Waiting for 34.82 seconds before next chunk...\n",
      "    -> Processing chunk 6/8...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~35 rows.\n",
      "      -> ⏱️ Waiting for 33.96 seconds before next chunk...\n",
      "    -> Processing chunk 7/8...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~41 rows.\n",
      "      -> ⏱️ Waiting for 34.33 seconds before next chunk...\n",
      "    -> Processing chunk 8/8...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~45 rows.\n",
      "  -> ✅ Table #1 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 2 (Markdown Format) ---\n",
      "  -> Divided into 2 chunks for processing.\n",
      "    -> Processing chunk 1/2...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~10 rows.\n",
      "      -> ⏱️ Waiting for 34.70 seconds before next chunk...\n",
      "    -> Processing chunk 2/2...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~12 rows.\n",
      "  -> ✅ Table #2 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 3 (Markdown Format) ---\n",
      "  -> Divided into 6 chunks for processing.\n",
      "    -> Processing chunk 1/6...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~9 rows.\n",
      "      -> ⏱️ Waiting for 33.52 seconds before next chunk...\n",
      "    -> Processing chunk 2/6...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~14 rows.\n",
      "      -> ⏱️ Waiting for 33.78 seconds before next chunk...\n",
      "    -> Processing chunk 3/6...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~19 rows.\n",
      "      -> ⏱️ Waiting for 34.14 seconds before next chunk...\n",
      "    -> Processing chunk 4/6...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~25 rows.\n",
      "      -> ⏱️ Waiting for 34.85 seconds before next chunk...\n",
      "    -> Processing chunk 5/6...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~30 rows.\n",
      "      -> ⏱️ Waiting for 34.11 seconds before next chunk...\n",
      "    -> Processing chunk 6/6...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~30 rows.\n",
      "  -> ✅ Table #3 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 4 (Markdown Format) ---\n",
      "  -> Divided into 3 chunks for processing.\n",
      "    -> Processing chunk 1/3...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~8 rows.\n",
      "      -> ⏱️ Waiting for 33.25 seconds before next chunk...\n",
      "    -> Processing chunk 2/3...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~14 rows.\n",
      "      -> ⏱️ Waiting for 34.97 seconds before next chunk...\n",
      "    -> Processing chunk 3/3...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~18 rows.\n",
      "  -> ✅ Table #4 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 5 (Markdown Format) ---\n",
      "  -> Divided into 6 chunks for processing.\n",
      "    -> Processing chunk 1/6...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~8 rows.\n",
      "      -> ⏱️ Waiting for 34.53 seconds before next chunk...\n",
      "    -> Processing chunk 2/6...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~13 rows.\n",
      "      -> ⏱️ Waiting for 34.34 seconds before next chunk...\n",
      "    -> Processing chunk 3/6...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~19 rows.\n",
      "      -> ⏱️ Waiting for 34.97 seconds before next chunk...\n",
      "    -> Processing chunk 4/6...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~24 rows.\n",
      "      -> ⏱️ Waiting for 33.55 seconds before next chunk...\n",
      "    -> Processing chunk 5/6...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~30 rows.\n",
      "      -> ⏱️ Waiting for 34.81 seconds before next chunk...\n",
      "    -> Processing chunk 6/6...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~34 rows.\n",
      "  -> ✅ Table #5 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 6 (Markdown Format) ---\n",
      "  -> Divided into 2 chunks for processing.\n",
      "    -> Processing chunk 1/2...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~8 rows.\n",
      "      -> ⏱️ Waiting for 34.04 seconds before next chunk...\n",
      "    -> Processing chunk 2/2...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~9 rows.\n",
      "  -> ✅ Table #6 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 7 (Markdown Format) ---\n",
      "  -> Divided into 3 chunks for processing.\n",
      "    -> Processing chunk 1/3...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~8 rows.\n",
      "      -> ⏱️ Waiting for 34.31 seconds before next chunk...\n",
      "    -> Processing chunk 2/3...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~14 rows.\n",
      "      -> ⏱️ Waiting for 34.87 seconds before next chunk...\n",
      "    -> Processing chunk 3/3...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~18 rows.\n",
      "  -> ✅ Table #7 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 8 (Markdown Format) ---\n",
      "  -> Divided into 5 chunks for processing.\n",
      "    -> Processing chunk 1/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~7 rows.\n",
      "      -> ⏱️ Waiting for 33.73 seconds before next chunk...\n",
      "    -> Processing chunk 2/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~12 rows.\n",
      "      -> ⏱️ Waiting for 33.09 seconds before next chunk...\n",
      "    -> Processing chunk 3/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~18 rows.\n",
      "      -> ⏱️ Waiting for 33.50 seconds before next chunk...\n",
      "    -> Processing chunk 4/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~24 rows.\n",
      "      -> ⏱️ Waiting for 34.23 seconds before next chunk...\n",
      "    -> Processing chunk 5/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~29 rows.\n",
      "  -> ✅ Table #8 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 9 (Markdown Format) ---\n",
      "  -> Divided into 2 chunks for processing.\n",
      "    -> Processing chunk 1/2...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~7 rows.\n",
      "      -> ⏱️ Waiting for 33.15 seconds before next chunk...\n",
      "    -> Processing chunk 2/2...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~7 rows.\n",
      "  -> ✅ Table #9 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 10 (Markdown Format) ---\n",
      "  -> Divided into 4 chunks for processing.\n",
      "    -> Processing chunk 1/4...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~7 rows.\n",
      "      -> ⏱️ Waiting for 34.50 seconds before next chunk...\n",
      "    -> Processing chunk 2/4...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~16 rows.\n",
      "      -> ⏱️ Waiting for 33.60 seconds before next chunk...\n",
      "    -> Processing chunk 3/4...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~24 rows.\n",
      "      -> ⏱️ Waiting for 33.03 seconds before next chunk...\n",
      "    -> Processing chunk 4/4...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~28 rows.\n",
      "  -> ✅ Table #10 reconstruction completed!\n",
      "  -> Extracted 63 text chunks with metadata.\n",
      "  -> ✅ Document 'download-financial-statements-pdf-5a9b6fa4' prepared successfully.\n",
      "  -> ✅ Saved prepared data to: i:\\My Drive\\M. Tech AI ML\\AIML SEM 4\\Dissertation\\Project\\output\\parsed_pdfs\\downloaded_verizon_quarterly_pdfs\\2025\\1Q\\download-financial-statements-pdf-5a9b6fa4\\download-financial-statements-pdf-5a9b6fa4.prepared.json\n",
      "\n",
      "Processing document: download-financial-operating-information-pdf-3dedf514\n",
      "\n",
      "--- Preparing Document for Graph Extraction: download-financial-operating-information-pdf-3dedf514 ---\n",
      "  -> Found 18 tables to process.\n",
      "\n",
      "--- Reconstructing Table # 0 (Markdown Format) ---\n",
      "  -> Divided into 1 chunks for processing.\n",
      "    -> Processing chunk 1/1...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~1 rows.\n",
      "  -> ✅ Table #0 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 1 (Markdown Format) ---\n",
      "  -> Divided into 12 chunks for processing.\n",
      "    -> Processing chunk 1/12...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~7 rows.\n",
      "      -> ⏱️ Waiting for 33.93 seconds before next chunk...\n",
      "    -> Processing chunk 2/12...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~9 rows.\n",
      "      -> ⏱️ Waiting for 33.37 seconds before next chunk...\n",
      "    -> Processing chunk 3/12...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~13 rows.\n",
      "      -> ⏱️ Waiting for 34.38 seconds before next chunk...\n",
      "    -> Processing chunk 4/12...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~16 rows.\n",
      "      -> ⏱️ Waiting for 34.56 seconds before next chunk...\n",
      "    -> Processing chunk 5/12...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~18 rows.\n",
      "      -> ⏱️ Waiting for 34.44 seconds before next chunk...\n",
      "    -> Processing chunk 6/12...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~21 rows.\n",
      "      -> ⏱️ Waiting for 33.69 seconds before next chunk...\n",
      "    -> Processing chunk 7/12...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~24 rows.\n",
      "      -> ⏱️ Waiting for 33.12 seconds before next chunk...\n",
      "    -> Processing chunk 8/12...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~27 rows.\n",
      "      -> ⏱️ Waiting for 34.62 seconds before next chunk...\n",
      "    -> Processing chunk 9/12...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~29 rows.\n",
      "      -> ⏱️ Waiting for 33.84 seconds before next chunk...\n",
      "    -> Processing chunk 10/12...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~33 rows.\n",
      "      -> ⏱️ Waiting for 33.19 seconds before next chunk...\n",
      "    -> Processing chunk 11/12...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~36 rows.\n",
      "      -> ⏱️ Waiting for 33.78 seconds before next chunk...\n",
      "    -> Processing chunk 12/12...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~36 rows.\n",
      "  -> ✅ Table #1 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 2 (Markdown Format) ---\n",
      "  -> Divided into 14 chunks for processing.\n",
      "    -> Processing chunk 1/14...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~6 rows.\n",
      "      -> ⏱️ Waiting for 33.65 seconds before next chunk...\n",
      "    -> Processing chunk 2/14...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~10 rows.\n",
      "      -> ⏱️ Waiting for 33.81 seconds before next chunk...\n",
      "    -> Processing chunk 3/14...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~13 rows.\n",
      "      -> ⏱️ Waiting for 34.73 seconds before next chunk...\n",
      "    -> Processing chunk 4/14...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~16 rows.\n",
      "      -> ⏱️ Waiting for 34.09 seconds before next chunk...\n",
      "    -> Processing chunk 5/14...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~19 rows.\n",
      "      -> ⏱️ Waiting for 34.74 seconds before next chunk...\n",
      "    -> Processing chunk 6/14...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~22 rows.\n",
      "      -> ⏱️ Waiting for 33.55 seconds before next chunk...\n",
      "    -> Processing chunk 7/14...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~27 rows.\n",
      "      -> ⏱️ Waiting for 34.18 seconds before next chunk...\n",
      "    -> Processing chunk 8/14...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~30 rows.\n",
      "      -> ⏱️ Waiting for 33.62 seconds before next chunk...\n",
      "    -> Processing chunk 9/14...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~33 rows.\n",
      "      -> ⏱️ Waiting for 34.95 seconds before next chunk...\n",
      "    -> Processing chunk 10/14...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~35 rows.\n",
      "      -> ⏱️ Waiting for 34.07 seconds before next chunk...\n",
      "    -> Processing chunk 11/14...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~39 rows.\n",
      "      -> ⏱️ Waiting for 34.40 seconds before next chunk...\n",
      "    -> Processing chunk 12/14...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~42 rows.\n",
      "      -> ⏱️ Waiting for 33.44 seconds before next chunk...\n",
      "    -> Processing chunk 13/14...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~45 rows.\n",
      "      -> ⏱️ Waiting for 33.31 seconds before next chunk...\n",
      "    -> Processing chunk 14/14...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "❌ ERROR: JSON parsing failed: Extra data: line 3 column 4 (char 4082)\n",
      "Corrupted JSON content preview: {\n",
      "  \"markdown_table\": \"|               | 12/31/23 | 3/31/24 | 6/30/24 | 9/30/24 | 12/31/24 | 3/31/25 |\\n|----------------|----------|----------|----------|----------|----------|----------|\\n| Assets         |          |          |          |          |          |          |\\n| Current assets |          |          |          |          |          |          |\\n| Cash and cash equivalents | $ 2,065 | $ 2,365 | $ 2,432 | $ 4,987 | $ 4,194 | $ 2,257 |\\n| Accounts receivable | 26,102 | 26,380 | 26,70\n",
      "      -> ⚠️ Skipping update due to invalid response from LLM.\n",
      "  -> ✅ Table #2 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 3 (Markdown Format) ---\n",
      "  -> Divided into 4 chunks for processing.\n",
      "    -> Processing chunk 1/4...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~4 rows.\n",
      "      -> ⏱️ Waiting for 34.07 seconds before next chunk...\n",
      "    -> Processing chunk 2/4...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~8 rows.\n",
      "      -> ⏱️ Waiting for 33.64 seconds before next chunk...\n",
      "    -> Processing chunk 3/4...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~12 rows.\n",
      "      -> ⏱️ Waiting for 34.96 seconds before next chunk...\n",
      "    -> Processing chunk 4/4...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~12 rows.\n",
      "  -> ✅ Table #3 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 4 (Markdown Format) ---\n",
      "  -> Divided into 10 chunks for processing.\n",
      "    -> Processing chunk 1/10...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~7 rows.\n",
      "      -> ⏱️ Waiting for 34.06 seconds before next chunk...\n",
      "    -> Processing chunk 2/10...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~10 rows.\n",
      "      -> ⏱️ Waiting for 34.65 seconds before next chunk...\n",
      "    -> Processing chunk 3/10...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~13 rows.\n",
      "      -> ⏱️ Waiting for 33.58 seconds before next chunk...\n",
      "    -> Processing chunk 4/10...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~16 rows.\n",
      "      -> ⏱️ Waiting for 33.54 seconds before next chunk...\n",
      "    -> Processing chunk 5/10...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~19 rows.\n",
      "      -> ⏱️ Waiting for 33.81 seconds before next chunk...\n",
      "    -> Processing chunk 6/10...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~23 rows.\n",
      "      -> ⏱️ Waiting for 33.40 seconds before next chunk...\n",
      "    -> Processing chunk 7/10...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~26 rows.\n",
      "      -> ⏱️ Waiting for 34.87 seconds before next chunk...\n",
      "    -> Processing chunk 8/10...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~29 rows.\n",
      "      -> ⏱️ Waiting for 33.66 seconds before next chunk...\n",
      "    -> Processing chunk 9/10...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~32 rows.\n",
      "      -> ⏱️ Waiting for 34.57 seconds before next chunk...\n",
      "    -> Processing chunk 10/10...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~32 rows.\n",
      "  -> ✅ Table #4 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 5 (Markdown Format) ---\n",
      "  -> Divided into 5 chunks for processing.\n",
      "    -> Processing chunk 1/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~6 rows.\n",
      "      -> ⏱️ Waiting for 33.88 seconds before next chunk...\n",
      "    -> Processing chunk 2/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~10 rows.\n",
      "      -> ⏱️ Waiting for 33.66 seconds before next chunk...\n",
      "    -> Processing chunk 3/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~14 rows.\n",
      "      -> ⏱️ Waiting for 34.18 seconds before next chunk...\n",
      "    -> Processing chunk 4/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~17 rows.\n",
      "      -> ⏱️ Waiting for 34.74 seconds before next chunk...\n",
      "    -> Processing chunk 5/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~19 rows.\n",
      "  -> ✅ Table #5 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 6 (Markdown Format) ---\n",
      "  -> Divided into 10 chunks for processing.\n",
      "    -> Processing chunk 1/10...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~6 rows.\n",
      "      -> ⏱️ Waiting for 33.98 seconds before next chunk...\n",
      "    -> Processing chunk 2/10...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~10 rows.\n",
      "      -> ⏱️ Waiting for 34.13 seconds before next chunk...\n",
      "    -> Processing chunk 3/10...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~14 rows.\n",
      "      -> ⏱️ Waiting for 34.36 seconds before next chunk...\n",
      "    -> Processing chunk 4/10...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~22 rows.\n",
      "      -> ⏱️ Waiting for 33.53 seconds before next chunk...\n",
      "    -> Processing chunk 5/10...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~25 rows.\n",
      "      -> ⏱️ Waiting for 33.13 seconds before next chunk...\n",
      "    -> Processing chunk 6/10...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~29 rows.\n",
      "      -> ⏱️ Waiting for 33.89 seconds before next chunk...\n",
      "    -> Processing chunk 7/10...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~35 rows.\n",
      "      -> ⏱️ Waiting for 34.13 seconds before next chunk...\n",
      "    -> Processing chunk 8/10...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~39 rows.\n",
      "      -> ⏱️ Waiting for 33.07 seconds before next chunk...\n",
      "    -> Processing chunk 9/10...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~46 rows.\n",
      "      -> ⏱️ Waiting for 34.60 seconds before next chunk...\n",
      "    -> Processing chunk 10/10...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~55 rows.\n",
      "  -> ✅ Table #6 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 7 (Markdown Format) ---\n",
      "  -> Divided into 5 chunks for processing.\n",
      "    -> Processing chunk 1/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~6 rows.\n",
      "      -> ⏱️ Waiting for 34.97 seconds before next chunk...\n",
      "    -> Processing chunk 2/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~10 rows.\n",
      "      -> ⏱️ Waiting for 33.84 seconds before next chunk...\n",
      "    -> Processing chunk 3/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~15 rows.\n",
      "      -> ⏱️ Waiting for 33.46 seconds before next chunk...\n",
      "    -> Processing chunk 4/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~17 rows.\n",
      "      -> ⏱️ Waiting for 33.10 seconds before next chunk...\n",
      "    -> Processing chunk 5/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~19 rows.\n",
      "  -> ✅ Table #7 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 8 (Markdown Format) ---\n",
      "  -> Divided into 7 chunks for processing.\n",
      "    -> Processing chunk 1/7...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~7 rows.\n",
      "      -> ⏱️ Waiting for 34.70 seconds before next chunk...\n",
      "    -> Processing chunk 2/7...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~11 rows.\n",
      "      -> ⏱️ Waiting for 33.17 seconds before next chunk...\n",
      "    -> Processing chunk 3/7...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~15 rows.\n",
      "      -> ⏱️ Waiting for 34.83 seconds before next chunk...\n",
      "    -> Processing chunk 4/7...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~18 rows.\n",
      "      -> ⏱️ Waiting for 33.38 seconds before next chunk...\n",
      "    -> Processing chunk 5/7...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~22 rows.\n",
      "      -> ⏱️ Waiting for 34.23 seconds before next chunk...\n",
      "    -> Processing chunk 6/7...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~25 rows.\n",
      "      -> ⏱️ Waiting for 33.70 seconds before next chunk...\n",
      "    -> Processing chunk 7/7...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~29 rows.\n",
      "  -> ✅ Table #8 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 9 (Markdown Format) ---\n",
      "  -> Divided into 8 chunks for processing.\n",
      "    -> Processing chunk 1/8...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~5 rows.\n",
      "      -> ⏱️ Waiting for 34.71 seconds before next chunk...\n",
      "    -> Processing chunk 2/8...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~11 rows.\n",
      "      -> ⏱️ Waiting for 34.20 seconds before next chunk...\n",
      "    -> Processing chunk 3/8...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~16 rows.\n",
      "      -> ⏱️ Waiting for 34.39 seconds before next chunk...\n",
      "    -> Processing chunk 4/8...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~19 rows.\n",
      "      -> ⏱️ Waiting for 33.76 seconds before next chunk...\n",
      "    -> Processing chunk 5/8...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~24 rows.\n",
      "      -> ⏱️ Waiting for 33.77 seconds before next chunk...\n",
      "    -> Processing chunk 6/8...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~32 rows.\n",
      "      -> ⏱️ Waiting for 34.51 seconds before next chunk...\n",
      "    -> Processing chunk 7/8...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~38 rows.\n",
      "      -> ⏱️ Waiting for 33.45 seconds before next chunk...\n",
      "    -> Processing chunk 8/8...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~40 rows.\n",
      "  -> ✅ Table #9 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 10 (Markdown Format) ---\n",
      "  -> Divided into 9 chunks for processing.\n",
      "    -> Processing chunk 1/9...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~4 rows.\n",
      "      -> ⏱️ Waiting for 34.46 seconds before next chunk...\n",
      "    -> Processing chunk 2/9...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~8 rows.\n",
      "      -> ⏱️ Waiting for 34.16 seconds before next chunk...\n",
      "    -> Processing chunk 3/9...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~11 rows.\n",
      "      -> ⏱️ Waiting for 33.91 seconds before next chunk...\n",
      "    -> Processing chunk 4/9...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~14 rows.\n",
      "      -> ⏱️ Waiting for 34.32 seconds before next chunk...\n",
      "    -> Processing chunk 5/9...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~16 rows.\n",
      "      -> ⏱️ Waiting for 33.42 seconds before next chunk...\n",
      "    -> Processing chunk 6/9...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~18 rows.\n",
      "      -> ⏱️ Waiting for 34.60 seconds before next chunk...\n",
      "    -> Processing chunk 7/9...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~21 rows.\n",
      "      -> ⏱️ Waiting for 33.26 seconds before next chunk...\n",
      "    -> Processing chunk 8/9...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~27 rows.\n",
      "      -> ⏱️ Waiting for 34.02 seconds before next chunk...\n",
      "    -> Processing chunk 9/9...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~27 rows.\n",
      "  -> ✅ Table #10 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 11 (Markdown Format) ---\n",
      "  -> Divided into 5 chunks for processing.\n",
      "    -> Processing chunk 1/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~7 rows.\n",
      "      -> ⏱️ Waiting for 33.48 seconds before next chunk...\n",
      "    -> Processing chunk 2/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~11 rows.\n",
      "      -> ⏱️ Waiting for 33.82 seconds before next chunk...\n",
      "    -> Processing chunk 3/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~14 rows.\n",
      "      -> ⏱️ Waiting for 33.57 seconds before next chunk...\n",
      "    -> Processing chunk 4/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~17 rows.\n",
      "      -> ⏱️ Waiting for 33.61 seconds before next chunk...\n",
      "    -> Processing chunk 5/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~20 rows.\n",
      "  -> ✅ Table #11 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 12 (Markdown Format) ---\n",
      "  -> Divided into 5 chunks for processing.\n",
      "    -> Processing chunk 1/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~4 rows.\n",
      "      -> ⏱️ Waiting for 34.31 seconds before next chunk...\n",
      "    -> Processing chunk 2/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~7 rows.\n",
      "      -> ⏱️ Waiting for 33.80 seconds before next chunk...\n",
      "    -> Processing chunk 3/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~10 rows.\n",
      "      -> ⏱️ Waiting for 34.94 seconds before next chunk...\n",
      "    -> Processing chunk 4/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~17 rows.\n",
      "      -> ⏱️ Waiting for 33.36 seconds before next chunk...\n",
      "    -> Processing chunk 5/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~21 rows.\n",
      "  -> ✅ Table #12 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 13 (Markdown Format) ---\n",
      "  -> Divided into 3 chunks for processing.\n",
      "    -> Processing chunk 1/3...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~5 rows.\n",
      "      -> ⏱️ Waiting for 34.48 seconds before next chunk...\n",
      "    -> Processing chunk 2/3...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~22 rows.\n",
      "      -> ⏱️ Waiting for 33.59 seconds before next chunk...\n",
      "    -> Processing chunk 3/3...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~23 rows.\n",
      "  -> ✅ Table #13 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 14 (Markdown Format) ---\n",
      "  -> Divided into 1 chunks for processing.\n",
      "    -> Processing chunk 1/1...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~8 rows.\n",
      "  -> ✅ Table #14 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 15 (Markdown Format) ---\n",
      "  -> Divided into 1 chunks for processing.\n",
      "    -> Processing chunk 1/1...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~6 rows.\n",
      "  -> ✅ Table #15 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 16 (Markdown Format) ---\n",
      "  -> Divided into 3 chunks for processing.\n",
      "    -> Processing chunk 1/3...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~6 rows.\n",
      "      -> ⏱️ Waiting for 34.78 seconds before next chunk...\n",
      "    -> Processing chunk 2/3...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~9 rows.\n",
      "      -> ⏱️ Waiting for 33.10 seconds before next chunk...\n",
      "    -> Processing chunk 3/3...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~11 rows.\n",
      "  -> ✅ Table #16 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 17 (Markdown Format) ---\n",
      "  -> Divided into 3 chunks for processing.\n",
      "    -> Processing chunk 1/3...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~6 rows.\n",
      "      -> ⏱️ Waiting for 33.94 seconds before next chunk...\n",
      "    -> Processing chunk 2/3...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~9 rows.\n",
      "      -> ⏱️ Waiting for 34.32 seconds before next chunk...\n",
      "    -> Processing chunk 3/3...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~10 rows.\n",
      "  -> ✅ Table #17 reconstruction completed!\n",
      "  -> Extracted 130 text chunks with metadata.\n",
      "  -> ✅ Document 'download-financial-operating-information-pdf-3dedf514' prepared successfully.\n",
      "  -> ✅ Saved prepared data to: i:\\My Drive\\M. Tech AI ML\\AIML SEM 4\\Dissertation\\Project\\output\\parsed_pdfs\\downloaded_verizon_quarterly_pdfs\\2025\\1Q\\download-financial-operating-information-pdf-3dedf514\\download-financial-operating-information-pdf-3dedf514.prepared.json\n",
      "\n",
      "Processing document: download-non-gaap-reconciliations-pdf-d419afb0\n",
      "\n",
      "--- Preparing Document for Graph Extraction: download-non-gaap-reconciliations-pdf-d419afb0 ---\n",
      "  -> Found 8 tables to process.\n",
      "\n",
      "--- Reconstructing Table # 0 (Markdown Format) ---\n",
      "  -> Divided into 9 chunks for processing.\n",
      "    -> Processing chunk 1/9...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~4 rows.\n",
      "      -> ⏱️ Waiting for 34.02 seconds before next chunk...\n",
      "    -> Processing chunk 2/9...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~8 rows.\n",
      "      -> ⏱️ Waiting for 33.77 seconds before next chunk...\n",
      "    -> Processing chunk 3/9...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~12 rows.\n",
      "      -> ⏱️ Waiting for 34.58 seconds before next chunk...\n",
      "    -> Processing chunk 4/9...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~15 rows.\n",
      "      -> ⏱️ Waiting for 34.70 seconds before next chunk...\n",
      "    -> Processing chunk 5/9...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~17 rows.\n",
      "      -> ⏱️ Waiting for 34.18 seconds before next chunk...\n",
      "    -> Processing chunk 6/9...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~19 rows.\n",
      "      -> ⏱️ Waiting for 33.31 seconds before next chunk...\n",
      "    -> Processing chunk 7/9...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~22 rows.\n",
      "      -> ⏱️ Waiting for 33.20 seconds before next chunk...\n",
      "    -> Processing chunk 8/9...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~27 rows.\n",
      "      -> ⏱️ Waiting for 34.53 seconds before next chunk...\n",
      "    -> Processing chunk 9/9...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~28 rows.\n",
      "  -> ✅ Table #0 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 1 (Markdown Format) ---\n",
      "  -> Divided into 5 chunks for processing.\n",
      "    -> Processing chunk 1/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~7 rows.\n",
      "      -> ⏱️ Waiting for 33.66 seconds before next chunk...\n",
      "    -> Processing chunk 2/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~11 rows.\n",
      "      -> ⏱️ Waiting for 34.47 seconds before next chunk...\n",
      "    -> Processing chunk 3/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~14 rows.\n",
      "      -> ⏱️ Waiting for 34.92 seconds before next chunk...\n",
      "    -> Processing chunk 4/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~17 rows.\n",
      "      -> ⏱️ Waiting for 34.16 seconds before next chunk...\n",
      "    -> Processing chunk 5/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~20 rows.\n",
      "  -> ✅ Table #1 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 2 (Markdown Format) ---\n",
      "  -> Divided into 5 chunks for processing.\n",
      "    -> Processing chunk 1/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~4 rows.\n",
      "      -> ⏱️ Waiting for 34.06 seconds before next chunk...\n",
      "    -> Processing chunk 2/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~7 rows.\n",
      "      -> ⏱️ Waiting for 33.18 seconds before next chunk...\n",
      "    -> Processing chunk 3/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~10 rows.\n",
      "      -> ⏱️ Waiting for 33.54 seconds before next chunk...\n",
      "    -> Processing chunk 4/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~17 rows.\n",
      "      -> ⏱️ Waiting for 33.58 seconds before next chunk...\n",
      "    -> Processing chunk 5/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~21 rows.\n",
      "  -> ✅ Table #2 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 3 (Markdown Format) ---\n",
      "  -> Divided into 3 chunks for processing.\n",
      "    -> Processing chunk 1/3...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~5 rows.\n",
      "      -> ⏱️ Waiting for 33.46 seconds before next chunk...\n",
      "    -> Processing chunk 2/3...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~14 rows.\n",
      "      -> ⏱️ Waiting for 34.17 seconds before next chunk...\n",
      "    -> Processing chunk 3/3...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~11 rows.\n",
      "  -> ✅ Table #3 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 4 (Markdown Format) ---\n",
      "  -> Divided into 1 chunks for processing.\n",
      "    -> Processing chunk 1/1...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~8 rows.\n",
      "  -> ✅ Table #4 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 5 (Markdown Format) ---\n",
      "  -> Divided into 1 chunks for processing.\n",
      "    -> Processing chunk 1/1...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~6 rows.\n",
      "  -> ✅ Table #5 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 6 (Markdown Format) ---\n",
      "  -> Divided into 3 chunks for processing.\n",
      "    -> Processing chunk 1/3...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~6 rows.\n",
      "      -> ⏱️ Waiting for 33.41 seconds before next chunk...\n",
      "    -> Processing chunk 2/3...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~9 rows.\n",
      "      -> ⏱️ Waiting for 34.99 seconds before next chunk...\n",
      "    -> Processing chunk 3/3...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~11 rows.\n",
      "  -> ✅ Table #6 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 7 (Markdown Format) ---\n",
      "  -> Divided into 3 chunks for processing.\n",
      "    -> Processing chunk 1/3...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~6 rows.\n",
      "      -> ⏱️ Waiting for 33.59 seconds before next chunk...\n",
      "    -> Processing chunk 2/3...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~9 rows.\n",
      "      -> ⏱️ Waiting for 34.99 seconds before next chunk...\n",
      "    -> Processing chunk 3/3...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~10 rows.\n",
      "  -> ✅ Table #7 reconstruction completed!\n",
      "  -> Extracted 60 text chunks with metadata.\n",
      "  -> ✅ Document 'download-non-gaap-reconciliations-pdf-d419afb0' prepared successfully.\n",
      "  -> ✅ Saved prepared data to: i:\\My Drive\\M. Tech AI ML\\AIML SEM 4\\Dissertation\\Project\\output\\parsed_pdfs\\downloaded_verizon_quarterly_pdfs\\2025\\1Q\\download-non-gaap-reconciliations-pdf-d419afb0\\download-non-gaap-reconciliations-pdf-d419afb0.prepared.json\n",
      "\n",
      "Processing document: download-webcast-transcript-pdf-c1bdcc0f\n",
      "\n",
      "--- Preparing Document for Graph Extraction: download-webcast-transcript-pdf-c1bdcc0f ---\n",
      "  -> No tables found in this document.\n",
      "  -> Extracted 391 text chunks with metadata.\n",
      "  -> ✅ Document 'download-webcast-transcript-pdf-c1bdcc0f' prepared successfully.\n",
      "  -> ✅ Saved prepared data to: i:\\My Drive\\M. Tech AI ML\\AIML SEM 4\\Dissertation\\Project\\output\\parsed_pdfs\\downloaded_verizon_quarterly_pdfs\\2025\\2Q\\download-webcast-transcript-pdf-c1bdcc0f\\download-webcast-transcript-pdf-c1bdcc0f.prepared.json\n",
      "\n",
      "Processing document: download-presentation-pdf-90cbcc8c\n",
      "\n",
      "--- Preparing Document for Graph Extraction: download-presentation-pdf-90cbcc8c ---\n",
      "  -> Found 3 tables to process.\n",
      "\n",
      "--- Reconstructing Table # 0 (Markdown Format) ---\n",
      "  -> Divided into 1 chunks for processing.\n",
      "    -> Processing chunk 1/1...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~7 rows.\n",
      "  -> ✅ Table #0 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 1 (Markdown Format) ---\n",
      "  -> Divided into 2 chunks for processing.\n",
      "    -> Processing chunk 1/2...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~10 rows.\n",
      "      -> ⏱️ Waiting for 33.47 seconds before next chunk...\n",
      "    -> Processing chunk 2/2...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~12 rows.\n",
      "  -> ✅ Table #1 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 2 (Markdown Format) ---\n",
      "  -> Divided into 1 chunks for processing.\n",
      "    -> Processing chunk 1/1...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~7 rows.\n",
      "  -> ✅ Table #2 reconstruction completed!\n",
      "  -> Extracted 144 text chunks with metadata.\n",
      "  -> ✅ Document 'download-presentation-pdf-90cbcc8c' prepared successfully.\n",
      "  -> ✅ Saved prepared data to: i:\\My Drive\\M. Tech AI ML\\AIML SEM 4\\Dissertation\\Project\\output\\parsed_pdfs\\downloaded_verizon_quarterly_pdfs\\2025\\2Q\\download-presentation-pdf-90cbcc8c\\download-presentation-pdf-90cbcc8c.prepared.json\n",
      "\n",
      "Processing document: download-infographic-pdf-5450fea0\n",
      "\n",
      "--- Preparing Document for Graph Extraction: download-infographic-pdf-5450fea0 ---\n",
      "  -> No tables found in this document.\n",
      "  -> Extracted 41 text chunks with metadata.\n",
      "  -> ✅ Document 'download-infographic-pdf-5450fea0' prepared successfully.\n",
      "  -> ✅ Saved prepared data to: i:\\My Drive\\M. Tech AI ML\\AIML SEM 4\\Dissertation\\Project\\output\\parsed_pdfs\\downloaded_verizon_quarterly_pdfs\\2025\\2Q\\download-infographic-pdf-5450fea0\\download-infographic-pdf-5450fea0.prepared.json\n",
      "\n",
      "Processing document: download-financial-statements-pdf-7b4b0746\n",
      "\n",
      "--- Preparing Document for Graph Extraction: download-financial-statements-pdf-7b4b0746 ---\n",
      "  -> Found 11 tables to process.\n",
      "\n",
      "--- Reconstructing Table # 0 (Markdown Format) ---\n",
      "  -> Divided into 9 chunks for processing.\n",
      "    -> Processing chunk 1/9...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~6 rows.\n",
      "      -> ⏱️ Waiting for 33.17 seconds before next chunk...\n",
      "    -> Processing chunk 2/9...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~10 rows.\n",
      "      -> ⏱️ Waiting for 34.57 seconds before next chunk...\n",
      "    -> Processing chunk 3/9...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~13 rows.\n",
      "      -> ⏱️ Waiting for 34.60 seconds before next chunk...\n",
      "    -> Processing chunk 4/9...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~15 rows.\n",
      "      -> ⏱️ Waiting for 34.27 seconds before next chunk...\n",
      "    -> Processing chunk 5/9...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~18 rows.\n",
      "      -> ⏱️ Waiting for 33.73 seconds before next chunk...\n",
      "    -> Processing chunk 6/9...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~21 rows.\n",
      "      -> ⏱️ Waiting for 34.83 seconds before next chunk...\n",
      "    -> Processing chunk 7/9...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~23 rows.\n",
      "      -> ⏱️ Waiting for 34.34 seconds before next chunk...\n",
      "    -> Processing chunk 8/9...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~25 rows.\n",
      "      -> ⏱️ Waiting for 34.49 seconds before next chunk...\n",
      "    -> Processing chunk 9/9...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~25 rows.\n",
      "  -> ✅ Table #0 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 1 (Markdown Format) ---\n",
      "  -> Divided into 8 chunks for processing.\n",
      "    -> Processing chunk 1/8...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~9 rows.\n",
      "      -> ⏱️ Waiting for 33.67 seconds before next chunk...\n",
      "    -> Processing chunk 2/8...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~14 rows.\n",
      "      -> ⏱️ Waiting for 34.75 seconds before next chunk...\n",
      "    -> Processing chunk 3/8...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~19 rows.\n",
      "      -> ⏱️ Waiting for 33.95 seconds before next chunk...\n",
      "    -> Processing chunk 4/8...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~22 rows.\n",
      "      -> ⏱️ Waiting for 34.98 seconds before next chunk...\n",
      "    -> Processing chunk 5/8...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~27 rows.\n",
      "      -> ⏱️ Waiting for 34.22 seconds before next chunk...\n",
      "    -> Processing chunk 6/8...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~32 rows.\n",
      "      -> ⏱️ Waiting for 33.45 seconds before next chunk...\n",
      "    -> Processing chunk 7/8...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~38 rows.\n",
      "      -> ⏱️ Waiting for 34.28 seconds before next chunk...\n",
      "    -> Processing chunk 8/8...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~42 rows.\n",
      "  -> ✅ Table #1 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 2 (Markdown Format) ---\n",
      "  -> Divided into 2 chunks for processing.\n",
      "    -> Processing chunk 1/2...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~10 rows.\n",
      "      -> ⏱️ Waiting for 33.06 seconds before next chunk...\n",
      "    -> Processing chunk 2/2...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~12 rows.\n",
      "  -> ✅ Table #2 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 3 (Markdown Format) ---\n",
      "  -> Divided into 6 chunks for processing.\n",
      "    -> Processing chunk 1/6...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~9 rows.\n",
      "      -> ⏱️ Waiting for 33.15 seconds before next chunk...\n",
      "    -> Processing chunk 2/6...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~14 rows.\n",
      "      -> ⏱️ Waiting for 33.07 seconds before next chunk...\n",
      "    -> Processing chunk 3/6...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~19 rows.\n",
      "      -> ⏱️ Waiting for 34.46 seconds before next chunk...\n",
      "    -> Processing chunk 4/6...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~25 rows.\n",
      "      -> ⏱️ Waiting for 33.62 seconds before next chunk...\n",
      "    -> Processing chunk 5/6...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~30 rows.\n",
      "      -> ⏱️ Waiting for 33.04 seconds before next chunk...\n",
      "    -> Processing chunk 6/6...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~30 rows.\n",
      "  -> ✅ Table #3 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 4 (Markdown Format) ---\n",
      "  -> Divided into 5 chunks for processing.\n",
      "    -> Processing chunk 1/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~6 rows.\n",
      "      -> ⏱️ Waiting for 33.97 seconds before next chunk...\n",
      "    -> Processing chunk 2/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~10 rows.\n",
      "      -> ⏱️ Waiting for 33.01 seconds before next chunk...\n",
      "    -> Processing chunk 3/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~13 rows.\n",
      "      -> ⏱️ Waiting for 34.10 seconds before next chunk...\n",
      "    -> Processing chunk 4/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~15 rows.\n",
      "      -> ⏱️ Waiting for 33.54 seconds before next chunk...\n",
      "    -> Processing chunk 5/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~18 rows.\n",
      "  -> ✅ Table #4 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 5 (Markdown Format) ---\n",
      "  -> Divided into 9 chunks for processing.\n",
      "    -> Processing chunk 1/9...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~7 rows.\n",
      "      -> ⏱️ Waiting for 33.37 seconds before next chunk...\n",
      "    -> Processing chunk 2/9...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~12 rows.\n",
      "      -> ⏱️ Waiting for 33.11 seconds before next chunk...\n",
      "    -> Processing chunk 3/9...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~15 rows.\n",
      "      -> ⏱️ Waiting for 34.84 seconds before next chunk...\n",
      "    -> Processing chunk 4/9...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~19 rows.\n",
      "      -> ⏱️ Waiting for 33.38 seconds before next chunk...\n",
      "    -> Processing chunk 5/9...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~23 rows.\n",
      "      -> ⏱️ Waiting for 34.59 seconds before next chunk...\n",
      "    -> Processing chunk 6/9...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~25 rows.\n",
      "      -> ⏱️ Waiting for 34.52 seconds before next chunk...\n",
      "    -> Processing chunk 7/9...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~30 rows.\n",
      "      -> ⏱️ Waiting for 33.85 seconds before next chunk...\n",
      "    -> Processing chunk 8/9...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~31 rows.\n",
      "      -> ⏱️ Waiting for 34.86 seconds before next chunk...\n",
      "    -> Processing chunk 9/9...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~32 rows.\n",
      "  -> ✅ Table #5 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 6 (Markdown Format) ---\n",
      "  -> Divided into 2 chunks for processing.\n",
      "    -> Processing chunk 1/2...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~6 rows.\n",
      "      -> ⏱️ Waiting for 33.39 seconds before next chunk...\n",
      "    -> Processing chunk 2/2...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~9 rows.\n",
      "  -> ✅ Table #6 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 7 (Markdown Format) ---\n",
      "  -> Divided into 5 chunks for processing.\n",
      "    -> Processing chunk 1/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~5 rows.\n",
      "      -> ⏱️ Waiting for 33.99 seconds before next chunk...\n",
      "    -> Processing chunk 2/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~9 rows.\n",
      "      -> ⏱️ Waiting for 33.39 seconds before next chunk...\n",
      "    -> Processing chunk 3/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~13 rows.\n",
      "      -> ⏱️ Waiting for 33.46 seconds before next chunk...\n",
      "    -> Processing chunk 4/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~15 rows.\n",
      "      -> ⏱️ Waiting for 33.49 seconds before next chunk...\n",
      "    -> Processing chunk 5/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~17 rows.\n",
      "  -> ✅ Table #7 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 8 (Markdown Format) ---\n",
      "  -> Divided into 7 chunks for processing.\n",
      "    -> Processing chunk 1/7...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~7 rows.\n",
      "      -> ⏱️ Waiting for 34.79 seconds before next chunk...\n",
      "    -> Processing chunk 2/7...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~11 rows.\n",
      "      -> ⏱️ Waiting for 33.27 seconds before next chunk...\n",
      "    -> Processing chunk 3/7...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~16 rows.\n",
      "      -> ⏱️ Waiting for 34.53 seconds before next chunk...\n",
      "    -> Processing chunk 4/7...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~19 rows.\n",
      "      -> ⏱️ Waiting for 33.81 seconds before next chunk...\n",
      "    -> Processing chunk 5/7...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~22 rows.\n",
      "      -> ⏱️ Waiting for 33.84 seconds before next chunk...\n",
      "    -> Processing chunk 6/7...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~27 rows.\n",
      "      -> ⏱️ Waiting for 33.31 seconds before next chunk...\n",
      "    -> Processing chunk 7/7...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~29 rows.\n",
      "  -> ✅ Table #8 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 9 (Markdown Format) ---\n",
      "  -> Divided into 2 chunks for processing.\n",
      "    -> Processing chunk 1/2...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~7 rows.\n",
      "      -> ⏱️ Waiting for 33.38 seconds before next chunk...\n",
      "    -> Processing chunk 2/2...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~7 rows.\n",
      "  -> ✅ Table #9 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 10 (Markdown Format) ---\n",
      "  -> Divided into 6 chunks for processing.\n",
      "    -> Processing chunk 1/6...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~6 rows.\n",
      "      -> ⏱️ Waiting for 33.14 seconds before next chunk...\n",
      "    -> Processing chunk 2/6...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~11 rows.\n",
      "      -> ⏱️ Waiting for 33.45 seconds before next chunk...\n",
      "    -> Processing chunk 3/6...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~15 rows.\n",
      "      -> ⏱️ Waiting for 34.78 seconds before next chunk...\n",
      "    -> Processing chunk 4/6...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~19 rows.\n",
      "      -> ⏱️ Waiting for 34.72 seconds before next chunk...\n",
      "    -> Processing chunk 5/6...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~24 rows.\n",
      "      -> ⏱️ Waiting for 33.56 seconds before next chunk...\n",
      "    -> Processing chunk 6/6...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~26 rows.\n",
      "  -> ✅ Table #10 reconstruction completed!\n",
      "  -> Extracted 62 text chunks with metadata.\n",
      "  -> ✅ Document 'download-financial-statements-pdf-7b4b0746' prepared successfully.\n",
      "  -> ✅ Saved prepared data to: i:\\My Drive\\M. Tech AI ML\\AIML SEM 4\\Dissertation\\Project\\output\\parsed_pdfs\\downloaded_verizon_quarterly_pdfs\\2025\\2Q\\download-financial-statements-pdf-7b4b0746\\download-financial-statements-pdf-7b4b0746.prepared.json\n",
      "\n",
      "Processing document: download-financial-operating-information-pdf-e1c5ab85\n",
      "\n",
      "--- Preparing Document for Graph Extraction: download-financial-operating-information-pdf-e1c5ab85 ---\n",
      "  -> Found 18 tables to process.\n",
      "\n",
      "--- Reconstructing Table # 0 (Markdown Format) ---\n",
      "  -> Divided into 1 chunks for processing.\n",
      "    -> Processing chunk 1/1...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~1 rows.\n",
      "  -> ✅ Table #0 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 1 (Markdown Format) ---\n",
      "  -> Divided into 13 chunks for processing.\n",
      "    -> Processing chunk 1/13...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~6 rows.\n",
      "      -> ⏱️ Waiting for 33.50 seconds before next chunk...\n",
      "    -> Processing chunk 2/13...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~8 rows.\n",
      "      -> ⏱️ Waiting for 33.94 seconds before next chunk...\n",
      "    -> Processing chunk 3/13...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~11 rows.\n",
      "      -> ⏱️ Waiting for 34.99 seconds before next chunk...\n",
      "    -> Processing chunk 4/13...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~13 rows.\n",
      "      -> ⏱️ Waiting for 34.07 seconds before next chunk...\n",
      "    -> Processing chunk 5/13...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~15 rows.\n",
      "      -> ⏱️ Waiting for 33.04 seconds before next chunk...\n",
      "    -> Processing chunk 6/13...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~17 rows.\n",
      "      -> ⏱️ Waiting for 33.15 seconds before next chunk...\n",
      "    -> Processing chunk 7/13...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~19 rows.\n",
      "      -> ⏱️ Waiting for 34.48 seconds before next chunk...\n",
      "    -> Processing chunk 8/13...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~22 rows.\n",
      "      -> ⏱️ Waiting for 33.10 seconds before next chunk...\n",
      "    -> Processing chunk 9/13...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~24 rows.\n",
      "      -> ⏱️ Waiting for 34.56 seconds before next chunk...\n",
      "    -> Processing chunk 10/13...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~25 rows.\n",
      "      -> ⏱️ Waiting for 33.69 seconds before next chunk...\n",
      "    -> Processing chunk 11/13...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~29 rows.\n",
      "      -> ⏱️ Waiting for 33.72 seconds before next chunk...\n",
      "    -> Processing chunk 12/13...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~32 rows.\n",
      "      -> ⏱️ Waiting for 33.16 seconds before next chunk...\n",
      "    -> Processing chunk 13/13...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~32 rows.\n",
      "  -> ✅ Table #1 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 2 (Markdown Format) ---\n",
      "  -> Divided into 16 chunks for processing.\n",
      "    -> Processing chunk 1/16...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~6 rows.\n",
      "      -> ⏱️ Waiting for 34.25 seconds before next chunk...\n",
      "    -> Processing chunk 2/16...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~8 rows.\n",
      "      -> ⏱️ Waiting for 33.20 seconds before next chunk...\n",
      "    -> Processing chunk 3/16...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~10 rows.\n",
      "      -> ⏱️ Waiting for 34.77 seconds before next chunk...\n",
      "    -> Processing chunk 4/16...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~13 rows.\n",
      "      -> ⏱️ Waiting for 34.84 seconds before next chunk...\n",
      "    -> Processing chunk 5/16...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~16 rows.\n",
      "      -> ⏱️ Waiting for 34.15 seconds before next chunk...\n",
      "    -> Processing chunk 6/16...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~19 rows.\n",
      "      -> ⏱️ Waiting for 33.69 seconds before next chunk...\n",
      "    -> Processing chunk 7/16...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~22 rows.\n",
      "      -> ⏱️ Waiting for 33.09 seconds before next chunk...\n",
      "    -> Processing chunk 8/16...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~27 rows.\n",
      "      -> ⏱️ Waiting for 33.08 seconds before next chunk...\n",
      "    -> Processing chunk 9/16...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~29 rows.\n",
      "      -> ⏱️ Waiting for 33.07 seconds before next chunk...\n",
      "    -> Processing chunk 10/16...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~32 rows.\n",
      "      -> ⏱️ Waiting for 34.12 seconds before next chunk...\n",
      "    -> Processing chunk 11/16...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~34 rows.\n",
      "      -> ⏱️ Waiting for 33.73 seconds before next chunk...\n",
      "    -> Processing chunk 12/16...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~38 rows.\n",
      "      -> ⏱️ Waiting for 33.24 seconds before next chunk...\n",
      "    -> Processing chunk 13/16...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~40 rows.\n",
      "      -> ⏱️ Waiting for 34.34 seconds before next chunk...\n",
      "    -> Processing chunk 14/16...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~43 rows.\n",
      "      -> ⏱️ Waiting for 33.22 seconds before next chunk...\n",
      "    -> Processing chunk 15/16...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~45 rows.\n",
      "      -> ⏱️ Waiting for 34.04 seconds before next chunk...\n",
      "    -> Processing chunk 16/16...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~46 rows.\n",
      "  -> ✅ Table #2 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 3 (Markdown Format) ---\n",
      "  -> Divided into 4 chunks for processing.\n",
      "    -> Processing chunk 1/4...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~4 rows.\n",
      "      -> ⏱️ Waiting for 33.30 seconds before next chunk...\n",
      "    -> Processing chunk 2/4...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~8 rows.\n",
      "      -> ⏱️ Waiting for 33.36 seconds before next chunk...\n",
      "    -> Processing chunk 3/4...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~11 rows.\n",
      "      -> ⏱️ Waiting for 34.46 seconds before next chunk...\n",
      "    -> Processing chunk 4/4...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~12 rows.\n",
      "  -> ✅ Table #3 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 4 (Markdown Format) ---\n",
      "  -> Divided into 11 chunks for processing.\n",
      "    -> Processing chunk 1/11...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~6 rows.\n",
      "      -> ⏱️ Waiting for 33.33 seconds before next chunk...\n",
      "    -> Processing chunk 2/11...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~10 rows.\n",
      "      -> ⏱️ Waiting for 34.00 seconds before next chunk...\n",
      "    -> Processing chunk 3/11...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~12 rows.\n",
      "      -> ⏱️ Waiting for 34.83 seconds before next chunk...\n",
      "    -> Processing chunk 4/11...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~17 rows.\n",
      "      -> ⏱️ Waiting for 33.76 seconds before next chunk...\n",
      "    -> Processing chunk 5/11...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~21 rows.\n",
      "      -> ⏱️ Waiting for 34.42 seconds before next chunk...\n",
      "    -> Processing chunk 6/11...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~23 rows.\n",
      "      -> ⏱️ Waiting for 33.97 seconds before next chunk...\n",
      "    -> Processing chunk 7/11...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~27 rows.\n",
      "      -> ⏱️ Waiting for 34.67 seconds before next chunk...\n",
      "    -> Processing chunk 8/11...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~30 rows.\n",
      "      -> ⏱️ Waiting for 34.39 seconds before next chunk...\n",
      "    -> Processing chunk 9/11...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~32 rows.\n",
      "      -> ⏱️ Waiting for 33.46 seconds before next chunk...\n",
      "    -> Processing chunk 10/11...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~35 rows.\n",
      "      -> ⏱️ Waiting for 34.89 seconds before next chunk...\n",
      "    -> Processing chunk 11/11...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~37 rows.\n",
      "  -> ✅ Table #4 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 5 (Markdown Format) ---\n",
      "  -> Divided into 6 chunks for processing.\n",
      "    -> Processing chunk 1/6...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~5 rows.\n",
      "      -> ⏱️ Waiting for 34.11 seconds before next chunk...\n",
      "    -> Processing chunk 2/6...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~9 rows.\n",
      "      -> ⏱️ Waiting for 34.22 seconds before next chunk...\n",
      "    -> Processing chunk 3/6...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~12 rows.\n",
      "      -> ⏱️ Waiting for 34.57 seconds before next chunk...\n",
      "    -> Processing chunk 4/6...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~15 rows.\n",
      "      -> ⏱️ Waiting for 33.16 seconds before next chunk...\n",
      "    -> Processing chunk 5/6...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~18 rows.\n",
      "      -> ⏱️ Waiting for 34.88 seconds before next chunk...\n",
      "    -> Processing chunk 6/6...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~19 rows.\n",
      "  -> ✅ Table #5 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 6 (Markdown Format) ---\n",
      "  -> Divided into 12 chunks for processing.\n",
      "    -> Processing chunk 1/12...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~7 rows.\n",
      "      -> ⏱️ Waiting for 34.79 seconds before next chunk...\n",
      "    -> Processing chunk 2/12...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~16 rows.\n",
      "      -> ⏱️ Waiting for 33.01 seconds before next chunk...\n",
      "    -> Processing chunk 3/12...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~31 rows.\n",
      "      -> ⏱️ Waiting for 33.21 seconds before next chunk...\n",
      "    -> Processing chunk 4/12...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~46 rows.\n",
      "      -> ⏱️ Waiting for 34.65 seconds before next chunk...\n",
      "    -> Processing chunk 5/12...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~54 rows.\n",
      "      -> ⏱️ Waiting for 33.20 seconds before next chunk...\n",
      "    -> Processing chunk 6/12...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~59 rows.\n",
      "      -> ⏱️ Waiting for 34.78 seconds before next chunk...\n",
      "    -> Processing chunk 7/12...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~64 rows.\n",
      "      -> ⏱️ Waiting for 34.06 seconds before next chunk...\n",
      "    -> Processing chunk 8/12...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~73 rows.\n",
      "      -> ⏱️ Waiting for 34.63 seconds before next chunk...\n",
      "    -> Processing chunk 9/12...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~81 rows.\n",
      "      -> ⏱️ Waiting for 33.59 seconds before next chunk...\n",
      "    -> Processing chunk 10/12...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~89 rows.\n",
      "      -> ⏱️ Waiting for 34.46 seconds before next chunk...\n",
      "    -> Processing chunk 11/12...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~99 rows.\n",
      "      -> ⏱️ Waiting for 34.32 seconds before next chunk...\n",
      "    -> Processing chunk 12/12...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~5 rows.\n",
      "  -> ✅ Table #6 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 7 (Markdown Format) ---\n",
      "  -> Divided into 6 chunks for processing.\n",
      "    -> Processing chunk 1/6...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~6 rows.\n",
      "      -> ⏱️ Waiting for 33.77 seconds before next chunk...\n",
      "    -> Processing chunk 2/6...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~10 rows.\n",
      "      -> ⏱️ Waiting for 34.45 seconds before next chunk...\n",
      "    -> Processing chunk 3/6...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~13 rows.\n",
      "      -> ⏱️ Waiting for 33.79 seconds before next chunk...\n",
      "    -> Processing chunk 4/6...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~15 rows.\n",
      "      -> ⏱️ Waiting for 34.57 seconds before next chunk...\n",
      "    -> Processing chunk 5/6...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~18 rows.\n",
      "      -> ⏱️ Waiting for 33.11 seconds before next chunk...\n",
      "    -> Processing chunk 6/6...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~19 rows.\n",
      "  -> ✅ Table #7 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 8 (Markdown Format) ---\n",
      "  -> Divided into 9 chunks for processing.\n",
      "    -> Processing chunk 1/9...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~7 rows.\n",
      "      -> ⏱️ Waiting for 33.93 seconds before next chunk...\n",
      "    -> Processing chunk 2/9...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~11 rows.\n",
      "      -> ⏱️ Waiting for 34.67 seconds before next chunk...\n",
      "    -> Processing chunk 3/9...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~30 rows.\n",
      "      -> ⏱️ Waiting for 34.46 seconds before next chunk...\n",
      "    -> Processing chunk 4/9...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~28 rows.\n",
      "      -> ⏱️ Waiting for 33.55 seconds before next chunk...\n",
      "    -> Processing chunk 5/9...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~30 rows.\n",
      "      -> ⏱️ Waiting for 33.13 seconds before next chunk...\n",
      "    -> Processing chunk 6/9...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~34 rows.\n",
      "      -> ⏱️ Waiting for 33.01 seconds before next chunk...\n",
      "    -> Processing chunk 7/9...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~37 rows.\n",
      "      -> ⏱️ Waiting for 34.12 seconds before next chunk...\n",
      "    -> Processing chunk 8/9...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~43 rows.\n",
      "      -> ⏱️ Waiting for 34.91 seconds before next chunk...\n",
      "    -> Processing chunk 9/9...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~44 rows.\n",
      "  -> ✅ Table #8 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 9 (Markdown Format) ---\n",
      "  -> Divided into 9 chunks for processing.\n",
      "    -> Processing chunk 1/9...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~7 rows.\n",
      "      -> ⏱️ Waiting for 34.10 seconds before next chunk...\n",
      "    -> Processing chunk 2/9...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~14 rows.\n",
      "      -> ⏱️ Waiting for 33.47 seconds before next chunk...\n",
      "    -> Processing chunk 3/9...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~16 rows.\n",
      "      -> ⏱️ Waiting for 33.49 seconds before next chunk...\n",
      "    -> Processing chunk 4/9...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~20 rows.\n",
      "      -> ⏱️ Waiting for 34.07 seconds before next chunk...\n",
      "    -> Processing chunk 5/9...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~24 rows.\n",
      "      -> ⏱️ Waiting for 34.60 seconds before next chunk...\n",
      "    -> Processing chunk 6/9...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~31 rows.\n",
      "      -> ⏱️ Waiting for 33.25 seconds before next chunk...\n",
      "    -> Processing chunk 7/9...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~43 rows.\n",
      "      -> ⏱️ Waiting for 34.27 seconds before next chunk...\n",
      "    -> Processing chunk 8/9...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~47 rows.\n",
      "      -> ⏱️ Waiting for 34.69 seconds before next chunk...\n",
      "    -> Processing chunk 9/9...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~50 rows.\n",
      "  -> ✅ Table #9 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 10 (Markdown Format) ---\n",
      "  -> Divided into 9 chunks for processing.\n",
      "    -> Processing chunk 1/9...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~5 rows.\n",
      "      -> ⏱️ Waiting for 33.14 seconds before next chunk...\n",
      "    -> Processing chunk 2/9...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~7 rows.\n",
      "      -> ⏱️ Waiting for 34.27 seconds before next chunk...\n",
      "    -> Processing chunk 3/9...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~11 rows.\n",
      "      -> ⏱️ Waiting for 34.72 seconds before next chunk...\n",
      "    -> Processing chunk 4/9...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~14 rows.\n",
      "      -> ⏱️ Waiting for 33.21 seconds before next chunk...\n",
      "    -> Processing chunk 5/9...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~16 rows.\n",
      "      -> ⏱️ Waiting for 33.85 seconds before next chunk...\n",
      "    -> Processing chunk 6/9...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~18 rows.\n",
      "      -> ⏱️ Waiting for 33.07 seconds before next chunk...\n",
      "    -> Processing chunk 7/9...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~21 rows.\n",
      "      -> ⏱️ Waiting for 33.23 seconds before next chunk...\n",
      "    -> Processing chunk 8/9...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~26 rows.\n",
      "      -> ⏱️ Waiting for 34.71 seconds before next chunk...\n",
      "    -> Processing chunk 9/9...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~27 rows.\n",
      "  -> ✅ Table #10 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 11 (Markdown Format) ---\n",
      "  -> Divided into 5 chunks for processing.\n",
      "    -> Processing chunk 1/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~7 rows.\n",
      "      -> ⏱️ Waiting for 34.14 seconds before next chunk...\n",
      "    -> Processing chunk 2/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~11 rows.\n",
      "      -> ⏱️ Waiting for 33.80 seconds before next chunk...\n",
      "    -> Processing chunk 3/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~14 rows.\n",
      "      -> ⏱️ Waiting for 34.71 seconds before next chunk...\n",
      "    -> Processing chunk 4/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~17 rows.\n",
      "      -> ⏱️ Waiting for 33.01 seconds before next chunk...\n",
      "    -> Processing chunk 5/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~20 rows.\n",
      "  -> ✅ Table #11 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 12 (Markdown Format) ---\n",
      "  -> Divided into 5 chunks for processing.\n",
      "    -> Processing chunk 1/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~4 rows.\n",
      "      -> ⏱️ Waiting for 35.00 seconds before next chunk...\n",
      "    -> Processing chunk 2/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~7 rows.\n",
      "      -> ⏱️ Waiting for 34.48 seconds before next chunk...\n",
      "    -> Processing chunk 3/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~10 rows.\n",
      "      -> ⏱️ Waiting for 33.08 seconds before next chunk...\n",
      "    -> Processing chunk 4/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~15 rows.\n",
      "      -> ⏱️ Waiting for 34.59 seconds before next chunk...\n",
      "    -> Processing chunk 5/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~22 rows.\n",
      "  -> ✅ Table #12 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 13 (Markdown Format) ---\n",
      "  -> Divided into 3 chunks for processing.\n",
      "    -> Processing chunk 1/3...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~12 rows.\n",
      "      -> ⏱️ Waiting for 33.46 seconds before next chunk...\n",
      "    -> Processing chunk 2/3...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~20 rows.\n",
      "      -> ⏱️ Waiting for 33.95 seconds before next chunk...\n",
      "    -> Processing chunk 3/3...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~29 rows.\n",
      "  -> ✅ Table #13 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 14 (Markdown Format) ---\n",
      "  -> Divided into 2 chunks for processing.\n",
      "    -> Processing chunk 1/2...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~6 rows.\n",
      "      -> ⏱️ Waiting for 33.87 seconds before next chunk...\n",
      "    -> Processing chunk 2/2...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~8 rows.\n",
      "  -> ✅ Table #14 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 15 (Markdown Format) ---\n",
      "  -> Divided into 1 chunks for processing.\n",
      "    -> Processing chunk 1/1...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~6 rows.\n",
      "  -> ✅ Table #15 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 16 (Markdown Format) ---\n",
      "  -> Divided into 3 chunks for processing.\n",
      "    -> Processing chunk 1/3...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~5 rows.\n",
      "      -> ⏱️ Waiting for 33.24 seconds before next chunk...\n",
      "    -> Processing chunk 2/3...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~8 rows.\n",
      "      -> ⏱️ Waiting for 33.04 seconds before next chunk...\n",
      "    -> Processing chunk 3/3...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~10 rows.\n",
      "  -> ✅ Table #16 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 17 (Markdown Format) ---\n",
      "  -> Divided into 3 chunks for processing.\n",
      "    -> Processing chunk 1/3...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~5 rows.\n",
      "      -> ⏱️ Waiting for 33.75 seconds before next chunk...\n",
      "    -> Processing chunk 2/3...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~8 rows.\n",
      "      -> ⏱️ Waiting for 34.43 seconds before next chunk...\n",
      "    -> Processing chunk 3/3...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~10 rows.\n",
      "  -> ✅ Table #17 reconstruction completed!\n",
      "  -> Extracted 131 text chunks with metadata.\n",
      "  -> ✅ Document 'download-financial-operating-information-pdf-e1c5ab85' prepared successfully.\n",
      "  -> ✅ Saved prepared data to: i:\\My Drive\\M. Tech AI ML\\AIML SEM 4\\Dissertation\\Project\\output\\parsed_pdfs\\downloaded_verizon_quarterly_pdfs\\2025\\2Q\\download-financial-operating-information-pdf-e1c5ab85\\download-financial-operating-information-pdf-e1c5ab85.prepared.json\n",
      "\n",
      "Processing document: download-non-gaap-reconciliations-pdf-eb924252\n",
      "\n",
      "--- Preparing Document for Graph Extraction: download-non-gaap-reconciliations-pdf-eb924252 ---\n",
      "  -> Found 8 tables to process.\n",
      "\n",
      "--- Reconstructing Table # 0 (Markdown Format) ---\n",
      "  -> Divided into 9 chunks for processing.\n",
      "    -> Processing chunk 1/9...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~4 rows.\n",
      "      -> ⏱️ Waiting for 34.29 seconds before next chunk...\n",
      "    -> Processing chunk 2/9...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~7 rows.\n",
      "      -> ⏱️ Waiting for 33.97 seconds before next chunk...\n",
      "    -> Processing chunk 3/9...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~11 rows.\n",
      "      -> ⏱️ Waiting for 33.95 seconds before next chunk...\n",
      "    -> Processing chunk 4/9...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~14 rows.\n",
      "      -> ⏱️ Waiting for 34.02 seconds before next chunk...\n",
      "    -> Processing chunk 5/9...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~16 rows.\n",
      "      -> ⏱️ Waiting for 34.45 seconds before next chunk...\n",
      "    -> Processing chunk 6/9...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~18 rows.\n",
      "      -> ⏱️ Waiting for 34.03 seconds before next chunk...\n",
      "    -> Processing chunk 7/9...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~21 rows.\n",
      "      -> ⏱️ Waiting for 34.25 seconds before next chunk...\n",
      "    -> Processing chunk 8/9...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~28 rows.\n",
      "      -> ⏱️ Waiting for 34.15 seconds before next chunk...\n",
      "    -> Processing chunk 9/9...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~28 rows.\n",
      "  -> ✅ Table #0 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 1 (Markdown Format) ---\n",
      "  -> Divided into 5 chunks for processing.\n",
      "    -> Processing chunk 1/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~7 rows.\n",
      "      -> ⏱️ Waiting for 34.03 seconds before next chunk...\n",
      "    -> Processing chunk 2/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~11 rows.\n",
      "      -> ⏱️ Waiting for 33.03 seconds before next chunk...\n",
      "    -> Processing chunk 3/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~14 rows.\n",
      "      -> ⏱️ Waiting for 33.18 seconds before next chunk...\n",
      "    -> Processing chunk 4/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~17 rows.\n",
      "      -> ⏱️ Waiting for 34.22 seconds before next chunk...\n",
      "    -> Processing chunk 5/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~20 rows.\n",
      "  -> ✅ Table #1 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 2 (Markdown Format) ---\n",
      "  -> Divided into 5 chunks for processing.\n",
      "    -> Processing chunk 1/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~4 rows.\n",
      "      -> ⏱️ Waiting for 33.38 seconds before next chunk...\n",
      "    -> Processing chunk 2/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~7 rows.\n",
      "      -> ⏱️ Waiting for 34.20 seconds before next chunk...\n",
      "    -> Processing chunk 3/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~10 rows.\n",
      "      -> ⏱️ Waiting for 34.33 seconds before next chunk...\n",
      "    -> Processing chunk 4/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~15 rows.\n",
      "      -> ⏱️ Waiting for 34.67 seconds before next chunk...\n",
      "    -> Processing chunk 5/5...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~22 rows.\n",
      "  -> ✅ Table #2 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 3 (Markdown Format) ---\n",
      "  -> Divided into 3 chunks for processing.\n",
      "    -> Processing chunk 1/3...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~12 rows.\n",
      "      -> ⏱️ Waiting for 34.99 seconds before next chunk...\n",
      "    -> Processing chunk 2/3...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~17 rows.\n",
      "      -> ⏱️ Waiting for 33.79 seconds before next chunk...\n",
      "    -> Processing chunk 3/3...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~20 rows.\n",
      "  -> ✅ Table #3 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 4 (Markdown Format) ---\n",
      "  -> Divided into 2 chunks for processing.\n",
      "    -> Processing chunk 1/2...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~6 rows.\n",
      "      -> ⏱️ Waiting for 33.85 seconds before next chunk...\n",
      "    -> Processing chunk 2/2...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~8 rows.\n",
      "  -> ✅ Table #4 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 5 (Markdown Format) ---\n",
      "  -> Divided into 1 chunks for processing.\n",
      "    -> Processing chunk 1/1...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~6 rows.\n",
      "  -> ✅ Table #5 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 6 (Markdown Format) ---\n",
      "  -> Divided into 3 chunks for processing.\n",
      "    -> Processing chunk 1/3...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~5 rows.\n",
      "      -> ⏱️ Waiting for 33.30 seconds before next chunk...\n",
      "    -> Processing chunk 2/3...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~8 rows.\n",
      "      -> ⏱️ Waiting for 34.29 seconds before next chunk...\n",
      "    -> Processing chunk 3/3...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~10 rows.\n",
      "  -> ✅ Table #6 reconstruction completed!\n",
      "\n",
      "--- Reconstructing Table # 7 (Markdown Format) ---\n",
      "  -> Divided into 3 chunks for processing.\n",
      "    -> Processing chunk 1/3...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~5 rows.\n",
      "      -> ⏱️ Waiting for 33.89 seconds before next chunk...\n",
      "    -> Processing chunk 2/3...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~8 rows.\n",
      "      -> ⏱️ Waiting for 33.55 seconds before next chunk...\n",
      "    -> Processing chunk 3/3...\n",
      "🔍 DEBUG: Attempting to parse JSON content...\n",
      "🔍 DEBUG: Found JSON block inside markdown.\n",
      "      -> ✅ Chunk merged. Table now has ~10 rows.\n",
      "  -> ✅ Table #7 reconstruction completed!\n",
      "  -> Extracted 61 text chunks with metadata.\n",
      "  -> ✅ Document 'download-non-gaap-reconciliations-pdf-eb924252' prepared successfully.\n",
      "  -> ✅ Saved prepared data to: i:\\My Drive\\M. Tech AI ML\\AIML SEM 4\\Dissertation\\Project\\output\\parsed_pdfs\\downloaded_verizon_quarterly_pdfs\\2025\\2Q\\download-non-gaap-reconciliations-pdf-eb924252\\download-non-gaap-reconciliations-pdf-eb924252.prepared.json\n",
      "\n",
      "\n",
      "--- ✅ BATCH PROCESSING COMPLETE ---\n",
      "  - Successfully Processed: 17\n",
      "  - Skipped (already done): 27\n",
      "  - Failed: 0\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm.notebook import tqdm # Use tqdm.notebook for a nice progress bar in Jupyter\n",
    "\n",
    "# --- Batch Processing Setup ---\n",
    "# Initialize agents once to be reused for all documents\n",
    "table_recon_agent = TableReconstructionAgent()\n",
    "doc_processor = DocumentProcessor(table_agent=table_recon_agent)\n",
    "\n",
    "processed_count = 0\n",
    "skipped_count = 0\n",
    "failed_count = 0\n",
    "\n",
    "print(f\"\\n--- Starting Batch Processing for {len(all_results)} Documents ---\")\n",
    "\n",
    "if all_results:\n",
    "    # Wrap the all_results list with tqdm for a progress bar\n",
    "    for summary in tqdm(all_results, desc=\"Processing Documents\"):\n",
    "        doc_id = summary.get(\"doc_id\")\n",
    "        out_dir = summary.get(\"out_dir\")\n",
    "\n",
    "        if not doc_id or not out_dir:\n",
    "            print(f\"⚠️ Skipping an item due to missing 'doc_id' or 'out_dir'.\")\n",
    "            failed_count += 1\n",
    "            continue\n",
    "\n",
    "        # --- Caching Check ---\n",
    "        # Check if the prepared data file already exists\n",
    "        prepared_file_path = os.path.join(out_dir, f\"{doc_id}.prepared.json\")\n",
    "        if os.path.exists(prepared_file_path):\n",
    "            # print(f\"  -> SKIP (already processed): {doc_id}\")\n",
    "            skipped_count += 1\n",
    "            continue\n",
    "        \n",
    "        # --- Process the Document ---\n",
    "        print(f\"\\nProcessing document: {doc_id}\")\n",
    "        prepared_document_data = doc_processor.process_document(summary)\n",
    "\n",
    "        # --- Save the Result ---\n",
    "        if prepared_document_data:\n",
    "            save_path = save_prepared_data(prepared_document_data, summary)\n",
    "            if save_path:\n",
    "                processed_count += 1\n",
    "            else:\n",
    "                print(f\"  -> ❌ FAILED to save: {doc_id}\")\n",
    "                failed_count += 1\n",
    "        else:\n",
    "            print(f\"  -> ❌ FAILED to process: {doc_id}\")\n",
    "            failed_count += 1\n",
    "\n",
    "    print(\"\\n\\n--- ✅ BATCH PROCESSING COMPLETE ---\")\n",
    "    print(f\"  - Successfully Processed: {processed_count}\")\n",
    "    print(f\"  - Skipped (already done): {skipped_count}\")\n",
    "    print(f\"  - Failed: {failed_count}\")\n",
    "    print(\"------------------------------------\")\n",
    "\n",
    "else:\n",
    "    print(\"\\n⚠️ Cannot start batch processing because 'all_results' is empty. Please run the ingestion cell first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45275210",
   "metadata": {},
   "outputs": [],
   "source": [
    "web_data_index = r\"output/verizon_document_index.json\"\n",
    "web_data = r\"output/verizon_production_web_documents.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e59545",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4c77ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Output directory: i:\\My Drive\\M. Tech AI ML\\AIML SEM 4\\Dissertation\\Project\\output\\extraction_results\n",
      "[INFO] GOOGLE_API_KEY loaded (length only) -> 39\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Enhanced Verizon knowledge extraction pipeline (schema-integrated; simplified & fixed).\n",
    "- LLM no longer produces IDs; we deterministically generate eid/sid/evid.\n",
    "- Name→EID mapping + synthesis prevents \"entities-only\" drops.\n",
    "- Predicate normalization removed.\n",
    "- Validation skipped.\n",
    "- Evidences attached to all statements in the same chunk (simple, robust).\n",
    "- Default model: gemini-2.5-flash-lite.\n",
    "- Verbose progress logs + token usage (best effort).\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import os, json, time, random, hashlib, re, traceback\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Iterator, Tuple\n",
    "from collections import defaultdict\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Configuration\n",
    "# ---------------------------------------------------------------------\n",
    "WEB_INDEX_PATH = Path(\"output/verizon_document_index.json\")  # reserved\n",
    "WEB_DOCS_PATH = Path(\"output/verizon_production_web_documents.json\")\n",
    "PDF_PREPARED_ROOT = Path(r\"output/parsed_pdfs/downloaded_verizon_quarterly_pdfs\")\n",
    "OUTPUT_DIR = Path(\"output/extraction_results\")\n",
    "\n",
    "try:\n",
    "    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"[INFO] Output directory: {OUTPUT_DIR.absolute()}\")\n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] Failed to create output directory: {e}\")\n",
    "    OUTPUT_DIR = Path.cwd() / \"output\" / \"extraction_results\"\n",
    "    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"[INFO] Using fallback output directory: {OUTPUT_DIR.absolute()}\")\n",
    "\n",
    "# RATE_LIMIT_SECONDS = 31.0\n",
    "RATE_LIMIT_SECONDS = 15.0\n",
    "JITTER_RANGE = (1.0, 5.0)\n",
    "CHECKPOINT_EVERY_RECORDS = 25\n",
    "CHECKPOINT_EVERY_SECONDS = 300\n",
    "RESUME = True\n",
    "SAVE_JSONL = True\n",
    "MAX_FAILURES = 50\n",
    "ENABLE_PROMPT_CACHE = True\n",
    "ENABLE_TOKEN_USAGE = True\n",
    "# VERBOSE = True  # top-level config\n",
    "VERBOSE = False  # top-level config\n",
    "\n",
    "# >>> how many sample statements/evidences to print per chunk\n",
    "VERBOSE_SAMPLES = 2\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Environment / API key\n",
    "# ---------------------------------------------------------------------\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "except ImportError:\n",
    "    pass  # optional\n",
    "\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not GOOGLE_API_KEY:\n",
    "    raise RuntimeError(\"GOOGLE_API_KEY not set in environment/.env\")\n",
    "print(\"[INFO] GOOGLE_API_KEY loaded (length only) ->\", len(GOOGLE_API_KEY))\n",
    "\n",
    "# Default Gemini model\n",
    "GENAI_MODEL = os.getenv(\"GENAI_MODEL\", \"gemini-2.5-flash-lite\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Utilities\n",
    "# ---------------------------------------------------------------------\n",
    "def _sha(s: str, n: int = 16) -> str:\n",
    "    return hashlib.sha256(s.encode(\"utf-8\")).hexdigest()[:n]\n",
    "\n",
    "def _slug(s: str) -> str:\n",
    "    if not s:\n",
    "        return \"\"\n",
    "    s = re.sub(r\"[^a-z0-9]+\", \"-\", s.lower()).strip(\"-\")\n",
    "    return re.sub(r\"-{2,}\", \"-\", s)\n",
    "\n",
    "def _numify(x: Any) -> Any:\n",
    "    if isinstance(x, (int, float)):\n",
    "        return x\n",
    "    if isinstance(x, str):\n",
    "        s = x.strip().replace(\",\", \"\")\n",
    "        if re.match(r\"^-?\\d+\\.\\d+$\", s):\n",
    "            try: return float(s)\n",
    "            except: return x\n",
    "        if re.match(r\"^-?\\d+$\", s):\n",
    "            try: return int(s)\n",
    "            except: return x\n",
    "    return x\n",
    "\n",
    "def _statement_hash(subject_eid: str, predicate: str, obj: Dict[str, Any]) -> str:\n",
    "    canon = json.dumps({\"s\": subject_eid, \"p\": predicate, \"o\": obj}, sort_keys=True, ensure_ascii=False)\n",
    "    return f\"st:{_sha(canon, 16)}\"\n",
    "\n",
    "def _evidence_hash(doc_id: str, chunk_id: str, span: Dict[str, Any], sid: Optional[str] = None) -> str:\n",
    "    base = f\"{doc_id}|{chunk_id}|{sid or ''}|{json.dumps(span, sort_keys=True, ensure_ascii=False)[:160]}\"\n",
    "    return f\"ev:{_sha(base, 16)}\"\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# LangChain setup\n",
    "# ---------------------------------------------------------------------\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "def build_base_llm(model_name: str, temperature: float = 0.0, top_p: float = 0.9, max_retries: int = 1, max_output_tokens: Optional[int] = 8024):\n",
    "    return ChatGoogleGenerativeAI(\n",
    "        model=model_name,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        max_retries=max_retries,\n",
    "        max_output_tokens=max_output_tokens,  # guard verbosity\n",
    "        # google_api_key=GOOGLE_API_KEY  # reads from env if omitted\n",
    "    )\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# GraphExtractionAgent\n",
    "# ---------------------------------------------------------------------\n",
    "class GraphExtractionAgent:\n",
    "    PROMPT_VERSION = \"v5-no-llm-ids\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 model_name: str = GENAI_MODEL,\n",
    "                 rate_limit: float = RATE_LIMIT_SECONDS,\n",
    "                 jitter_range: Tuple[float, float] = JITTER_RANGE,\n",
    "                 temperature: float = 0.0,\n",
    "                 top_p: float = 0.9,\n",
    "                 max_retries: int = 1,\n",
    "                 enable_cache: bool = ENABLE_PROMPT_CACHE):\n",
    "        self.model_name = model_name\n",
    "        self.rate_limit = rate_limit\n",
    "        self.jitter_range = jitter_range\n",
    "        self.last_request_time: Optional[float] = None\n",
    "        self.stats = defaultdict(int)\n",
    "        self.enable_cache = enable_cache\n",
    "        self._cache: Dict[str, Dict[str, Any]] = {}\n",
    "        \n",
    "        self.verbose = VERBOSE\n",
    "        \n",
    "        # >>> token running totals\n",
    "        self.stats[\"prompt_tokens\"] = 0\n",
    "        self.stats[\"completion_tokens\"] = 0\n",
    "        self.stats[\"total_tokens\"] = 0\n",
    "\n",
    "        base_llm = build_base_llm(model_name, temperature=temperature, top_p=top_p, max_retries=max_retries)\n",
    "        self.llm_table = self._create_table_prompt() | base_llm\n",
    "        self.llm_text  = self._create_text_prompt()  | base_llm\n",
    "\n",
    "    # ---- Prompts (escaped braces kept) ----\n",
    "    def _create_table_prompt(self):\n",
    "        return ChatPromptTemplate.from_template(\"\"\"\n",
    "            You convert a FINANCIAL MARKDOWN TABLE into a knowledge graph **without generating IDs**.\n",
    "\n",
    "            Return ONLY a JSON object:\n",
    "            {{\n",
    "              \"entities\": [{{\"name\": \"...\", \"type\": \"Organization|Person|Metric|TimePeriod|Product|Event|Location|Role|Thing\"}}],\n",
    "              \"statements\": [\n",
    "                {{\n",
    "                  \"subject\": \"...\",\n",
    "                  \"predicate\": \"...\",\n",
    "                  \"object\": (\n",
    "                    {{ \"is_entity\": true,  \"name\": \"...\", \"type\": \"...\" }}\n",
    "                    OR\n",
    "                    {{ \"is_entity\": false, \"value\": <number|string>, \"dtype\": \"Money|Percent|Number|Text\", \"unit\": \"...\", \"currency\": \"...\", \"period\": \"...\" }}\n",
    "                  ),\n",
    "                  \"confidence\": 0.0..1.0\n",
    "                }}\n",
    "              ],\n",
    "              \"evidences\": [\n",
    "                {{ \"span\": {{ \"row_label\": \"...\", \"column\": \"...\" }} OR {{ \"quote\":\"...\" }}, \"confidence\": 0.0..1.0 }}\n",
    "              ]\n",
    "            }}\n",
    "\n",
    "            Rules:\n",
    "            1) Use column headers for period (e.g., \"Q1 2025\") when emitting numeric metric statements.\n",
    "            2) Parse numbers (e.g., \"33,485\" -> 33485). Keep currency/units if shown.\n",
    "            3) Provide concrete statements only; avoid vague claims.\n",
    "\n",
    "            Context:\n",
    "            doc_id={doc_id}\n",
    "            chunk_id={chunk_id}\n",
    "            page={page}\n",
    "            table_id={table_id}\n",
    "            source={source}\n",
    "\n",
    "            Financial_Context (JSON):\n",
    "            {financial_context}\n",
    "\n",
    "            Markdown_Table:\n",
    "            {markdown}\n",
    "            \"\"\".strip())\n",
    "\n",
    "    def _create_text_prompt(self):\n",
    "        return ChatPromptTemplate.from_template(\"\"\"\n",
    "            Extract entities, statements, evidences (NO IDs; we will assign IDs).\n",
    "\n",
    "            Return ONLY a JSON object with keys: entities, statements, evidences.\n",
    "\n",
    "            entities:\n",
    "            - array of objects like: {{ \"name\": \"...\", \"type\": \"Organization|Person|Metric|TimePeriod|Product|Event|Location|Role|Thing\" }}\n",
    "\n",
    "            statements:\n",
    "            - array of objects like:\n",
    "              {{\n",
    "                \"subject\": \"...\",\n",
    "                \"predicate\": \"...\",\n",
    "                \"object\": (\n",
    "                  {{ \"is_entity\": true,  \"name\": \"...\", \"type\": \"...\" }}\n",
    "                  OR\n",
    "                  {{ \"is_entity\": false, \"value\": <number|string>, \"dtype\": \"Money|Percent|Number|Text\", \"unit\": \"...\", \"currency\": \"...\", \"period\": \"...\" }}\n",
    "                ),\n",
    "                \"confidence\": 0.0..1.0\n",
    "              }}\n",
    "\n",
    "            evidences:\n",
    "            - array of objects like: {{ \"span\": {{ \"quote\": \"...\" }}, \"confidence\": 0.0..1.0 }}\n",
    "\n",
    "            Context:\n",
    "            doc_id={doc_id}\n",
    "            chunk_id={chunk_id}\n",
    "            page={page}\n",
    "            label={label}\n",
    "            source={source}\n",
    "\n",
    "            Text:\n",
    "            {text}\n",
    "            \"\"\".strip())\n",
    "\n",
    "    # ---- Rate limit ----\n",
    "    def _enforce_rate_limit(self):\n",
    "        import time\n",
    "        if self.last_request_time is None:\n",
    "            self.last_request_time = time.time()\n",
    "            return\n",
    "        elapsed = time.time() - self.last_request_time\n",
    "        wait = self.rate_limit - elapsed\n",
    "        if wait > 0:\n",
    "            self.stats[\"rate_limit_waits\"] += 1\n",
    "            # >>> show wait time\n",
    "            jitter = random.uniform(*self.jitter_range)\n",
    "            wait += jitter\n",
    "            if self.verbose:\n",
    "                print(f\"[RATE] sleeping {wait:.1f}s (base+{jitter:.1f}s jitter)\")\n",
    "            time.sleep(wait)\n",
    "        self.last_request_time = time.time()\n",
    "\n",
    "    # ---- JSON parsing ----\n",
    "    _TOP_OBJECT_RE = re.compile(r'\\{.*\\}', re.DOTALL)\n",
    "\n",
    "    def _safe_json_loads(self, raw: str) -> Dict[str, Any]:\n",
    "        txt = raw.strip()\n",
    "        if txt.startswith(\"```\"):\n",
    "            txt = re.sub(r\"^```(?:json)?\", \"\", txt, flags=re.IGNORECASE).strip()\n",
    "            if txt.endswith(\"```\"):\n",
    "                txt = txt[:-3].strip()\n",
    "        m = self._TOP_OBJECT_RE.search(txt)\n",
    "        candidate = m.group(0) if m else txt\n",
    "        candidate = re.sub(r',\\s*([\\]}])', r'\\1', candidate)\n",
    "        try:\n",
    "            data = json.loads(candidate)\n",
    "            if isinstance(data, dict):\n",
    "                return data\n",
    "        except Exception:\n",
    "            pass\n",
    "        return {\"entities\": [], \"statements\": [], \"evidences\": []}\n",
    "\n",
    "    # ---- Token accounting (best effort) ----\n",
    "    def _record_usage(self, resp, prompt_vars: Dict[str, Any]) -> Tuple[int,int,int]:\n",
    "        \"\"\"Try to collect prompt/completion/total tokens; fallback to approximate.\"\"\"\n",
    "        pm = cm = tt = 0\n",
    "        meta = getattr(resp, \"response_metadata\", None) or {}\n",
    "        # Gemini often exposes usage as usageMetadata or token_count-like fields\n",
    "        usage = (\n",
    "            meta.get(\"usage_metadata\")\n",
    "            or meta.get(\"usageMetadata\")\n",
    "            or meta.get(\"token_usage\")\n",
    "            or meta.get(\"token_count\")\n",
    "            or {}\n",
    "        )\n",
    "        if isinstance(usage, dict):\n",
    "            # Try common key variants\n",
    "            for k in (\"promptTokenCount\", \"prompt_tokens\", \"input_tokens\", \"prompt_tokens_count\"):\n",
    "                if k in usage: pm = int(usage[k]); break\n",
    "            for k in (\"candidatesTokenCount\", \"completion_tokens\", \"output_tokens\"):\n",
    "                if k in usage: cm = int(usage[k]); break\n",
    "            for k in (\"totalTokenCount\", \"total_tokens\", \"total\"):\n",
    "                if k in usage: tt = int(usage[k]); break\n",
    "        if not tt and (pm or cm):\n",
    "            tt = pm + cm\n",
    "        if not (pm or cm or tt):\n",
    "            # Fallback heuristic (roughly 4 chars per token)\n",
    "            prompt_str = json.dumps(prompt_vars, ensure_ascii=False)\n",
    "            pm = max(1, int(len(prompt_str) / 4))\n",
    "            cm = max(1, int(len(getattr(resp, \"content\", \"\")) / 4))\n",
    "            tt = pm + cm\n",
    "\n",
    "        self.stats[\"prompt_tokens\"] += pm\n",
    "        self.stats[\"completion_tokens\"] += cm\n",
    "        self.stats[\"total_tokens\"] += tt\n",
    "        return pm, cm, tt\n",
    "\n",
    "    # ---- Normalization (build IDs here) ----\n",
    "    def _norm_literal_object(self, o: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        v = _numify(o.get(\"value\"))\n",
    "        dtype = o.get(\"dtype\") or (\"Number\" if isinstance(v, (int, float)) else \"Text\")\n",
    "        out = {\"is_entity\": False, \"value\": v, \"dtype\": dtype}\n",
    "        for k in (\"unit\", \"currency\", \"period\"):\n",
    "            if o.get(k) is not None:\n",
    "                out[k] = o[k]\n",
    "        return out\n",
    "\n",
    "    def _normalize(self, raw: Dict[str, Any], doc_ctx: Dict[str, Any], meta: Dict[str, Any]) -> Dict[str, List[Dict[str, Any]]]:\n",
    "        # 1) Entities → deterministic eids + name map\n",
    "        ents_in = raw.get(\"entities\") or []\n",
    "        entities: List[Dict[str, Any]] = []\n",
    "        name_to_eid: Dict[str, str] = {}\n",
    "        seen_eids = set()\n",
    "\n",
    "        for e in ents_in:\n",
    "            if not isinstance(e, dict):\n",
    "                continue\n",
    "            name = (e.get(\"name\") or \"\").strip()\n",
    "            if not name:\n",
    "                continue\n",
    "            etype = (e.get(\"type\") or \"Thing\").strip()\n",
    "            eid = f\"{_slug(etype)}:{_slug(name)}\"\n",
    "            if eid in seen_eids:\n",
    "                continue\n",
    "            entities.append({\"eid\": eid, \"name\": name, \"type\": etype})\n",
    "            name_to_eid[name.lower()] = eid\n",
    "            seen_eids.add(eid)\n",
    "\n",
    "        # 2) Statements → map names to eids; synthesize entities when needed\n",
    "        stmts_in = raw.get(\"statements\") or []\n",
    "        statements: List[Dict[str, Any]] = []\n",
    "        for s in stmts_in:\n",
    "            if not isinstance(s, dict):\n",
    "                continue\n",
    "            subj_name = (s.get(\"subject\") or s.get(\"subject_eid\") or \"\").strip()\n",
    "            pred = (s.get(\"predicate\") or \"\").strip()\n",
    "            obj = s.get(\"object\")\n",
    "\n",
    "            if not subj_name or not pred or obj is None:\n",
    "                continue\n",
    "\n",
    "            # subject: map or synthesize\n",
    "            subj_eid = name_to_eid.get(subj_name.lower())\n",
    "            if not subj_eid:\n",
    "                subj_eid = f\"thing:{_slug(subj_name)}\"\n",
    "                if subj_eid not in seen_eids:\n",
    "                    entities.append({\"eid\": subj_eid, \"name\": subj_name, \"type\": \"Thing\"})\n",
    "                    name_to_eid[subj_name.lower()] = subj_eid\n",
    "                    seen_eids.add(subj_eid)\n",
    "\n",
    "            # object: entity or literal\n",
    "            if isinstance(obj, dict) and obj.get(\"is_entity\"):\n",
    "                obj_name = (obj.get(\"name\") or \"\").strip()\n",
    "                obj_type = (obj.get(\"type\") or \"Thing\").strip()\n",
    "                if obj_name:\n",
    "                    obj_eid = name_to_eid.get(obj_name.lower())\n",
    "                    if not obj_eid:\n",
    "                        obj_eid = f\"{_slug(obj_type)}:{_slug(obj_name)}\"\n",
    "                        if obj_eid not in seen_eids:\n",
    "                            entities.append({\"eid\": obj_eid, \"name\": obj_name, \"type\": obj_type})\n",
    "                            name_to_eid[obj_name.lower()] = obj_eid\n",
    "                            seen_eids.add(obj_eid)\n",
    "                    object_norm = {\"is_entity\": True, \"eid\": obj_eid}\n",
    "                else:\n",
    "                    object_norm = {\"is_entity\": False, \"value\": str(obj), \"dtype\": \"Text\"}\n",
    "            elif isinstance(obj, dict) and ((\"value\" in obj) or (\"dtype\" in obj) or (\"period\" in obj)):\n",
    "                object_norm = self._norm_literal_object(obj)\n",
    "            else:\n",
    "                object_norm = {\"is_entity\": False, \"value\": str(obj), \"dtype\": \"Text\"}\n",
    "\n",
    "            sid = _statement_hash(subj_eid, pred, object_norm)\n",
    "            conf = float(s.get(\"confidence\", 0.7))\n",
    "            statements.append({\n",
    "                \"sid\": sid,\n",
    "                \"subject_eid\": subj_eid,\n",
    "                \"predicate\": pred,           # NO normalization\n",
    "                \"object\": object_norm,\n",
    "                \"confidence\": conf,\n",
    "                \"model\": self.model_name,\n",
    "                \"prompt_version\": self.PROMPT_VERSION\n",
    "            })\n",
    "\n",
    "        # 3) Evidences → attach spans to all statements in the chunk\n",
    "        evids_in = raw.get(\"evidences\") or []\n",
    "        doc_id = doc_ctx.get(\"doc_id\")\n",
    "        chunk_id = meta.get(\"chunk_id\")\n",
    "        page = meta.get(\"page\")\n",
    "        table_id = meta.get(\"table_id\")\n",
    "        label = meta.get(\"label\") or (\"table\" if table_id else \"text\")\n",
    "\n",
    "        evidences: List[Dict[str, Any]] = []\n",
    "        if evids_in:\n",
    "            for ev in evids_in:\n",
    "                span = ev.get(\"span\") or ev.get(\"quote\") or \"\"\n",
    "                if isinstance(span, str):\n",
    "                    span = {\"quote\": span[:300]}\n",
    "                conf = float(ev.get(\"confidence\", 0.8))\n",
    "                for s in statements:\n",
    "                    evid = _evidence_hash(doc_id or \"\", chunk_id or \"\", span, s[\"sid\"])  # <-- pass sid\n",
    "                    evidences.append({\n",
    "                        \"evid\": evid,\n",
    "                        \"sid\": s[\"sid\"],\n",
    "                        \"doc_id\": doc_id,\n",
    "                        \"chunk_id\": chunk_id,\n",
    "                        \"page\": page,\n",
    "                        \"table_id\": table_id,\n",
    "                        \"label\": label,\n",
    "                        \"span\": span,\n",
    "                        \"confidence\": conf,\n",
    "                        \"model\": self.model_name,\n",
    "                        \"prompt_version\": self.PROMPT_VERSION\n",
    "                    })\n",
    "        else:\n",
    "            for s in statements:\n",
    "                span = {\"note\": \"auto-evidence (no span provided)\"}\n",
    "                evid = _evidence_hash(doc_id or \"\", chunk_id or \"\", span, s['sid'])\n",
    "                evidences.append({\n",
    "                    \"evid\": evid,\n",
    "                    \"sid\": s[\"sid\"],\n",
    "                    \"doc_id\": doc_id,\n",
    "                    \"chunk_id\": chunk_id,\n",
    "                    \"page\": page,\n",
    "                    \"table_id\": table_id,\n",
    "                    \"label\": label,\n",
    "                    \"span\": span,\n",
    "                    \"confidence\": 0.55,\n",
    "                    \"model\": self.model_name,\n",
    "                    \"prompt_version\": self.PROMPT_VERSION\n",
    "                })\n",
    "\n",
    "        return {\"entities\": entities, \"statements\": statements, \"evidences\": evidences}\n",
    "\n",
    "    # ---- Cache key ----\n",
    "    def _cache_key(self, kind: str, payload: Dict[str, Any]) -> str:\n",
    "        base = json.dumps({\"k\": kind, \"p\": payload}, sort_keys=True, ensure_ascii=False)\n",
    "        return hashlib.sha256(base.encode()).hexdigest()\n",
    "\n",
    "    # ---- Public APIs with verbose printing + token accounting ----\n",
    "    def extract_table(self, *, table_markdown: str, doc_ctx: Dict[str, Any], meta: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        if not table_markdown.strip():\n",
    "            return {\"entities\": [], \"statements\": [], \"evidences\": []}\n",
    "        vars_payload = {\n",
    "            \"doc_id\": doc_ctx.get(\"doc_id\"),\n",
    "            \"chunk_id\": meta.get(\"chunk_id\"),\n",
    "            \"page\": meta.get(\"page\"),\n",
    "            \"table_id\": meta.get(\"table_id\"),\n",
    "            \"source\": doc_ctx.get(\"source\"),\n",
    "            \"financial_context\": json.dumps(meta.get(\"financial_context\") or {}, ensure_ascii=False),\n",
    "            \"markdown\": table_markdown\n",
    "        }\n",
    "        ck = self._cache_key(\"table\", vars_payload)\n",
    "        if self.enable_cache and ck in self._cache:\n",
    "            if self.verbose:\n",
    "                print(f\"[HIT] cache table doc={doc_ctx.get('doc_id')} chunk={meta.get('chunk_id')}\")\n",
    "            return self._cache[ck]\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"[RUN] table  doc={doc_ctx.get('doc_id')} chunk={meta.get('chunk_id')} page={meta.get('page')} tbl={meta.get('table_id')}\")\n",
    "        self._enforce_rate_limit()\n",
    "        self.stats[\"requests\"] += 1\n",
    "        try:\n",
    "            resp = self.llm_table.invoke(vars_payload)\n",
    "            pm, cm, tt = self._record_usage(resp, vars_payload)\n",
    "            raw = self._safe_json_loads(resp.content)\n",
    "            norm = self._normalize(raw, doc_ctx, meta)\n",
    "            self._tally(norm)\n",
    "\n",
    "            if self.verbose:\n",
    "                print(f\"[OK ] table  doc={doc_ctx.get('doc_id')} chunk={meta.get('chunk_id')} \"\n",
    "                    f\"ents={len(norm['entities'])} stmts={len(norm['statements'])} evs={len(norm['evidences'])} | tokens p={pm} c={cm} t={tt}\")\n",
    "                for s in norm[\"statements\"][:VERBOSE_SAMPLES]:\n",
    "                    print(\"      • stmt\", json.dumps({\"subj\": s[\"subject_eid\"], \"pred\": s[\"predicate\"], \"obj\": s[\"object\"]}, ensure_ascii=False))\n",
    "                for ev in norm[\"evidences\"][:VERBOSE_SAMPLES]:\n",
    "                    span = ev.get(\"span\", {})\n",
    "                    span_str = json.dumps(span, ensure_ascii=False)\n",
    "                    print(\"      • evid\", span_str if len(span_str) < 160 else json.dumps({\"preview\": span_str[:160] + \"...\"}, ensure_ascii=False))\n",
    "\n",
    "            if self.enable_cache:\n",
    "                self._cache[ck] = norm\n",
    "            return norm\n",
    "        except Exception as e:\n",
    "            self.stats[\"failures\"] += 1\n",
    "            if self.verbose:\n",
    "                print(f\"[ERROR] Table extraction error: {e}\\n{traceback.format_exc()}\")\n",
    "            return {\"entities\": [], \"statements\": [], \"evidences\": []}\n",
    "\n",
    "    def extract_text(self, *, text: str, doc_ctx: Dict[str, Any], meta: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        if not text or len(text.split()) < 5:\n",
    "            return {\"entities\": [], \"statements\": [], \"evidences\": []}\n",
    "        vars_payload = {\n",
    "            \"doc_id\": doc_ctx.get(\"doc_id\"),\n",
    "            \"chunk_id\": meta.get(\"chunk_id\"),\n",
    "            \"page\": meta.get(\"page\"),\n",
    "            \"label\": meta.get(\"label\") or \"text\",\n",
    "            \"source\": doc_ctx.get(\"source\"),\n",
    "            \"text\": text\n",
    "        }\n",
    "        ck = self._cache_key(\"text\", vars_payload)\n",
    "        if self.enable_cache and ck in self._cache:\n",
    "            if self.verbose:\n",
    "                print(f\"[HIT] cache text  doc={doc_ctx.get('doc_id')} chunk={meta.get('chunk_id')}\")\n",
    "            return self._cache[ck]\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"[RUN] text   doc={doc_ctx.get('doc_id')} chunk={meta.get('chunk_id')} page={meta.get('page')} words={len(text.split())}\")\n",
    "        self._enforce_rate_limit()\n",
    "        self.stats[\"requests\"] += 1\n",
    "        try:\n",
    "            resp = self.llm_text.invoke(vars_payload)\n",
    "            pm, cm, tt = self._record_usage(resp, vars_payload)\n",
    "            raw = self._safe_json_loads(resp.content)\n",
    "            norm = self._normalize(raw, doc_ctx, meta)\n",
    "            self._tally(norm)\n",
    "\n",
    "            if self.verbose:\n",
    "                print(f\"[OK ] text   doc={doc_ctx.get('doc_id')} chunk={meta.get('chunk_id')} \"\n",
    "                    f\"ents={len(norm['entities'])} stmts={len(norm['statements'])} evs={len(norm['evidences'])} | tokens p={pm} c={cm} t={tt}\")\n",
    "                for s in norm[\"statements\"][:VERBOSE_SAMPLES]:\n",
    "                    print(\"      • stmt\", json.dumps({\"subj\": s[\"subject_eid\"], \"pred\": s[\"predicate\"], \"obj\": s[\"object\"]}, ensure_ascii=False))\n",
    "                for ev in norm[\"evidences\"][:VERBOSE_SAMPLES]:\n",
    "                    span = ev.get(\"span\", {})\n",
    "                    span_str = json.dumps(span, ensure_ascii=False)\n",
    "                    print(\"      • evid\", span_str if len(span_str) < 160 else json.dumps({\"preview\": span_str[:160] + \"...\"}, ensure_ascii=False))\n",
    "\n",
    "            if self.enable_cache:\n",
    "                self._cache[ck] = norm\n",
    "            return norm\n",
    "        except Exception as e:\n",
    "            self.stats[\"failures\"] += 1\n",
    "            if self.verbose:\n",
    "                print(f\"[ERROR] Text extraction error: {e}\\n{traceback.format_exc()}\")\n",
    "            return {\"entities\": [], \"statements\": [], \"evidences\": []}\n",
    "\n",
    "    def _tally(self, norm: Dict[str, List[Dict[str, Any]]]):\n",
    "        self.stats[\"success\"] += 1\n",
    "        self.stats[\"entities\"] += len(norm[\"entities\"])\n",
    "        self.stats[\"statements\"] += len(norm[\"statements\"])\n",
    "        self.stats[\"evidences\"] += len(norm[\"evidences\"])\n",
    "\n",
    "    def get_statistics(self) -> Dict[str, Any]:\n",
    "        total_req = max(1, self.stats.get(\"requests\", 0))\n",
    "        succ = self.stats.get(\"success\", 0)\n",
    "        return {\n",
    "            **self.stats,\n",
    "            \"success_rate\": succ / total_req,\n",
    "            \"avg_entities_per_success\": self.stats.get(\"entities\", 0) / max(1, succ),\n",
    "            \"avg_statements_per_success\": self.stats.get(\"statements\", 0) / max(1, succ),\n",
    "            \"avg_evidences_per_success\": self.stats.get(\"evidences\", 0) / max(1, succ),\n",
    "        }\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Data loading (web + prepared PDF)  [unchanged]\n",
    "# ---------------------------------------------------------------------\n",
    "def _safe_load_json(path: Path) -> Optional[dict]:\n",
    "    try:\n",
    "        with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "            return json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Failed reading {path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_raw_web_docs() -> Any:\n",
    "    if not WEB_DOCS_PATH.exists():\n",
    "        print(f\"[INFO] Web docs file not found: {WEB_DOCS_PATH}\")\n",
    "        return None\n",
    "    try:\n",
    "        with WEB_DOCS_PATH.open(\"r\", encoding=\"utf-8\") as f:\n",
    "            return json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Failed to load web docs: {e}\")\n",
    "        return None\n",
    "\n",
    "def _normalize_web_docs(raw: Any) -> List[Dict[str, Any]]:\n",
    "    if raw is None:\n",
    "        return []\n",
    "    if isinstance(raw, list):\n",
    "        return [r for r in raw if isinstance(r, dict)]\n",
    "    if isinstance(raw, dict):\n",
    "        if \"documents\" in raw and isinstance(raw[\"documents\"], list):\n",
    "            return [r for r in raw[\"documents\"] if isinstance(r, dict)]\n",
    "        if \"data\" in raw and isinstance(raw[\"data\"], list):\n",
    "            return [r for r in raw[\"data\"] if isinstance(r, dict)]\n",
    "        if all(isinstance(v, dict) for v in raw.values()):\n",
    "            out = []\n",
    "            for k, v in raw.items():\n",
    "                if \"doc_id\" not in v and \"id\" not in v:\n",
    "                    v = {**v, \"doc_id\": k}\n",
    "                out.append(v)\n",
    "            return out\n",
    "    print(\"[WARN] Unrecognized web docs JSON structure; proceeding with empty list.\")\n",
    "    return []\n",
    "\n",
    "def _derive_web_doc_id(meta: Dict[str, Any], index: int) -> str:\n",
    "    for key in (\"doc_id\", \"id\", \"slug\"):\n",
    "        if meta.get(key):\n",
    "            return str(meta[key])\n",
    "    url = meta.get(\"url\")\n",
    "    if url:\n",
    "        return \"web:\" + _sha(url, 12)\n",
    "    snippet = (meta.get(\"text\") or meta.get(\"content\") or \"\")[:80]\n",
    "    return \"web:\" + _sha(f\"{index}|{snippet}\", 12)\n",
    "\n",
    "def iter_web_chunks(max_chars: int = 10000) -> Iterator[Dict[str, Any]]:\n",
    "    raw = load_raw_web_docs()\n",
    "    records = _normalize_web_docs(raw)\n",
    "    if not records:\n",
    "        return\n",
    "    for idx, meta in enumerate(records):\n",
    "        if not isinstance(meta, dict):\n",
    "            continue\n",
    "        doc_id = _derive_web_doc_id(meta, idx)\n",
    "        text = meta.get(\"text\") or meta.get(\"content\") or \"\"\n",
    "        if not isinstance(text, str):\n",
    "            if isinstance(text, list):\n",
    "                text = \" \".join(str(t) for t in text)\n",
    "            else:\n",
    "                text = str(text)\n",
    "        if not text.strip():\n",
    "            continue\n",
    "        url = meta.get(\"url\")\n",
    "        start = 0\n",
    "        chunk_idx = 0\n",
    "        while start < len(text):\n",
    "            piece = text[start:start+max_chars]\n",
    "            yield {\n",
    "                \"source_type\": \"web\",\n",
    "                \"doc_id\": doc_id,\n",
    "                \"chunk_id\": f\"{doc_id}::chunk_{chunk_idx}\",\n",
    "                \"page\": None,\n",
    "                \"table_id\": None,\n",
    "                \"content_type\": \"text\",\n",
    "                \"text\": piece,\n",
    "                \"markdown_table\": None,\n",
    "                \"url\": url,\n",
    "                \"meta\": {k: v for k, v in meta.items() if k not in (\"text\", \"content\")},\n",
    "                \"year\": meta.get(\"year\"),\n",
    "                \"quarter\": meta.get(\"quarter\"),\n",
    "            }\n",
    "            start += max_chars\n",
    "            chunk_idx += 1\n",
    "\n",
    "def iter_prepared_pdf_files(years: Optional[List[str]] = None) -> Iterator[Path]:\n",
    "    if not PDF_PREPARED_ROOT.exists():\n",
    "        return\n",
    "    for p in PDF_PREPARED_ROOT.rglob(\"*.prepared.json\"):\n",
    "        year = next((seg for seg in p.parts if seg.isdigit() and len(seg) == 4), None)\n",
    "        if years and year and year not in years:\n",
    "            continue\n",
    "        yield p\n",
    "\n",
    "def _extract_pdf_tables(d: dict) -> List[dict]:\n",
    "    tables = d.get(\"reconstructed_tables\") or d.get(\"tables\") or []\n",
    "    out = []\n",
    "    for t in tables:\n",
    "        md = t.get(\"markdown\") or t.get(\"markdown_table\") or t.get(\"md\")\n",
    "        if not md:\n",
    "            continue\n",
    "        out.append({\n",
    "            \"table_id\": t.get(\"id\") or t.get(\"table_id\"),\n",
    "            \"page\": t.get(\"page\") or t.get(\"page_no\") or t.get(\"page_number\"),\n",
    "            \"markdown\": md,\n",
    "            \"title\": t.get(\"title\"),\n",
    "            \"financial_context\": t.get(\"financial_context\"),\n",
    "            \"meta\": {k: v for k, v in t.items() if k not in (\"markdown\",\"markdown_table\",\"md\")}\n",
    "        })\n",
    "    return out\n",
    "\n",
    "def _extract_pdf_text(d: dict) -> List[dict]:\n",
    "    chunks = d.get(\"text_chunks\") or d.get(\"chunks\") or []\n",
    "    out = []\n",
    "    for c in chunks:\n",
    "        txt = c.get(\"text\") or c.get(\"content\") or \"\"\n",
    "        if not txt.strip():\n",
    "            continue\n",
    "        out.append({\n",
    "            \"chunk_id\": c.get(\"id\") or c.get(\"chunk_id\"),\n",
    "            \"page\": c.get(\"page\") or c.get(\"page_no\") or c.get(\"page_number\"),\n",
    "            \"text\": txt,\n",
    "            \"meta\": {k: v for k, v in c.items() if k not in (\"text\",\"content\")}\n",
    "        })\n",
    "    return out\n",
    "\n",
    "def iter_pdf_records(years: Optional[List[str]] = None,\n",
    "                     include_tables: bool = True,\n",
    "                     include_text: bool = True) -> Iterator[Dict[str, Any]]:\n",
    "    for path in iter_prepared_pdf_files(years=years):\n",
    "        data = _safe_load_json(path)\n",
    "        if not data:\n",
    "            continue\n",
    "        meta = data.get(\"document_meta\") or data.get(\"meta\") or {}\n",
    "        doc_id = data.get(\"doc_id\") or f\"pdfdoc:{_sha(str(path), 12)}\"\n",
    "        year = next((seg for seg in path.parts if seg.isdigit() and len(seg) == 4), None)\n",
    "        quarter = None\n",
    "        for seg in path.parts:\n",
    "            if len(seg) == 2 and seg[0].isdigit() and seg[1].upper() == \"Q\":\n",
    "                quarter = seg.upper()\n",
    "        if include_tables:\n",
    "            for t in _extract_pdf_tables(data):\n",
    "                yield {\n",
    "                    \"source_type\": \"pdf\",\n",
    "                    \"doc_id\": doc_id,\n",
    "                    \"chunk_id\": f\"{doc_id}::table::{t['table_id']}\",\n",
    "                    \"page\": t[\"page\"],\n",
    "                    \"table_id\": t[\"table_id\"],\n",
    "                    \"content_type\": \"table\",\n",
    "                    \"text\": None,\n",
    "                    \"markdown_table\": t[\"markdown\"],\n",
    "                    \"url\": meta.get(\"source_url\"),\n",
    "                    \"meta\": {**meta, **t.get(\"meta\", {}), \"title\": t.get(\"title\")},\n",
    "                    \"financial_context\": t.get(\"financial_context\"),\n",
    "                    \"source_path\": str(path),\n",
    "                    \"year\": year,\n",
    "                    \"quarter\": quarter\n",
    "                }\n",
    "        if include_text:\n",
    "            for c in _extract_pdf_text(data):\n",
    "                cid = c[\"chunk_id\"] or f\"{doc_id}::chunk::{_sha(str(path)+str(c['page'])+c['text'][:32], 10)}\"\n",
    "                yield {\n",
    "                    \"source_type\": \"pdf\",\n",
    "                    \"doc_id\": doc_id,\n",
    "                    \"chunk_id\": cid,\n",
    "                    \"page\": c[\"page\"],\n",
    "                    \"table_id\": None,\n",
    "                    \"content_type\": \"text\",\n",
    "                    \"text\": c[\"text\"],\n",
    "                    \"markdown_table\": None,\n",
    "                    \"url\": meta.get(\"source_url\"),\n",
    "                    \"meta\": {**meta, **c.get(\"meta\", {})},\n",
    "                    \"source_path\": str(path),\n",
    "                    \"year\": year,\n",
    "                    \"quarter\": quarter\n",
    "                }\n",
    "\n",
    "def iter_all_records(years: Optional[List[str]] = None,\n",
    "                     include_web: bool = True,\n",
    "                     include_pdf_tables: bool = True,\n",
    "                     include_pdf_text: bool = True) -> Iterator[Dict[str, Any]]:\n",
    "    if include_web:\n",
    "        yield from iter_web_chunks()\n",
    "    yield from iter_pdf_records(years=years,\n",
    "                                include_tables=include_pdf_tables,\n",
    "                                include_text=include_pdf_text)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Aggregation / Merge\n",
    "# ---------------------------------------------------------------------\n",
    "@dataclass\n",
    "class AggregateStore:\n",
    "    entities: Dict[str, Dict[str, Any]] = field(default_factory=dict)\n",
    "    statements: Dict[str, Dict[str, Any]] = field(default_factory=dict)   # sid -> statement\n",
    "    evidences: Dict[str, Dict[str, Any]] = field(default_factory=dict)    # evid -> evidence\n",
    "    stmt_key_index: Dict[str, str] = field(default_factory=dict)          # canonical key -> sid\n",
    "\n",
    "    def _statement_canonical_key(self, s: Dict[str, Any]) -> str:\n",
    "        obj = json.dumps(s[\"object\"], sort_keys=True, ensure_ascii=False)\n",
    "        return f\"{s['subject_eid']}|{s['predicate']}|{obj}\"\n",
    "\n",
    "    def merge_batch(self, batch: Dict[str, List[Dict[str, Any]]]):\n",
    "        # entities\n",
    "        for e in batch[\"entities\"]:\n",
    "            self.entities[e[\"eid\"]] = e\n",
    "        # statements with dedupe\n",
    "        for s in batch[\"statements\"]:\n",
    "            skey = self._statement_canonical_key(s)\n",
    "            existing_sid = self.stmt_key_index.get(skey)\n",
    "            if existing_sid:\n",
    "                existing = self.statements[existing_sid]\n",
    "                existing[\"confidence\"] = max(existing.get(\"confidence\", 0), s.get(\"confidence\", 0))\n",
    "            else:\n",
    "                self.statements[s[\"sid\"]] = s\n",
    "                self.stmt_key_index[skey] = s[\"sid\"]\n",
    "        # evidences\n",
    "        for ev in batch[\"evidences\"]:\n",
    "            self.evidences[ev[\"evid\"]] = ev\n",
    "\n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        sid_to_evids = defaultdict(list)\n",
    "        for ev in self.evidences.values():\n",
    "            sid_to_evids[ev[\"sid\"]].append(ev[\"evid\"])\n",
    "        out_statements = []\n",
    "        for s in self.statements.values():\n",
    "            s_out = dict(s)\n",
    "            s_out[\"evidence_ids\"] = sid_to_evids.get(s[\"sid\"], [])\n",
    "            out_statements.append(s_out)\n",
    "        return {\n",
    "            \"entities\": list(self.entities.values()),\n",
    "            \"statements\": out_statements,\n",
    "            \"evidences\": list(self.evidences.values())\n",
    "        }\n",
    "\n",
    "    def save(self, path: Path):\n",
    "        tmp = path.with_suffix(\".tmp\")\n",
    "        with tmp.open(\"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(self.to_dict(), f, indent=2)\n",
    "        tmp.replace(path)\n",
    "\n",
    "    def save_jsonl(self, base_dir: Path):\n",
    "        data = self.to_dict()\n",
    "        for name, seq in data.items():\n",
    "            p = base_dir / f\"{name}.jsonl\"\n",
    "            tmp = p.with_suffix(\".tmp\")\n",
    "            with tmp.open(\"w\", encoding=\"utf-8\") as f:\n",
    "                for obj in seq:\n",
    "                    f.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "            tmp.replace(p)\n",
    "\n",
    "def load_existing_store(path: Path) -> AggregateStore:\n",
    "    if not path.exists():\n",
    "        return AggregateStore()\n",
    "    try:\n",
    "        data = json.loads(path.read_text(encoding=\"utf-8\"))\n",
    "        store = AggregateStore()\n",
    "        for e in data.get(\"entities\", []):\n",
    "            store.entities[e[\"eid\"]] = e\n",
    "        for s in data.get(\"statements\", []):\n",
    "            store.statements[s[\"sid\"]] = s\n",
    "            store.stmt_key_index[store._statement_canonical_key(s)] = s[\"sid\"]\n",
    "        for ev in data.get(\"evidences\", []):\n",
    "            store.evidences[ev[\"evid\"]] = ev\n",
    "        return store\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Failed loading existing aggregate: {e}\")\n",
    "        return AggregateStore()\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Orchestrator\n",
    "# ---------------------------------------------------------------------\n",
    "def run_extraction(years: Optional[List[str]] = None,\n",
    "                   include_web: bool = True,\n",
    "                   include_pdf_tables: bool = True,\n",
    "                   include_pdf_text: bool = True,\n",
    "                   max_records: Optional[int] = None,\n",
    "                   output_name: str = \"knowledge_extraction_aggregate.json\",\n",
    "                   model_name: str = GENAI_MODEL) -> Dict[str, Any]:\n",
    "    aggregate_path = OUTPUT_DIR / output_name\n",
    "    store = load_existing_store(aggregate_path) if RESUME else AggregateStore()\n",
    "    \n",
    "    # New addition\n",
    "    # ===============================================================\n",
    "    pre_counts = {\n",
    "        \"entities\": len(store.entities),\n",
    "        \"statements\": len(store.statements),\n",
    "        \"evidences\": len(store.evidences),\n",
    "    }\n",
    "    print(\"[INIT] store entities=%d statements=%d evidences=%d\" %\n",
    "        (pre_counts[\"entities\"], pre_counts[\"statements\"], pre_counts[\"evidences\"]))\n",
    "    # ==============================================================\n",
    "\n",
    "    extractor = GraphExtractionAgent(model_name=model_name)\n",
    "\n",
    "    # Optional token usage measurement via callback context (kept; may or may not populate)\n",
    "    if ENABLE_TOKEN_USAGE:\n",
    "        try:\n",
    "            from langchain.callbacks.token_usage import TokenUsageCallbackHandler\n",
    "            token_handler = TokenUsageCallbackHandler()\n",
    "            token_handler.__enter__()  # start tracking\n",
    "            print(\"[INFO] TokenUsageCallbackHandler active\")\n",
    "        except Exception:\n",
    "            token_handler = None\n",
    "    else:\n",
    "        token_handler = None\n",
    "\n",
    "    failures = 0\n",
    "    processed = 0\n",
    "    start_time = time.time()\n",
    "    last_checkpoint = start_time\n",
    "\n",
    "    for rec in iter_all_records(years=years,\n",
    "                                include_web=include_web,\n",
    "                                include_pdf_tables=include_pdf_tables,\n",
    "                                include_pdf_text=include_pdf_text):\n",
    "        if max_records and processed >= max_records:\n",
    "            break\n",
    "\n",
    "        doc_ctx = {\"doc_id\": rec[\"doc_id\"], \"source\": rec.get(\"url\")}\n",
    "        meta = {\n",
    "            \"chunk_id\": rec[\"chunk_id\"],\n",
    "            \"page\": rec.get(\"page\"),\n",
    "            \"table_id\": rec.get(\"table_id\"),\n",
    "            \"label\": \"table\" if rec[\"content_type\"] == \"table\" else \"text\",\n",
    "            \"financial_context\": rec.get(\"financial_context\")\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            if rec[\"content_type\"] == \"table\" and rec.get(\"markdown_table\"):\n",
    "                batch = extractor.extract_table(table_markdown=rec[\"markdown_table\"], doc_ctx=doc_ctx, meta=meta)\n",
    "            else:\n",
    "                batch = extractor.extract_text(text=rec.get(\"text\") or \"\", doc_ctx=doc_ctx, meta=meta)\n",
    "            store.merge_batch(batch)\n",
    "        except Exception as e:\n",
    "            failures += 1\n",
    "            print(f\"[ERROR] Extraction failure doc_id={rec['doc_id']} type={rec['content_type']}: {e}\")\n",
    "            if failures >= MAX_FAILURES:\n",
    "                print(\"[ABORT] Failure threshold reached.\")\n",
    "                break\n",
    "\n",
    "        processed += 1\n",
    "        now = time.time()\n",
    "        if (processed % CHECKPOINT_EVERY_RECORDS == 0) or (now - last_checkpoint >= CHECKPOINT_EVERY_SECONDS):\n",
    "            store.save(aggregate_path)\n",
    "            if SAVE_JSONL:\n",
    "                store.save_jsonl(OUTPUT_DIR)\n",
    "            tokens_total = extractor.stats.get(\"total_tokens\", 0)\n",
    "            print(f\"[CHECKPOINT] processed={processed} ents={len(store.entities)} stmts={len(store.statements)} \"\n",
    "                f\"evs={len(store.evidences)} elapsed={int(now-start_time)}s tokens_total={tokens_total}\")\n",
    "            last_checkpoint = now\n",
    "\n",
    "    # Final save\n",
    "    store.save(aggregate_path)\n",
    "    if SAVE_JSONL:\n",
    "        store.save_jsonl(OUTPUT_DIR)\n",
    "\n",
    "    stats_agent = extractor.get_statistics()\n",
    "    summary = {\n",
    "        \"processed_records\": processed,\n",
    "        \"entities\": len(store.entities),\n",
    "        \"statements\": len(store.statements),\n",
    "        \"evidences\": len(store.evidences),\n",
    "        \"failures\": failures,\n",
    "        \"elapsed_seconds\": int(time.time() - start_time),\n",
    "        \"agent_stats\": stats_agent\n",
    "    }\n",
    "\n",
    "    # Close token tracking\n",
    "    if token_handler:\n",
    "        try:\n",
    "            token_handler.__exit__(None, None, None)\n",
    "            summary[\"token_usage_callback\"] = getattr(token_handler, \"total_usage\", {})\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # >>> Always include our in-agent totals\n",
    "    summary[\"token_usage_agent\"] = {\n",
    "        \"prompt_tokens\": stats_agent.get(\"prompt_tokens\", 0),\n",
    "        \"completion_tokens\": stats_agent.get(\"completion_tokens\", 0),\n",
    "        \"total_tokens\": stats_agent.get(\"total_tokens\", 0),\n",
    "    }\n",
    "\n",
    "    with (OUTPUT_DIR / \"run_summary.json\").open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    print(\"[DONE]\", json.dumps(summary, indent=2))\n",
    "    \n",
    "    # Added Code\n",
    "    # ================================================================================\n",
    "    post_counts = {\n",
    "        \"entities\": len(store.entities),\n",
    "        \"statements\": len(store.statements),\n",
    "        \"evidences\": len(store.evidences),\n",
    "    }\n",
    "    summary[\"delta\"] = {\n",
    "        \"entities_added\": post_counts[\"entities\"] - pre_counts[\"entities\"],\n",
    "        \"statements_added\": post_counts[\"statements\"] - pre_counts[\"statements\"],\n",
    "        \"evidences_added\": post_counts[\"evidences\"] - pre_counts[\"evidences\"],\n",
    "    }\n",
    "\n",
    "    # ================================================================================\n",
    "    \n",
    "    \n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8aa639e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Enhanced Verizon knowledge extraction pipeline (schema-integrated; simplified & fixed).\n",
    "- LLM no longer produces IDs; we deterministically generate eid/sid/evid.\n",
    "- Name→EID mapping + synthesis prevents \"entities-only\" drops.\n",
    "- Predicate normalization removed.\n",
    "- Validation skipped.\n",
    "- Evidences attached to all statements in the same chunk (simple, robust).\n",
    "- Default model: gemini-2.5-flash-lite.\n",
    "- Verbose progress logs + token usage (best effort).\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import os, json, time, random, hashlib, re, traceback\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Iterator, Tuple\n",
    "from collections import defaultdict\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Configuration\n",
    "# ---------------------------------------------------------------------\n",
    "WEB_INDEX_PATH = Path(\"output/verizon_document_index.json\")  # reserved\n",
    "WEB_DOCS_PATH = Path(\"output/verizon_production_web_documents.json\")\n",
    "PDF_PREPARED_ROOT = Path(r\"output/parsed_pdfs/downloaded_verizon_quarterly_pdfs\")\n",
    "OUTPUT_DIR = Path(\"output/extraction_results\")\n",
    "\n",
    "try:\n",
    "    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"[INFO] Output directory: {OUTPUT_DIR.absolute()}\")\n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] Failed to create output directory: {e}\")\n",
    "    OUTPUT_DIR = Path.cwd() / \"output\" / \"extraction_results\"\n",
    "    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"[INFO] Using fallback output directory: {OUTPUT_DIR.absolute()}\")\n",
    "\n",
    "# RATE_LIMIT_SECONDS = 31.0\n",
    "RATE_LIMIT_SECONDS = 15.0\n",
    "JITTER_RANGE = (1.0, 5.0)\n",
    "CHECKPOINT_EVERY_RECORDS = 25\n",
    "CHECKPOINT_EVERY_SECONDS = 300\n",
    "RESUME = True\n",
    "SAVE_JSONL = True\n",
    "MAX_FAILURES = 50\n",
    "ENABLE_PROMPT_CACHE = True\n",
    "ENABLE_TOKEN_USAGE = True\n",
    "# VERBOSE = True  # top-level config\n",
    "VERBOSE = False  # top-level config\n",
    "\n",
    "# >>> how many sample statements/evidences to print per chunk\n",
    "VERBOSE_SAMPLES = 2\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Environment / API key\n",
    "# ---------------------------------------------------------------------\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "except ImportError:\n",
    "    pass  # optional\n",
    "\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not GOOGLE_API_KEY:\n",
    "    raise RuntimeError(\"GOOGLE_API_KEY not set in environment/.env\")\n",
    "print(\"[INFO] GOOGLE_API_KEY loaded (length only) ->\", len(GOOGLE_API_KEY))\n",
    "\n",
    "# Default Gemini model\n",
    "GENAI_MODEL = os.getenv(\"GENAI_MODEL\", \"gemini-2.5-flash-lite\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Utilities\n",
    "# ---------------------------------------------------------------------\n",
    "def _sha(s: str, n: int = 16) -> str:\n",
    "    return hashlib.sha256(s.encode(\"utf-8\")).hexdigest()[:n]\n",
    "\n",
    "def _slug(s: str) -> str:\n",
    "    if not s:\n",
    "        return \"\"\n",
    "    s = re.sub(r\"[^a-z0-9]+\", \"-\", s.lower()).strip(\"-\")\n",
    "    return re.sub(r\"-{2,}\", \"-\", s)\n",
    "\n",
    "def _numify(x: Any) -> Any:\n",
    "    if isinstance(x, (int, float)):\n",
    "        return x\n",
    "    if isinstance(x, str):\n",
    "        s = x.strip().replace(\",\", \"\")\n",
    "        if re.match(r\"^-?\\d+\\.\\d+$\", s):\n",
    "            try: return float(s)\n",
    "            except: return x\n",
    "        if re.match(r\"^-?\\d+$\", s):\n",
    "            try: return int(s)\n",
    "            except: return x\n",
    "    return x\n",
    "\n",
    "def _statement_hash(subject_eid: str, predicate: str, obj: Dict[str, Any]) -> str:\n",
    "    canon = json.dumps({\"s\": subject_eid, \"p\": predicate, \"o\": obj}, sort_keys=True, ensure_ascii=False)\n",
    "    return f\"st:{_sha(canon, 16)}\"\n",
    "\n",
    "def _evidence_hash(doc_id: str, chunk_id: str, span: Dict[str, Any], sid: Optional[str] = None) -> str:\n",
    "    base = f\"{doc_id}|{chunk_id}|{sid or ''}|{json.dumps(span, sort_keys=True, ensure_ascii=False)[:160]}\"\n",
    "    return f\"ev:{_sha(base, 16)}\"\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# LangChain setup\n",
    "# ---------------------------------------------------------------------\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "def build_base_llm(model_name: str, temperature: float = 0.0, top_p: float = 0.9, max_retries: int = 1, max_output_tokens: Optional[int] = 8024):\n",
    "    return ChatGoogleGenerativeAI(\n",
    "        model=model_name,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        max_retries=max_retries,\n",
    "        max_output_tokens=max_output_tokens,  # guard verbosity\n",
    "        # google_api_key=GOOGLE_API_KEY  # reads from env if omitted\n",
    "    )\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# GraphExtractionAgent\n",
    "# ---------------------------------------------------------------------\n",
    "class GraphExtractionAgent:\n",
    "    PROMPT_VERSION = \"v5-no-llm-ids\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 model_name: str = GENAI_MODEL,\n",
    "                 rate_limit: float = RATE_LIMIT_SECONDS,\n",
    "                 jitter_range: Tuple[float, float] = JITTER_RANGE,\n",
    "                 temperature: float = 0.0,\n",
    "                 top_p: float = 0.9,\n",
    "                 max_retries: int = 1,\n",
    "                 enable_cache: bool = ENABLE_PROMPT_CACHE):\n",
    "        self.model_name = model_name\n",
    "        self.rate_limit = rate_limit\n",
    "        self.jitter_range = jitter_range\n",
    "        self.last_request_time: Optional[float] = None\n",
    "        self.stats = defaultdict(int)\n",
    "        self.enable_cache = enable_cache\n",
    "        self._cache: Dict[str, Dict[str, Any]] = {}\n",
    "        \n",
    "        self.verbose = VERBOSE\n",
    "        \n",
    "        # >>> token running totals\n",
    "        self.stats[\"prompt_tokens\"] = 0\n",
    "        self.stats[\"completion_tokens\"] = 0\n",
    "        self.stats[\"total_tokens\"] = 0\n",
    "\n",
    "        base_llm = build_base_llm(model_name, temperature=temperature, top_p=top_p, max_retries=max_retries)\n",
    "        self.llm_table = self._create_table_prompt() | base_llm\n",
    "        self.llm_text  = self._create_text_prompt()  | base_llm\n",
    "\n",
    "    # ---- Prompts (escaped braces kept) ----\n",
    "    def _create_table_prompt(self):\n",
    "        return ChatPromptTemplate.from_template(\"\"\"\n",
    "            You convert a FINANCIAL MARKDOWN TABLE into a knowledge graph **without generating IDs**.\n",
    "\n",
    "            Return ONLY a JSON object:\n",
    "            {{\n",
    "              \"entities\": [{{\"name\": \"...\", \"type\": \"Organization|Person|Metric|TimePeriod|Product|Event|Location|Role|Thing\"}}],\n",
    "              \"statements\": [\n",
    "                {{\n",
    "                  \"subject\": \"...\",\n",
    "                  \"predicate\": \"...\",\n",
    "                  \"object\": (\n",
    "                    {{ \"is_entity\": true,  \"name\": \"...\", \"type\": \"...\" }}\n",
    "                    OR\n",
    "                    {{ \"is_entity\": false, \"value\": <number|string>, \"dtype\": \"Money|Percent|Number|Text\", \"unit\": \"...\", \"currency\": \"...\", \"period\": \"...\" }}\n",
    "                  ),\n",
    "                  \"confidence\": 0.0..1.0\n",
    "                }}\n",
    "              ],\n",
    "              \"evidences\": [\n",
    "                {{ \"span\": {{ \"row_label\": \"...\", \"column\": \"...\" }} OR {{ \"quote\":\"...\" }}, \"confidence\": 0.0..1.0 }}\n",
    "              ]\n",
    "            }}\n",
    "\n",
    "            Rules:\n",
    "            1) Use column headers for period (e.g., \"Q1 2025\") when emitting numeric metric statements.\n",
    "            2) Parse numbers (e.g., \"33,485\" -> 33485). Keep currency/units if shown.\n",
    "            3) Provide concrete statements only; avoid vague claims.\n",
    "\n",
    "            Context:\n",
    "            doc_id={doc_id}\n",
    "            chunk_id={chunk_id}\n",
    "            page={page}\n",
    "            table_id={table_id}\n",
    "            source={source}\n",
    "\n",
    "            Financial_Context (JSON):\n",
    "            {financial_context}\n",
    "\n",
    "            Markdown_Table:\n",
    "            {markdown}\n",
    "            \"\"\".strip())\n",
    "\n",
    "    def _create_text_prompt(self):\n",
    "        return ChatPromptTemplate.from_template(\"\"\"\n",
    "            Extract entities, statements, evidences (NO IDs; we will assign IDs).\n",
    "\n",
    "            Return ONLY a JSON object with keys: entities, statements, evidences.\n",
    "\n",
    "            entities:\n",
    "            - array of objects like: {{ \"name\": \"...\", \"type\": \"Organization|Person|Metric|TimePeriod|Product|Event|Location|Role|Thing\" }}\n",
    "\n",
    "            statements:\n",
    "            - array of objects like:\n",
    "              {{\n",
    "                \"subject\": \"...\",\n",
    "                \"predicate\": \"...\",\n",
    "                \"object\": (\n",
    "                  {{ \"is_entity\": true,  \"name\": \"...\", \"type\": \"...\" }}\n",
    "                  OR\n",
    "                  {{ \"is_entity\": false, \"value\": <number|string>, \"dtype\": \"Money|Percent|Number|Text\", \"unit\": \"...\", \"currency\": \"...\", \"period\": \"...\" }}\n",
    "                ),\n",
    "                \"confidence\": 0.0..1.0\n",
    "              }}\n",
    "\n",
    "            evidences:\n",
    "            - array of objects like: {{ \"span\": {{ \"quote\": \"...\" }}, \"confidence\": 0.0..1.0 }}\n",
    "\n",
    "            Context:\n",
    "            doc_id={doc_id}\n",
    "            chunk_id={chunk_id}\n",
    "            page={page}\n",
    "            label={label}\n",
    "            source={source}\n",
    "\n",
    "            Text:\n",
    "            {text}\n",
    "            \"\"\".strip())\n",
    "\n",
    "    # ---- Rate limit ----\n",
    "    def _enforce_rate_limit(self):\n",
    "        import time\n",
    "        if self.last_request_time is None:\n",
    "            self.last_request_time = time.time()\n",
    "            return\n",
    "        elapsed = time.time() - self.last_request_time\n",
    "        wait = self.rate_limit - elapsed\n",
    "        if wait > 0:\n",
    "            self.stats[\"rate_limit_waits\"] += 1\n",
    "            # >>> show wait time\n",
    "            jitter = random.uniform(*self.jitter_range)\n",
    "            wait += jitter\n",
    "            if self.verbose:\n",
    "                print(f\"[RATE] sleeping {wait:.1f}s (base+{jitter:.1f}s jitter)\")\n",
    "            time.sleep(wait)\n",
    "        self.last_request_time = time.time()\n",
    "\n",
    "    # ---- JSON parsing ----\n",
    "    _TOP_OBJECT_RE = re.compile(r'\\{.*\\}', re.DOTALL)\n",
    "\n",
    "    def _safe_json_loads(self, raw: str) -> Dict[str, Any]:\n",
    "        txt = raw.strip()\n",
    "        if txt.startswith(\"```\"):\n",
    "            txt = re.sub(r\"^```(?:json)?\", \"\", txt, flags=re.IGNORECASE).strip()\n",
    "            if txt.endswith(\"```\"):\n",
    "                txt = txt[:-3].strip()\n",
    "        m = self._TOP_OBJECT_RE.search(txt)\n",
    "        candidate = m.group(0) if m else txt\n",
    "        candidate = re.sub(r',\\s*([\\]}])', r'\\1', candidate)\n",
    "        try:\n",
    "            data = json.loads(candidate)\n",
    "            if isinstance(data, dict):\n",
    "                return data\n",
    "        except Exception:\n",
    "            pass\n",
    "        return {\"entities\": [], \"statements\": [], \"evidences\": []}\n",
    "\n",
    "    # ---- Token accounting (best effort) ----\n",
    "    def _record_usage(self, resp, prompt_vars: Dict[str, Any]) -> Tuple[int,int,int]:\n",
    "        \"\"\"Try to collect prompt/completion/total tokens; fallback to approximate.\"\"\"\n",
    "        pm = cm = tt = 0\n",
    "        meta = getattr(resp, \"response_metadata\", None) or {}\n",
    "        # Gemini often exposes usage as usageMetadata or token_count-like fields\n",
    "        usage = (\n",
    "            meta.get(\"usage_metadata\")\n",
    "            or meta.get(\"usageMetadata\")\n",
    "            or meta.get(\"token_usage\")\n",
    "            or meta.get(\"token_count\")\n",
    "            or {}\n",
    "        )\n",
    "        if isinstance(usage, dict):\n",
    "            # Try common key variants\n",
    "            for k in (\"promptTokenCount\", \"prompt_tokens\", \"input_tokens\", \"prompt_tokens_count\"):\n",
    "                if k in usage: pm = int(usage[k]); break\n",
    "            for k in (\"candidatesTokenCount\", \"completion_tokens\", \"output_tokens\"):\n",
    "                if k in usage: cm = int(usage[k]); break\n",
    "            for k in (\"totalTokenCount\", \"total_tokens\", \"total\"):\n",
    "                if k in usage: tt = int(usage[k]); break\n",
    "        if not tt and (pm or cm):\n",
    "            tt = pm + cm\n",
    "        if not (pm or cm or tt):\n",
    "            # Fallback heuristic (roughly 4 chars per token)\n",
    "            prompt_str = json.dumps(prompt_vars, ensure_ascii=False)\n",
    "            pm = max(1, int(len(prompt_str) / 4))\n",
    "            cm = max(1, int(len(getattr(resp, \"content\", \"\")) / 4))\n",
    "            tt = pm + cm\n",
    "\n",
    "        self.stats[\"prompt_tokens\"] += pm\n",
    "        self.stats[\"completion_tokens\"] += cm\n",
    "        self.stats[\"total_tokens\"] += tt\n",
    "        return pm, cm, tt\n",
    "\n",
    "    # ---- Normalization (build IDs here) ----\n",
    "    def _norm_literal_object(self, o: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        v = _numify(o.get(\"value\"))\n",
    "        dtype = o.get(\"dtype\") or (\"Number\" if isinstance(v, (int, float)) else \"Text\")\n",
    "        out = {\"is_entity\": False, \"value\": v, \"dtype\": dtype}\n",
    "        for k in (\"unit\", \"currency\", \"period\"):\n",
    "            if o.get(k) is not None:\n",
    "                out[k] = o[k]\n",
    "        return out\n",
    "\n",
    "    def _normalize(self, raw: Dict[str, Any], doc_ctx: Dict[str, Any], meta: Dict[str, Any]) -> Dict[str, List[Dict[str, Any]]]:\n",
    "        # 1) Entities → deterministic eids + name map\n",
    "        ents_in = raw.get(\"entities\") or []\n",
    "        entities: List[Dict[str, Any]] = []\n",
    "        name_to_eid: Dict[str, str] = {}\n",
    "        seen_eids = set()\n",
    "\n",
    "        for e in ents_in:\n",
    "            if not isinstance(e, dict):\n",
    "                continue\n",
    "            name = (e.get(\"name\") or \"\").strip()\n",
    "            if not name:\n",
    "                continue\n",
    "            etype = (e.get(\"type\") or \"Thing\").strip()\n",
    "            eid = f\"{_slug(etype)}:{_slug(name)}\"\n",
    "            if eid in seen_eids:\n",
    "                continue\n",
    "            entities.append({\"eid\": eid, \"name\": name, \"type\": etype})\n",
    "            name_to_eid[name.lower()] = eid\n",
    "            seen_eids.add(eid)\n",
    "\n",
    "        # 2) Statements → map names to eids; synthesize entities when needed\n",
    "        stmts_in = raw.get(\"statements\") or []\n",
    "        statements: List[Dict[str, Any]] = []\n",
    "        for s in stmts_in:\n",
    "            if not isinstance(s, dict):\n",
    "                continue\n",
    "            subj_name = (s.get(\"subject\") or s.get(\"subject_eid\") or \"\").strip()\n",
    "            pred = (s.get(\"predicate\") or \"\").strip()\n",
    "            obj = s.get(\"object\")\n",
    "\n",
    "            if not subj_name or not pred or obj is None:\n",
    "                continue\n",
    "\n",
    "            # subject: map or synthesize\n",
    "            subj_eid = name_to_eid.get(subj_name.lower())\n",
    "            if not subj_eid:\n",
    "                subj_eid = f\"thing:{_slug(subj_name)}\"\n",
    "                if subj_eid not in seen_eids:\n",
    "                    entities.append({\"eid\": subj_eid, \"name\": subj_name, \"type\": \"Thing\"})\n",
    "                    name_to_eid[subj_name.lower()] = subj_eid\n",
    "                    seen_eids.add(subj_eid)\n",
    "\n",
    "            # object: entity or literal\n",
    "            if isinstance(obj, dict) and obj.get(\"is_entity\"):\n",
    "                obj_name = (obj.get(\"name\") or \"\").strip()\n",
    "                obj_type = (obj.get(\"type\") or \"Thing\").strip()\n",
    "                if obj_name:\n",
    "                    obj_eid = name_to_eid.get(obj_name.lower())\n",
    "                    if not obj_eid:\n",
    "                        obj_eid = f\"{_slug(obj_type)}:{_slug(obj_name)}\"\n",
    "                        if obj_eid not in seen_eids:\n",
    "                            entities.append({\"eid\": obj_eid, \"name\": obj_name, \"type\": obj_type})\n",
    "                            name_to_eid[obj_name.lower()] = obj_eid\n",
    "                            seen_eids.add(obj_eid)\n",
    "                    object_norm = {\"is_entity\": True, \"eid\": obj_eid}\n",
    "                else:\n",
    "                    object_norm = {\"is_entity\": False, \"value\": str(obj), \"dtype\": \"Text\"}\n",
    "            elif isinstance(obj, dict) and ((\"value\" in obj) or (\"dtype\" in obj) or (\"period\" in obj)):\n",
    "                object_norm = self._norm_literal_object(obj)\n",
    "            else:\n",
    "                object_norm = {\"is_entity\": False, \"value\": str(obj), \"dtype\": \"Text\"}\n",
    "\n",
    "            sid = _statement_hash(subj_eid, pred, object_norm)\n",
    "            conf = float(s.get(\"confidence\", 0.7))\n",
    "            statements.append({\n",
    "                \"sid\": sid,\n",
    "                \"subject_eid\": subj_eid,\n",
    "                \"predicate\": pred,           # NO normalization\n",
    "                \"object\": object_norm,\n",
    "                \"confidence\": conf,\n",
    "                \"model\": self.model_name,\n",
    "                \"prompt_version\": self.PROMPT_VERSION\n",
    "            })\n",
    "\n",
    "        # 3) Evidences → attach spans to all statements in the chunk\n",
    "        evids_in = raw.get(\"evidences\") or []\n",
    "        doc_id = doc_ctx.get(\"doc_id\")\n",
    "        chunk_id = meta.get(\"chunk_id\")\n",
    "        page = meta.get(\"page\")\n",
    "        table_id = meta.get(\"table_id\")\n",
    "        label = meta.get(\"label\") or (\"table\" if table_id else \"text\")\n",
    "\n",
    "        evidences: List[Dict[str, Any]] = []\n",
    "        if evids_in:\n",
    "            for ev in evids_in:\n",
    "                span = ev.get(\"span\") or ev.get(\"quote\") or \"\"\n",
    "                if isinstance(span, str):\n",
    "                    span = {\"quote\": span[:300]}\n",
    "                conf = float(ev.get(\"confidence\", 0.8))\n",
    "                for s in statements:\n",
    "                    evid = _evidence_hash(doc_id or \"\", chunk_id or \"\", span, s[\"sid\"])  # <-- pass sid\n",
    "                    evidences.append({\n",
    "                        \"evid\": evid,\n",
    "                        \"sid\": s[\"sid\"],\n",
    "                        \"doc_id\": doc_id,\n",
    "                        \"chunk_id\": chunk_id,\n",
    "                        \"page\": page,\n",
    "                        \"table_id\": table_id,\n",
    "                        \"label\": label,\n",
    "                        \"span\": span,\n",
    "                        \"confidence\": conf,\n",
    "                        \"model\": self.model_name,\n",
    "                        \"prompt_version\": self.PROMPT_VERSION\n",
    "                    })\n",
    "        else:\n",
    "            for s in statements:\n",
    "                span = {\"note\": \"auto-evidence (no span provided)\"}\n",
    "                evid = _evidence_hash(doc_id or \"\", chunk_id or \"\", span, s['sid'])\n",
    "                evidences.append({\n",
    "                    \"evid\": evid,\n",
    "                    \"sid\": s[\"sid\"],\n",
    "                    \"doc_id\": doc_id,\n",
    "                    \"chunk_id\": chunk_id,\n",
    "                    \"page\": page,\n",
    "                    \"table_id\": table_id,\n",
    "                    \"label\": label,\n",
    "                    \"span\": span,\n",
    "                    \"confidence\": 0.55,\n",
    "                    \"model\": self.model_name,\n",
    "                    \"prompt_version\": self.PROMPT_VERSION\n",
    "                })\n",
    "\n",
    "        return {\"entities\": entities, \"statements\": statements, \"evidences\": evidences}\n",
    "\n",
    "    # ---- Cache key ----\n",
    "    def _cache_key(self, kind: str, payload: Dict[str, Any]) -> str:\n",
    "        base = json.dumps({\"k\": kind, \"p\": payload}, sort_keys=True, ensure_ascii=False)\n",
    "        return hashlib.sha256(base.encode()).hexdigest()\n",
    "\n",
    "    # ---- Public APIs with verbose printing + token accounting ----\n",
    "    def extract_table(self, *, table_markdown: str, doc_ctx: Dict[str, Any], meta: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        if not table_markdown.strip():\n",
    "            return {\"entities\": [], \"statements\": [], \"evidences\": []}\n",
    "        vars_payload = {\n",
    "            \"doc_id\": doc_ctx.get(\"doc_id\"),\n",
    "            \"chunk_id\": meta.get(\"chunk_id\"),\n",
    "            \"page\": meta.get(\"page\"),\n",
    "            \"table_id\": meta.get(\"table_id\"),\n",
    "            \"source\": doc_ctx.get(\"source\"),\n",
    "            \"financial_context\": json.dumps(meta.get(\"financial_context\") or {}, ensure_ascii=False),\n",
    "            \"markdown\": table_markdown\n",
    "        }\n",
    "        ck = self._cache_key(\"table\", vars_payload)\n",
    "        if self.enable_cache and ck in self._cache:\n",
    "            if self.verbose:\n",
    "                print(f\"[HIT] cache table doc={doc_ctx.get('doc_id')} chunk={meta.get('chunk_id')}\")\n",
    "            return self._cache[ck]\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"[RUN] table  doc={doc_ctx.get('doc_id')} chunk={meta.get('chunk_id')} page={meta.get('page')} tbl={meta.get('table_id')}\")\n",
    "        self._enforce_rate_limit()\n",
    "        self.stats[\"requests\"] += 1\n",
    "        try:\n",
    "            resp = self.llm_table.invoke(vars_payload)\n",
    "            pm, cm, tt = self._record_usage(resp, vars_payload)\n",
    "            raw = self._safe_json_loads(resp.content)\n",
    "            norm = self._normalize(raw, doc_ctx, meta)\n",
    "            self._tally(norm)\n",
    "\n",
    "            if self.verbose:\n",
    "                print(f\"[OK ] table  doc={doc_ctx.get('doc_id')} chunk={meta.get('chunk_id')} \"\n",
    "                    f\"ents={len(norm['entities'])} stmts={len(norm['statements'])} evs={len(norm['evidences'])} | tokens p={pm} c={cm} t={tt}\")\n",
    "                for s in norm[\"statements\"][:VERBOSE_SAMPLES]:\n",
    "                    print(\"      • stmt\", json.dumps({\"subj\": s[\"subject_eid\"], \"pred\": s[\"predicate\"], \"obj\": s[\"object\"]}, ensure_ascii=False))\n",
    "                for ev in norm[\"evidences\"][:VERBOSE_SAMPLES]:\n",
    "                    span = ev.get(\"span\", {})\n",
    "                    span_str = json.dumps(span, ensure_ascii=False)\n",
    "                    print(\"      • evid\", span_str if len(span_str) < 160 else json.dumps({\"preview\": span_str[:160] + \"...\"}, ensure_ascii=False))\n",
    "\n",
    "            if self.enable_cache:\n",
    "                self._cache[ck] = norm\n",
    "            return norm\n",
    "        except Exception as e:\n",
    "            self.stats[\"failures\"] += 1\n",
    "            if self.verbose:\n",
    "                print(f\"[ERROR] Table extraction error: {e}\\n{traceback.format_exc()}\")\n",
    "            return {\"entities\": [], \"statements\": [], \"evidences\": []}\n",
    "\n",
    "    def extract_text(self, *, text: str, doc_ctx: Dict[str, Any], meta: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        if not text or len(text.split()) < 5:\n",
    "            return {\"entities\": [], \"statements\": [], \"evidences\": []}\n",
    "        vars_payload = {\n",
    "            \"doc_id\": doc_ctx.get(\"doc_id\"),\n",
    "            \"chunk_id\": meta.get(\"chunk_id\"),\n",
    "            \"page\": meta.get(\"page\"),\n",
    "            \"label\": meta.get(\"label\") or \"text\",\n",
    "            \"source\": doc_ctx.get(\"source\"),\n",
    "            \"text\": text\n",
    "        }\n",
    "        ck = self._cache_key(\"text\", vars_payload)\n",
    "        if self.enable_cache and ck in self._cache:\n",
    "            if self.verbose:\n",
    "                print(f\"[HIT] cache text  doc={doc_ctx.get('doc_id')} chunk={meta.get('chunk_id')}\")\n",
    "            return self._cache[ck]\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"[RUN] text   doc={doc_ctx.get('doc_id')} chunk={meta.get('chunk_id')} page={meta.get('page')} words={len(text.split())}\")\n",
    "        self._enforce_rate_limit()\n",
    "        self.stats[\"requests\"] += 1\n",
    "        try:\n",
    "            resp = self.llm_text.invoke(vars_payload)\n",
    "            pm, cm, tt = self._record_usage(resp, vars_payload)\n",
    "            raw = self._safe_json_loads(resp.content)\n",
    "            norm = self._normalize(raw, doc_ctx, meta)\n",
    "            self._tally(norm)\n",
    "\n",
    "            if self.verbose:\n",
    "                print(f\"[OK ] text   doc={doc_ctx.get('doc_id')} chunk={meta.get('chunk_id')} \"\n",
    "                    f\"ents={len(norm['entities'])} stmts={len(norm['statements'])} evs={len(norm['evidences'])} | tokens p={pm} c={cm} t={tt}\")\n",
    "                for s in norm[\"statements\"][:VERBOSE_SAMPLES]:\n",
    "                    print(\"      • stmt\", json.dumps({\"subj\": s[\"subject_eid\"], \"pred\": s[\"predicate\"], \"obj\": s[\"object\"]}, ensure_ascii=False))\n",
    "                for ev in norm[\"evidences\"][:VERBOSE_SAMPLES]:\n",
    "                    span = ev.get(\"span\", {})\n",
    "                    span_str = json.dumps(span, ensure_ascii=False)\n",
    "                    print(\"      • evid\", span_str if len(span_str) < 160 else json.dumps({\"preview\": span_str[:160] + \"...\"}, ensure_ascii=False))\n",
    "\n",
    "            if self.enable_cache:\n",
    "                self._cache[ck] = norm\n",
    "            return norm\n",
    "        except Exception as e:\n",
    "            self.stats[\"failures\"] += 1\n",
    "            if self.verbose:\n",
    "                print(f\"[ERROR] Text extraction error: {e}\\n{traceback.format_exc()}\")\n",
    "            return {\"entities\": [], \"statements\": [], \"evidences\": []}\n",
    "\n",
    "    def _tally(self, norm: Dict[str, List[Dict[str, Any]]]):\n",
    "        self.stats[\"success\"] += 1\n",
    "        self.stats[\"entities\"] += len(norm[\"entities\"])\n",
    "        self.stats[\"statements\"] += len(norm[\"statements\"])\n",
    "        self.stats[\"evidences\"] += len(norm[\"evidences\"])\n",
    "\n",
    "    def get_statistics(self) -> Dict[str, Any]:\n",
    "        total_req = max(1, self.stats.get(\"requests\", 0))\n",
    "        succ = self.stats.get(\"success\", 0)\n",
    "        return {\n",
    "            **self.stats,\n",
    "            \"success_rate\": succ / total_req,\n",
    "            \"avg_entities_per_success\": self.stats.get(\"entities\", 0) / max(1, succ),\n",
    "            \"avg_statements_per_success\": self.stats.get(\"statements\", 0) / max(1, succ),\n",
    "            \"avg_evidences_per_success\": self.stats.get(\"evidences\", 0) / max(1, succ),\n",
    "        }\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Data loading (web + prepared PDF)  [unchanged]\n",
    "# ---------------------------------------------------------------------\n",
    "def _safe_load_json(path: Path) -> Optional[dict]:\n",
    "    try:\n",
    "        with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "            return json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Failed reading {path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_raw_web_docs() -> Any:\n",
    "    if not WEB_DOCS_PATH.exists():\n",
    "        print(f\"[INFO] Web docs file not found: {WEB_DOCS_PATH}\")\n",
    "        return None\n",
    "    try:\n",
    "        with WEB_DOCS_PATH.open(\"r\", encoding=\"utf-8\") as f:\n",
    "            return json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Failed to load web docs: {e}\")\n",
    "        return None\n",
    "\n",
    "def _normalize_web_docs(raw: Any) -> List[Dict[str, Any]]:\n",
    "    if raw is None:\n",
    "        return []\n",
    "    if isinstance(raw, list):\n",
    "        return [r for r in raw if isinstance(r, dict)]\n",
    "    if isinstance(raw, dict):\n",
    "        if \"documents\" in raw and isinstance(raw[\"documents\"], list):\n",
    "            return [r for r in raw[\"documents\"] if isinstance(r, dict)]\n",
    "        if \"data\" in raw and isinstance(raw[\"data\"], list):\n",
    "            return [r for r in raw[\"data\"] if isinstance(r, dict)]\n",
    "        if all(isinstance(v, dict) for v in raw.values()):\n",
    "            out = []\n",
    "            for k, v in raw.items():\n",
    "                if \"doc_id\" not in v and \"id\" not in v:\n",
    "                    v = {**v, \"doc_id\": k}\n",
    "                out.append(v)\n",
    "            return out\n",
    "    print(\"[WARN] Unrecognized web docs JSON structure; proceeding with empty list.\")\n",
    "    return []\n",
    "\n",
    "def _derive_web_doc_id(meta: Dict[str, Any], index: int) -> str:\n",
    "    for key in (\"doc_id\", \"id\", \"slug\"):\n",
    "        if meta.get(key):\n",
    "            return str(meta[key])\n",
    "    url = meta.get(\"url\")\n",
    "    if url:\n",
    "        return \"web:\" + _sha(url, 12)\n",
    "    snippet = (meta.get(\"text\") or meta.get(\"content\") or \"\")[:80]\n",
    "    return \"web:\" + _sha(f\"{index}|{snippet}\", 12)\n",
    "\n",
    "def iter_web_chunks(max_chars: int = 10000) -> Iterator[Dict[str, Any]]:\n",
    "    raw = load_raw_web_docs()\n",
    "    records = _normalize_web_docs(raw)\n",
    "    if not records:\n",
    "        return\n",
    "    for idx, meta in enumerate(records):\n",
    "        if not isinstance(meta, dict):\n",
    "            continue\n",
    "        doc_id = _derive_web_doc_id(meta, idx)\n",
    "        text = meta.get(\"text\") or meta.get(\"content\") or \"\"\n",
    "        if not isinstance(text, str):\n",
    "            if isinstance(text, list):\n",
    "                text = \" \".join(str(t) for t in text)\n",
    "            else:\n",
    "                text = str(text)\n",
    "        if not text.strip():\n",
    "            continue\n",
    "        url = meta.get(\"url\")\n",
    "        start = 0\n",
    "        chunk_idx = 0\n",
    "        while start < len(text):\n",
    "            piece = text[start:start+max_chars]\n",
    "            yield {\n",
    "                \"source_type\": \"web\",\n",
    "                \"doc_id\": doc_id,\n",
    "                \"chunk_id\": f\"{doc_id}::chunk_{chunk_idx}\",\n",
    "                \"page\": None,\n",
    "                \"table_id\": None,\n",
    "                \"content_type\": \"text\",\n",
    "                \"text\": piece,\n",
    "                \"markdown_table\": None,\n",
    "                \"url\": url,\n",
    "                \"meta\": {k: v for k, v in meta.items() if k not in (\"text\", \"content\")},\n",
    "                \"year\": meta.get(\"year\"),\n",
    "                \"quarter\": meta.get(\"quarter\"),\n",
    "            }\n",
    "            start += max_chars\n",
    "            chunk_idx += 1\n",
    "\n",
    "def iter_prepared_pdf_files(years: Optional[List[str]] = None) -> Iterator[Path]:\n",
    "    if not PDF_PREPARED_ROOT.exists():\n",
    "        return\n",
    "    for p in PDF_PREPARED_ROOT.rglob(\"*.prepared.json\"):\n",
    "        year = next((seg for seg in p.parts if seg.isdigit() and len(seg) == 4), None)\n",
    "        if years and year and year not in years:\n",
    "            continue\n",
    "        yield p\n",
    "\n",
    "def _extract_pdf_tables(d: dict) -> List[dict]:\n",
    "    tables = d.get(\"reconstructed_tables\") or d.get(\"tables\") or []\n",
    "    out = []\n",
    "    for t in tables:\n",
    "        md = t.get(\"markdown\") or t.get(\"markdown_table\") or t.get(\"md\")\n",
    "        if not md:\n",
    "            continue\n",
    "        out.append({\n",
    "            \"table_id\": t.get(\"id\") or t.get(\"table_id\"),\n",
    "            \"page\": t.get(\"page\") or t.get(\"page_no\") or t.get(\"page_number\"),\n",
    "            \"markdown\": md,\n",
    "            \"title\": t.get(\"title\"),\n",
    "            \"financial_context\": t.get(\"financial_context\"),\n",
    "            \"meta\": {k: v for k, v in t.items() if k not in (\"markdown\",\"markdown_table\",\"md\")}\n",
    "        })\n",
    "    return out\n",
    "\n",
    "def _extract_pdf_text(d: dict) -> List[dict]:\n",
    "    chunks = d.get(\"text_chunks\") or d.get(\"chunks\") or []\n",
    "    out = []\n",
    "    for c in chunks:\n",
    "        txt = c.get(\"text\") or c.get(\"content\") or \"\"\n",
    "        if not txt.strip():\n",
    "            continue\n",
    "        out.append({\n",
    "            \"chunk_id\": c.get(\"id\") or c.get(\"chunk_id\"),\n",
    "            \"page\": c.get(\"page\") or c.get(\"page_no\") or c.get(\"page_number\"),\n",
    "            \"text\": txt,\n",
    "            \"meta\": {k: v for k, v in c.items() if k not in (\"text\",\"content\")}\n",
    "        })\n",
    "    return out\n",
    "\n",
    "def iter_pdf_records(years: Optional[List[str]] = None,\n",
    "                     include_tables: bool = True,\n",
    "                     include_text: bool = True) -> Iterator[Dict[str, Any]]:\n",
    "    for path in iter_prepared_pdf_files(years=years):\n",
    "        data = _safe_load_json(path)\n",
    "        if not data:\n",
    "            continue\n",
    "        meta = data.get(\"document_meta\") or data.get(\"meta\") or {}\n",
    "        doc_id = data.get(\"doc_id\") or f\"pdfdoc:{_sha(str(path), 12)}\"\n",
    "        year = next((seg for seg in path.parts if seg.isdigit() and len(seg) == 4), None)\n",
    "        quarter = None\n",
    "        for seg in path.parts:\n",
    "            if len(seg) == 2 and seg[0].isdigit() and seg[1].upper() == \"Q\":\n",
    "                quarter = seg.upper()\n",
    "        if include_tables:\n",
    "            for t in _extract_pdf_tables(data):\n",
    "                yield {\n",
    "                    \"source_type\": \"pdf\",\n",
    "                    \"doc_id\": doc_id,\n",
    "                    \"chunk_id\": f\"{doc_id}::table::{t['table_id']}\",\n",
    "                    \"page\": t[\"page\"],\n",
    "                    \"table_id\": t[\"table_id\"],\n",
    "                    \"content_type\": \"table\",\n",
    "                    \"text\": None,\n",
    "                    \"markdown_table\": t[\"markdown\"],\n",
    "                    \"url\": meta.get(\"source_url\"),\n",
    "                    \"meta\": {**meta, **t.get(\"meta\", {}), \"title\": t.get(\"title\")},\n",
    "                    \"financial_context\": t.get(\"financial_context\"),\n",
    "                    \"source_path\": str(path),\n",
    "                    \"year\": year,\n",
    "                    \"quarter\": quarter\n",
    "                }\n",
    "        if include_text:\n",
    "            for c in _extract_pdf_text(data):\n",
    "                cid = c[\"chunk_id\"] or f\"{doc_id}::chunk::{_sha(str(path)+str(c['page'])+c['text'][:32], 10)}\"\n",
    "                yield {\n",
    "                    \"source_type\": \"pdf\",\n",
    "                    \"doc_id\": doc_id,\n",
    "                    \"chunk_id\": cid,\n",
    "                    \"page\": c[\"page\"],\n",
    "                    \"table_id\": None,\n",
    "                    \"content_type\": \"text\",\n",
    "                    \"text\": c[\"text\"],\n",
    "                    \"markdown_table\": None,\n",
    "                    \"url\": meta.get(\"source_url\"),\n",
    "                    \"meta\": {**meta, **c.get(\"meta\", {})},\n",
    "                    \"source_path\": str(path),\n",
    "                    \"year\": year,\n",
    "                    \"quarter\": quarter\n",
    "                }\n",
    "\n",
    "def iter_all_records(years: Optional[List[str]] = None,\n",
    "                     include_web: bool = True,\n",
    "                     include_pdf_tables: bool = True,\n",
    "                     include_pdf_text: bool = True) -> Iterator[Dict[str, Any]]:\n",
    "    if include_web:\n",
    "        yield from iter_web_chunks()\n",
    "    yield from iter_pdf_records(years=years,\n",
    "                                include_tables=include_pdf_tables,\n",
    "                                include_text=include_pdf_text)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Aggregation / Merge\n",
    "# ---------------------------------------------------------------------\n",
    "@dataclass\n",
    "class AggregateStore:\n",
    "    entities: Dict[str, Dict[str, Any]] = field(default_factory=dict)\n",
    "    statements: Dict[str, Dict[str, Any]] = field(default_factory=dict)   # sid -> statement\n",
    "    evidences: Dict[str, Dict[str, Any]] = field(default_factory=dict)    # evid -> evidence\n",
    "    stmt_key_index: Dict[str, str] = field(default_factory=dict)          # canonical key -> sid\n",
    "\n",
    "    def _statement_canonical_key(self, s: Dict[str, Any]) -> str:\n",
    "        obj = json.dumps(s[\"object\"], sort_keys=True, ensure_ascii=False)\n",
    "        return f\"{s['subject_eid']}|{s['predicate']}|{obj}\"\n",
    "\n",
    "    def merge_batch(self, batch: Dict[str, List[Dict[str, Any]]]):\n",
    "        # entities\n",
    "        for e in batch[\"entities\"]:\n",
    "            self.entities[e[\"eid\"]] = e\n",
    "        # statements with dedupe\n",
    "        for s in batch[\"statements\"]:\n",
    "            skey = self._statement_canonical_key(s)\n",
    "            existing_sid = self.stmt_key_index.get(skey)\n",
    "            if existing_sid:\n",
    "                existing = self.statements[existing_sid]\n",
    "                existing[\"confidence\"] = max(existing.get(\"confidence\", 0), s.get(\"confidence\", 0))\n",
    "            else:\n",
    "                self.statements[s[\"sid\"]] = s\n",
    "                self.stmt_key_index[skey] = s[\"sid\"]\n",
    "        # evidences\n",
    "        for ev in batch[\"evidences\"]:\n",
    "            self.evidences[ev[\"evid\"]] = ev\n",
    "\n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        sid_to_evids = defaultdict(list)\n",
    "        for ev in self.evidences.values():\n",
    "            sid_to_evids[ev[\"sid\"]].append(ev[\"evid\"])\n",
    "        out_statements = []\n",
    "        for s in self.statements.values():\n",
    "            s_out = dict(s)\n",
    "            s_out[\"evidence_ids\"] = sid_to_evids.get(s[\"sid\"], [])\n",
    "            out_statements.append(s_out)\n",
    "        return {\n",
    "            \"entities\": list(self.entities.values()),\n",
    "            \"statements\": out_statements,\n",
    "            \"evidences\": list(self.evidences.values())\n",
    "        }\n",
    "\n",
    "    def save(self, path: Path):\n",
    "        tmp = path.with_suffix(\".tmp\")\n",
    "        with tmp.open(\"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(self.to_dict(), f, indent=2)\n",
    "        tmp.replace(path)\n",
    "\n",
    "    def save_jsonl(self, base_dir: Path):\n",
    "        data = self.to_dict()\n",
    "        for name, seq in data.items():\n",
    "            p = base_dir / f\"{name}.jsonl\"\n",
    "            tmp = p.with_suffix(\".tmp\")\n",
    "            with tmp.open(\"w\", encoding=\"utf-8\") as f:\n",
    "                for obj in seq:\n",
    "                    f.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "            tmp.replace(p)\n",
    "\n",
    "def load_existing_store(path: Path) -> AggregateStore:\n",
    "    if not path.exists():\n",
    "        return AggregateStore()\n",
    "    try:\n",
    "        data = json.loads(path.read_text(encoding=\"utf-8\"))\n",
    "        store = AggregateStore()\n",
    "        for e in data.get(\"entities\", []):\n",
    "            store.entities[e[\"eid\"]] = e\n",
    "        for s in data.get(\"statements\", []):\n",
    "            store.statements[s[\"sid\"]] = s\n",
    "            store.stmt_key_index[store._statement_canonical_key(s)] = s[\"sid\"]\n",
    "        for ev in data.get(\"evidences\", []):\n",
    "            store.evidences[ev[\"evid\"]] = ev\n",
    "        return store\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Failed loading existing aggregate: {e}\")\n",
    "        return AggregateStore()\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Orchestrator\n",
    "# ---------------------------------------------------------------------\n",
    "def run_extraction(years: Optional[List[str]] = None,\n",
    "                   include_web: bool = True,\n",
    "                   include_pdf_tables: bool = True,\n",
    "                   include_pdf_text: bool = True,\n",
    "                   max_records: Optional[int] = None,\n",
    "                   output_name: str = \"knowledge_extraction_aggregate.json\",\n",
    "                   model_name: str = GENAI_MODEL) -> Dict[str, Any]:\n",
    "    aggregate_path = OUTPUT_DIR / output_name\n",
    "    store = load_existing_store(aggregate_path) if RESUME else AggregateStore()\n",
    "    \n",
    "    # New addition\n",
    "    # ===============================================================\n",
    "    pre_counts = {\n",
    "        \"entities\": len(store.entities),\n",
    "        \"statements\": len(store.statements),\n",
    "        \"evidences\": len(store.evidences),\n",
    "    }\n",
    "    print(\"[INIT] store entities=%d statements=%d evidences=%d\" %\n",
    "        (pre_counts[\"entities\"], pre_counts[\"statements\"], pre_counts[\"evidences\"]))\n",
    "    # ==============================================================\n",
    "\n",
    "    extractor = GraphExtractionAgent(model_name=model_name)\n",
    "\n",
    "    # Optional token usage measurement via callback context (kept; may or may not populate)\n",
    "    if ENABLE_TOKEN_USAGE:\n",
    "        try:\n",
    "            from langchain.callbacks.token_usage import TokenUsageCallbackHandler\n",
    "            token_handler = TokenUsageCallbackHandler()\n",
    "            token_handler.__enter__()  # start tracking\n",
    "            print(\"[INFO] TokenUsageCallbackHandler active\")\n",
    "        except Exception:\n",
    "            token_handler = None\n",
    "    else:\n",
    "        token_handler = None\n",
    "\n",
    "    failures = 0\n",
    "    processed = 0\n",
    "    start_time = time.time()\n",
    "    last_checkpoint = start_time\n",
    "\n",
    "    for rec in iter_all_records(years=years,\n",
    "                                include_web=include_web,\n",
    "                                include_pdf_tables=include_pdf_tables,\n",
    "                                include_pdf_text=include_pdf_text):\n",
    "        if max_records and processed >= max_records:\n",
    "            break\n",
    "\n",
    "        doc_ctx = {\"doc_id\": rec[\"doc_id\"], \"source\": rec.get(\"url\")}\n",
    "        meta = {\n",
    "            \"chunk_id\": rec[\"chunk_id\"],\n",
    "            \"page\": rec.get(\"page\"),\n",
    "            \"table_id\": rec.get(\"table_id\"),\n",
    "            \"label\": \"table\" if rec[\"content_type\"] == \"table\" else \"text\",\n",
    "            \"financial_context\": rec.get(\"financial_context\")\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            if rec[\"content_type\"] == \"table\" and rec.get(\"markdown_table\"):\n",
    "                batch = extractor.extract_table(table_markdown=rec[\"markdown_table\"], doc_ctx=doc_ctx, meta=meta)\n",
    "            else:\n",
    "                batch = extractor.extract_text(text=rec.get(\"text\") or \"\", doc_ctx=doc_ctx, meta=meta)\n",
    "            store.merge_batch(batch)\n",
    "        except Exception as e:\n",
    "            failures += 1\n",
    "            print(f\"[ERROR] Extraction failure doc_id={rec['doc_id']} type={rec['content_type']}: {e}\")\n",
    "            if failures >= MAX_FAILURES:\n",
    "                print(\"[ABORT] Failure threshold reached.\")\n",
    "                break\n",
    "\n",
    "        processed += 1\n",
    "        now = time.time()\n",
    "        if (processed % CHECKPOINT_EVERY_RECORDS == 0) or (now - last_checkpoint >= CHECKPOINT_EVERY_SECONDS):\n",
    "            store.save(aggregate_path)\n",
    "            if SAVE_JSONL:\n",
    "                store.save_jsonl(OUTPUT_DIR)\n",
    "            tokens_total = extractor.stats.get(\"total_tokens\", 0)\n",
    "            print(f\"[CHECKPOINT] processed={processed} ents={len(store.entities)} stmts={len(store.statements)} \"\n",
    "                f\"evs={len(store.evidences)} elapsed={int(now-start_time)}s tokens_total={tokens_total}\")\n",
    "            last_checkpoint = now\n",
    "\n",
    "    # Final save\n",
    "    store.save(aggregate_path)\n",
    "    if SAVE_JSONL:\n",
    "        store.save_jsonl(OUTPUT_DIR)\n",
    "\n",
    "    stats_agent = extractor.get_statistics()\n",
    "    summary = {\n",
    "        \"processed_records\": processed,\n",
    "        \"entities\": len(store.entities),\n",
    "        \"statements\": len(store.statements),\n",
    "        \"evidences\": len(store.evidences),\n",
    "        \"failures\": failures,\n",
    "        \"elapsed_seconds\": int(time.time() - start_time),\n",
    "        \"agent_stats\": stats_agent\n",
    "    }\n",
    "\n",
    "    # Close token tracking\n",
    "    if token_handler:\n",
    "        try:\n",
    "            token_handler.__exit__(None, None, None)\n",
    "            summary[\"token_usage_callback\"] = getattr(token_handler, \"total_usage\", {})\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # >>> Always include our in-agent totals\n",
    "    summary[\"token_usage_agent\"] = {\n",
    "        \"prompt_tokens\": stats_agent.get(\"prompt_tokens\", 0),\n",
    "        \"completion_tokens\": stats_agent.get(\"completion_tokens\", 0),\n",
    "        \"total_tokens\": stats_agent.get(\"total_tokens\", 0),\n",
    "    }\n",
    "\n",
    "    with (OUTPUT_DIR / \"run_summary.json\").open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    print(\"[DONE]\", json.dumps(summary, indent=2))\n",
    "    \n",
    "    # Added Code\n",
    "    # ================================================================================\n",
    "    post_counts = {\n",
    "        \"entities\": len(store.entities),\n",
    "        \"statements\": len(store.statements),\n",
    "        \"evidences\": len(store.evidences),\n",
    "    }\n",
    "    summary[\"delta\"] = {\n",
    "        \"entities_added\": post_counts[\"entities\"] - pre_counts[\"entities\"],\n",
    "        \"statements_added\": post_counts[\"statements\"] - pre_counts[\"statements\"],\n",
    "        \"evidences_added\": post_counts[\"evidences\"] - pre_counts[\"evidences\"],\n",
    "    }\n",
    "\n",
    "    # ================================================================================\n",
    "    \n",
    "    \n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60a4cc16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CHECKPOINT] processed=18 ents=38 stmts=28 evs=55 elapsed=307s tokens_total=28769\n",
      "[CHECKPOINT] processed=25 ents=38 stmts=28 evs=75 elapsed=433s tokens_total=38577\n",
      "[CHECKPOINT] processed=42 ents=38 stmts=28 evs=75 elapsed=746s tokens_total=71225\n",
      "[CHECKPOINT] processed=50 ents=41 stmts=32 evs=79 elapsed=895s tokens_total=82337\n",
      "[CHECKPOINT] processed=67 ents=52 stmts=48 evs=121 elapsed=1201s tokens_total=103240\n",
      "[CHECKPOINT] processed=75 ents=67 stmts=69 evs=194 elapsed=1344s tokens_total=113423\n",
      "[CHECKPOINT] processed=92 ents=83 stmts=89 evs=246 elapsed=1791s tokens_total=135120\n",
      "[CHECKPOINT] processed=100 ents=88 stmts=104 evs=300 elapsed=1918s tokens_total=144800\n",
      "[CHECKPOINT] processed=117 ents=114 stmts=145 evs=427 elapsed=2228s tokens_total=166308\n",
      "[CHECKPOINT] processed=125 ents=116 stmts=148 evs=436 elapsed=2370s tokens_total=177399\n",
      "[CHECKPOINT] processed=135 ents=134 stmts=165 evs=502 elapsed=2761s tokens_total=188372\n",
      "[CHECKPOINT] processed=150 ents=135 stmts=170 evs=510 elapsed=3009s tokens_total=212763\n",
      "[CHECKPOINT] processed=167 ents=143 stmts=184 evs=562 elapsed=3314s tokens_total=241446\n",
      "[CHECKPOINT] processed=175 ents=143 stmts=184 evs=562 elapsed=3467s tokens_total=256415\n",
      "[CHECKPOINT] processed=192 ents=152 stmts=195 evs=591 elapsed=3771s tokens_total=283116\n",
      "[CHECKPOINT] processed=200 ents=158 stmts=200 evs=626 elapsed=3917s tokens_total=296521\n",
      "[CHECKPOINT] processed=215 ents=161 stmts=218 evs=708 elapsed=4221s tokens_total=318711\n",
      "[CHECKPOINT] processed=225 ents=171 stmts=241 evs=796 elapsed=4397s tokens_total=333851\n",
      "[CHECKPOINT] processed=242 ents=177 stmts=253 evs=847 elapsed=4705s tokens_total=361218\n",
      "[CHECKPOINT] processed=250 ents=180 stmts=257 evs=858 elapsed=4846s tokens_total=372562\n",
      "[CHECKPOINT] processed=267 ents=180 stmts=257 evs=858 elapsed=5146s tokens_total=400179\n",
      "[CHECKPOINT] processed=275 ents=187 stmts=268 evs=899 elapsed=5296s tokens_total=411474\n",
      "[CHECKPOINT] processed=290 ents=192 stmts=277 evs=915 elapsed=5600s tokens_total=436587\n",
      "[CHECKPOINT] processed=300 ents=194 stmts=282 evs=931 elapsed=5771s tokens_total=451442\n",
      "[CHECKPOINT] processed=317 ents=201 stmts=288 evs=961 elapsed=6080s tokens_total=482205\n",
      "[CHECKPOINT] processed=325 ents=204 stmts=293 evs=972 elapsed=6222s tokens_total=493436\n",
      "[CHECKPOINT] processed=341 ents=204 stmts=293 evs=972 elapsed=6549s tokens_total=523020\n",
      "[CHECKPOINT] processed=350 ents=224 stmts=320 evs=1063 elapsed=6697s tokens_total=534368\n",
      "[CHECKPOINT] processed=367 ents=232 stmts=326 evs=1087 elapsed=7006s tokens_total=563862\n",
      "[CHECKPOINT] processed=375 ents=232 stmts=326 evs=1087 elapsed=7151s tokens_total=579167\n",
      "[CHECKPOINT] processed=392 ents=256 stmts=352 evs=1198 elapsed=7455s tokens_total=602775\n",
      "[CHECKPOINT] processed=400 ents=256 stmts=355 evs=1222 elapsed=7600s tokens_total=616281\n",
      "[CHECKPOINT] processed=418 ents=260 stmts=360 evs=1254 elapsed=7917s tokens_total=645291\n",
      "[CHECKPOINT] processed=425 ents=260 stmts=360 evs=1254 elapsed=8044s tokens_total=658789\n",
      "[CHECKPOINT] processed=442 ents=260 stmts=360 evs=1254 elapsed=8360s tokens_total=691449\n",
      "[CHECKPOINT] processed=450 ents=260 stmts=360 evs=1254 elapsed=8582s tokens_total=706726\n",
      "[CHECKPOINT] processed=464 ents=265 stmts=362 evs=1258 elapsed=8890s tokens_total=730594\n",
      "[CHECKPOINT] processed=475 ents=268 stmts=364 evs=1260 elapsed=9195s tokens_total=750282\n",
      "[CHECKPOINT] processed=492 ents=275 stmts=373 evs=1281 elapsed=9502s tokens_total=778509\n",
      "[CHECKPOINT] processed=500 ents=279 stmts=383 evs=1339 elapsed=9626s tokens_total=788280\n",
      "[CHECKPOINT] processed=516 ents=279 stmts=383 evs=1339 elapsed=9932s tokens_total=817350\n",
      "[CHECKPOINT] processed=525 ents=283 stmts=392 evs=1377 elapsed=10146s tokens_total=832339\n",
      "[CHECKPOINT] processed=542 ents=285 stmts=401 evs=1439 elapsed=10458s tokens_total=859943\n",
      "[CHECKPOINT] processed=550 ents=291 stmts=411 evs=1475 elapsed=10605s tokens_total=869987\n",
      "[CHECKPOINT] processed=568 ents=301 stmts=431 evs=1524 elapsed=10920s tokens_total=897982\n",
      "[CHECKPOINT] processed=575 ents=305 stmts=442 evs=1578 elapsed=11045s tokens_total=909781\n",
      "[CHECKPOINT] processed=590 ents=310 stmts=446 evs=1586 elapsed=11345s tokens_total=936246\n",
      "[CHECKPOINT] processed=600 ents=310 stmts=451 evs=1596 elapsed=11525s tokens_total=952421\n",
      "[CHECKPOINT] processed=621 ents=347 stmts=495 evs=1640 elapsed=11852s tokens_total=957926\n",
      "[CHECKPOINT] processed=625 ents=355 stmts=503 evs=1648 elapsed=11892s tokens_total=958910\n",
      "[CHECKPOINT] processed=645 ents=399 stmts=556 evs=1792 elapsed=12199s tokens_total=966988\n",
      "[CHECKPOINT] processed=650 ents=416 stmts=571 evs=1840 elapsed=12288s tokens_total=970371\n",
      "[CHECKPOINT] processed=669 ents=469 stmts=645 evs=2111 elapsed=12594s tokens_total=981595\n",
      "[CHECKPOINT] processed=675 ents=477 stmts=658 evs=2147 elapsed=12667s tokens_total=983784\n",
      "[CHECKPOINT] processed=694 ents=536 stmts=729 evs=2337 elapsed=12980s tokens_total=994597\n",
      "[CHECKPOINT] processed=700 ents=540 stmts=740 evs=2365 elapsed=13095s tokens_total=999983\n",
      "[CHECKPOINT] processed=721 ents=572 stmts=784 evs=2478 elapsed=13407s tokens_total=1008849\n",
      "[CHECKPOINT] processed=725 ents=577 stmts=795 evs=2519 elapsed=13482s tokens_total=1010658\n",
      "[CHECKPOINT] processed=746 ents=607 stmts=859 evs=2713 elapsed=13791s tokens_total=1020557\n",
      "[CHECKPOINT] processed=750 ents=616 stmts=874 evs=2729 elapsed=13927s tokens_total=1022257\n",
      "[CHECKPOINT] processed=771 ents=638 stmts=919 evs=2818 elapsed=14236s tokens_total=1031632\n",
      "[CHECKPOINT] processed=775 ents=644 stmts=928 evs=2847 elapsed=14313s tokens_total=1033320\n",
      "[CHECKPOINT] processed=797 ents=664 stmts=975 evs=3012 elapsed=14615s tokens_total=1042145\n",
      "[CHECKPOINT] processed=800 ents=669 stmts=985 evs=3040 elapsed=14652s tokens_total=1043256\n",
      "[CHECKPOINT] processed=821 ents=695 stmts=1036 evs=3164 elapsed=14955s tokens_total=1052173\n",
      "[CHECKPOINT] processed=825 ents=695 stmts=1037 evs=3180 elapsed=14991s tokens_total=1053178\n",
      "[CHECKPOINT] processed=844 ents=713 stmts=1072 evs=3295 elapsed=15322s tokens_total=1061529\n",
      "[CHECKPOINT] processed=850 ents=719 stmts=1090 evs=3365 elapsed=15397s tokens_total=1064009\n",
      "[CHECKPOINT] processed=873 ents=734 stmts=1122 evs=3484 elapsed=15700s tokens_total=1073463\n",
      "[CHECKPOINT] processed=875 ents=735 stmts=1123 evs=3485 elapsed=15718s tokens_total=1073664\n",
      "[CHECKPOINT] processed=897 ents=763 stmts=1170 evs=3612 elapsed=16033s tokens_total=1081685\n",
      "[CHECKPOINT] processed=900 ents=764 stmts=1172 evs=3614 elapsed=16066s tokens_total=1082066\n",
      "[CHECKPOINT] processed=925 ents=782 stmts=1200 evs=3686 elapsed=16358s tokens_total=1088554\n",
      "[CHECKPOINT] processed=950 ents=796 stmts=1220 evs=3744 elapsed=16559s tokens_total=1093616\n",
      "[CHECKPOINT] processed=975 ents=796 stmts=1220 evs=3744 elapsed=16560s tokens_total=1093616\n",
      "[CHECKPOINT] processed=1000 ents=838 stmts=1262 evs=3810 elapsed=16826s tokens_total=1099731\n",
      "[CHECKPOINT] processed=1025 ents=848 stmts=1285 evs=3853 elapsed=16950s tokens_total=1103282\n",
      "[CHECKPOINT] processed=1050 ents=865 stmts=1313 evs=3882 elapsed=17167s tokens_total=1107134\n",
      "[CHECKPOINT] processed=1075 ents=870 stmts=1324 evs=3899 elapsed=17225s tokens_total=1108381\n",
      "[CHECKPOINT] processed=1100 ents=882 stmts=1338 evs=3913 elapsed=17317s tokens_total=1110131\n",
      "[CHECKPOINT] processed=1125 ents=882 stmts=1338 evs=3913 elapsed=17318s tokens_total=1110131\n",
      "[CHECKPOINT] processed=1150 ents=907 stmts=1363 evs=3946 elapsed=17490s tokens_total=1113581\n",
      "[CHECKPOINT] processed=1175 ents=914 stmts=1367 evs=3950 elapsed=17510s tokens_total=1114276\n",
      "[CHECKPOINT] processed=1200 ents=926 stmts=1384 evs=3976 elapsed=17655s tokens_total=1119901\n",
      "[CHECKPOINT] processed=1225 ents=942 stmts=1413 evs=4011 elapsed=17882s tokens_total=1125473\n",
      "[CHECKPOINT] processed=1250 ents=952 stmts=1416 evs=4014 elapsed=17954s tokens_total=1126270\n",
      "[CHECKPOINT] processed=1275 ents=956 stmts=1419 evs=4017 elapsed=18006s tokens_total=1126884\n",
      "[CHECKPOINT] processed=1300 ents=963 stmts=1426 evs=4024 elapsed=18061s tokens_total=1127728\n",
      "[CHECKPOINT] processed=1325 ents=964 stmts=1427 evs=4027 elapsed=18098s tokens_total=1128186\n",
      "[CHECKPOINT] processed=1350 ents=972 stmts=1442 evs=4043 elapsed=18244s tokens_total=1130189\n",
      "[CHECKPOINT] processed=1375 ents=975 stmts=1448 evs=4049 elapsed=18332s tokens_total=1131993\n",
      "[CHECKPOINT] processed=1392 ents=986 stmts=1467 evs=4104 elapsed=18643s tokens_total=1139881\n",
      "[CHECKPOINT] processed=1400 ents=986 stmts=1467 evs=4104 elapsed=18788s tokens_total=1150619\n",
      "[CHECKPOINT] processed=1425 ents=1007 stmts=1481 evs=4118 elapsed=18984s tokens_total=1153327\n",
      "[CHECKPOINT] processed=1450 ents=1025 stmts=1499 evs=4148 elapsed=19271s tokens_total=1158165\n",
      "[CHECKPOINT] processed=1467 ents=1029 stmts=1505 evs=4158 elapsed=19583s tokens_total=1180709\n",
      "[CHECKPOINT] processed=1475 ents=1030 stmts=1506 evs=4159 elapsed=19676s tokens_total=1185782\n",
      "[CHECKPOINT] processed=1500 ents=1033 stmts=1512 evs=4167 elapsed=19812s tokens_total=1187560\n",
      "[CHECKPOINT] processed=1525 ents=1036 stmts=1520 evs=4193 elapsed=20041s tokens_total=1191803\n",
      "[CHECKPOINT] processed=1549 ents=1055 stmts=1551 evs=4238 elapsed=20346s tokens_total=1201380\n",
      "[CHECKPOINT] processed=1550 ents=1056 stmts=1554 evs=4241 elapsed=20364s tokens_total=1201786\n",
      "[CHECKPOINT] processed=1570 ents=1079 stmts=1600 evs=4326 elapsed=20670s tokens_total=1211441\n",
      "[CHECKPOINT] processed=1575 ents=1095 stmts=1614 evs=4356 elapsed=20768s tokens_total=1214582\n",
      "[CHECKPOINT] processed=1600 ents=1103 stmts=1634 evs=4383 elapsed=20978s tokens_total=1218389\n",
      "[CHECKPOINT] processed=1625 ents=1103 stmts=1637 evs=4390 elapsed=21248s tokens_total=1232312\n",
      "[CHECKPOINT] processed=1645 ents=1103 stmts=1653 evs=4453 elapsed=21552s tokens_total=1243121\n",
      "[CHECKPOINT] processed=1650 ents=1103 stmts=1660 evs=4500 elapsed=21642s tokens_total=1245677\n",
      "[CHECKPOINT] processed=1675 ents=1109 stmts=1675 evs=4561 elapsed=21920s tokens_total=1253023\n",
      "[CHECKPOINT] processed=1700 ents=1111 stmts=1679 evs=4573 elapsed=22098s tokens_total=1255251\n",
      "[CHECKPOINT] processed=1722 ents=1120 stmts=1707 evs=4617 elapsed=22404s tokens_total=1259960\n",
      "[CHECKPOINT] processed=1725 ents=1121 stmts=1708 evs=4619 elapsed=22441s tokens_total=1260444\n",
      "[CHECKPOINT] processed=1745 ents=1153 stmts=1754 evs=4739 elapsed=22746s tokens_total=1269498\n",
      "[CHECKPOINT] processed=1750 ents=1169 stmts=1774 evs=4804 elapsed=22815s tokens_total=1272269\n",
      "[CHECKPOINT] processed=1770 ents=1185 stmts=1820 evs=4958 elapsed=23119s tokens_total=1283304\n",
      "[CHECKPOINT] processed=1775 ents=1200 stmts=1847 evs=5026 elapsed=23212s tokens_total=1286714\n",
      "[CHECKPOINT] processed=1794 ents=1237 stmts=1908 evs=5184 elapsed=23521s tokens_total=1298289\n",
      "[CHECKPOINT] processed=1800 ents=1244 stmts=1927 evs=5244 elapsed=23627s tokens_total=1302025\n",
      "[CHECKPOINT] processed=1824 ents=1262 stmts=1981 evs=5431 elapsed=23935s tokens_total=1310399\n",
      "[CHECKPOINT] processed=1825 ents=1262 stmts=1981 evs=5431 elapsed=23936s tokens_total=1310399\n",
      "[CHECKPOINT] processed=1849 ents=1284 stmts=2019 evs=5533 elapsed=24247s tokens_total=1317910\n",
      "[CHECKPOINT] processed=1850 ents=1285 stmts=2023 evs=5545 elapsed=24263s tokens_total=1318484\n",
      "[CHECKPOINT] processed=1872 ents=1317 stmts=2075 evs=5704 elapsed=24573s tokens_total=1326798\n",
      "[CHECKPOINT] processed=1875 ents=1324 stmts=2091 evs=5800 elapsed=24626s tokens_total=1328971\n",
      "[CHECKPOINT] processed=1896 ents=1345 stmts=2133 evs=5907 elapsed=24931s tokens_total=1337431\n",
      "[CHECKPOINT] processed=1900 ents=1347 stmts=2141 evs=5949 elapsed=24968s tokens_total=1338716\n",
      "[CHECKPOINT] processed=1924 ents=1357 stmts=2173 evs=6055 elapsed=25273s tokens_total=1346745\n",
      "[CHECKPOINT] processed=1925 ents=1357 stmts=2173 evs=6055 elapsed=25274s tokens_total=1346745\n",
      "[CHECKPOINT] processed=1950 ents=1374 stmts=2202 evs=6133 elapsed=25589s tokens_total=1354325\n",
      "[CHECKPOINT] processed=1973 ents=1397 stmts=2247 evs=6248 elapsed=25894s tokens_total=1362081\n",
      "[CHECKPOINT] processed=1975 ents=1400 stmts=2252 evs=6275 elapsed=25931s tokens_total=1363121\n",
      "[CHECKPOINT] processed=1999 ents=1424 stmts=2292 evs=6384 elapsed=26237s tokens_total=1371547\n",
      "[CHECKPOINT] processed=2000 ents=1424 stmts=2292 evs=6384 elapsed=26257s tokens_total=1372498\n",
      "[CHECKPOINT] processed=2025 ents=1434 stmts=2321 evs=6479 elapsed=26534s tokens_total=1378942\n",
      "[CHECKPOINT] processed=2050 ents=1438 stmts=2338 evs=6524 elapsed=26826s tokens_total=1385999\n",
      "[CHECKPOINT] processed=2075 ents=1440 stmts=2352 evs=6589 elapsed=27077s tokens_total=1393367\n",
      "[CHECKPOINT] processed=2100 ents=1440 stmts=2352 evs=6589 elapsed=27078s tokens_total=1393367\n",
      "[CHECKPOINT] processed=2125 ents=1440 stmts=2352 evs=6589 elapsed=27257s tokens_total=1404907\n",
      "[CHECKPOINT] processed=2150 ents=1440 stmts=2352 evs=6589 elapsed=27349s tokens_total=1411419\n",
      "[CHECKPOINT] processed=2175 ents=1440 stmts=2352 evs=6589 elapsed=27385s tokens_total=1413345\n",
      "[CHECKPOINT] processed=2200 ents=1440 stmts=2352 evs=6589 elapsed=27524s tokens_total=1420571\n",
      "[CHECKPOINT] processed=2225 ents=1440 stmts=2352 evs=6589 elapsed=27524s tokens_total=1420571\n",
      "[CHECKPOINT] processed=2250 ents=1440 stmts=2352 evs=6589 elapsed=27610s tokens_total=1423290\n",
      "[CHECKPOINT] processed=2275 ents=1440 stmts=2352 evs=6589 elapsed=27737s tokens_total=1430473\n",
      "[CHECKPOINT] processed=2300 ents=1440 stmts=2352 evs=6589 elapsed=27738s tokens_total=1430473\n",
      "[CHECKPOINT] processed=2325 ents=1440 stmts=2352 evs=6589 elapsed=27865s tokens_total=1454503\n",
      "[CHECKPOINT] processed=2350 ents=1443 stmts=2358 evs=6595 elapsed=27934s tokens_total=1458151\n",
      "[CHECKPOINT] processed=2375 ents=1446 stmts=2361 evs=6598 elapsed=27972s tokens_total=1458597\n",
      "[CHECKPOINT] processed=2400 ents=1449 stmts=2366 evs=6604 elapsed=28064s tokens_total=1459725\n",
      "[CHECKPOINT] processed=2425 ents=1449 stmts=2366 evs=6604 elapsed=28065s tokens_total=1459725\n",
      "[CHECKPOINT] processed=2450 ents=1450 stmts=2368 evs=6606 elapsed=28101s tokens_total=1460118\n",
      "[CHECKPOINT] processed=2475 ents=1450 stmts=2369 evs=6607 elapsed=28119s tokens_total=1460293\n",
      "[CHECKPOINT] processed=2500 ents=1457 stmts=2385 evs=6623 elapsed=28242s tokens_total=1462318\n",
      "[CHECKPOINT] processed=2525 ents=1460 stmts=2389 evs=6628 elapsed=28314s tokens_total=1463963\n",
      "[CHECKPOINT] processed=2550 ents=1460 stmts=2391 evs=6654 elapsed=28607s tokens_total=1480250\n",
      "[CHECKPOINT] processed=2575 ents=1460 stmts=2394 evs=6671 elapsed=28859s tokens_total=1483816\n",
      "[CHECKPOINT] processed=2598 ents=1463 stmts=2410 evs=6701 elapsed=29161s tokens_total=1497720\n",
      "[CHECKPOINT] processed=2600 ents=1463 stmts=2410 evs=6701 elapsed=29194s tokens_total=1501061\n",
      "[CHECKPOINT] processed=2625 ents=1464 stmts=2411 evs=6704 elapsed=29453s tokens_total=1515625\n",
      "[CHECKPOINT] processed=2650 ents=1465 stmts=2414 evs=6722 elapsed=29666s tokens_total=1518976\n",
      "[CHECKPOINT] processed=2675 ents=1467 stmts=2425 evs=6751 elapsed=29921s tokens_total=1523639\n",
      "[CHECKPOINT] processed=2700 ents=1475 stmts=2440 evs=6846 elapsed=30234s tokens_total=1536268\n",
      "[CHECKPOINT] processed=2725 ents=1480 stmts=2450 evs=6881 elapsed=30529s tokens_total=1543693\n",
      "[CHECKPOINT] processed=2750 ents=1480 stmts=2451 evs=6886 elapsed=30767s tokens_total=1555389\n",
      "[CHECKPOINT] processed=2771 ents=1481 stmts=2458 evs=6990 elapsed=31081s tokens_total=1566302\n",
      "[CHECKPOINT] processed=2775 ents=1481 stmts=2458 evs=6990 elapsed=31102s tokens_total=1567455\n",
      "[CHECKPOINT] processed=2800 ents=1484 stmts=2475 evs=7088 elapsed=31382s tokens_total=1575036\n",
      "[CHECKPOINT] processed=2825 ents=1485 stmts=2479 evs=7116 elapsed=31548s tokens_total=1577817\n",
      "[CHECKPOINT] processed=2844 ents=1505 stmts=2534 evs=7236 elapsed=31853s tokens_total=1585840\n",
      "[CHECKPOINT] processed=2850 ents=1510 stmts=2551 evs=7267 elapsed=31922s tokens_total=1587908\n",
      "[CHECKPOINT] processed=2869 ents=1548 stmts=2614 evs=7410 elapsed=32231s tokens_total=1596624\n",
      "[CHECKPOINT] processed=2875 ents=1555 stmts=2626 evs=7447 elapsed=32303s tokens_total=1599309\n",
      "[CHECKPOINT] processed=2896 ents=1595 stmts=2692 evs=7611 elapsed=32610s tokens_total=1609761\n",
      "[CHECKPOINT] processed=2900 ents=1607 stmts=2704 evs=7637 elapsed=32683s tokens_total=1611805\n",
      "[CHECKPOINT] processed=2925 ents=1625 stmts=2761 evs=7758 elapsed=32971s tokens_total=1620790\n",
      "[CHECKPOINT] processed=2950 ents=1625 stmts=2766 evs=7770 elapsed=32989s tokens_total=1621664\n",
      "[CHECKPOINT] processed=2975 ents=1642 stmts=2799 evs=7823 elapsed=33186s tokens_total=1626890\n",
      "[CHECKPOINT] processed=3000 ents=1650 stmts=2809 evs=7833 elapsed=33259s tokens_total=1628470\n",
      "[CHECKPOINT] processed=3025 ents=1654 stmts=2833 evs=7871 elapsed=33368s tokens_total=1631915\n",
      "[CHECKPOINT] processed=3050 ents=1670 stmts=2859 evs=7903 elapsed=33565s tokens_total=1635928\n",
      "[CHECKPOINT] processed=3075 ents=1674 stmts=2870 evs=7914 elapsed=33638s tokens_total=1637309\n",
      "[CHECKPOINT] processed=3100 ents=1677 stmts=2885 evs=7929 elapsed=33743s tokens_total=1639246\n",
      "[CHECKPOINT] processed=3125 ents=1680 stmts=2893 evs=7939 elapsed=33817s tokens_total=1640713\n",
      "[CHECKPOINT] processed=3150 ents=1688 stmts=2911 evs=7969 elapsed=33924s tokens_total=1643281\n",
      "[CHECKPOINT] processed=3175 ents=1694 stmts=2930 evs=7997 elapsed=34069s tokens_total=1646565\n",
      "[CHECKPOINT] processed=3200 ents=1705 stmts=2947 evs=8018 elapsed=34282s tokens_total=1654135\n",
      "[CHECKPOINT] processed=3225 ents=1708 stmts=2952 evs=8024 elapsed=34387s tokens_total=1655364\n",
      "[CHECKPOINT] processed=3250 ents=1708 stmts=2952 evs=8024 elapsed=34388s tokens_total=1655364\n",
      "[CHECKPOINT] processed=3275 ents=1714 stmts=2964 evs=8037 elapsed=34529s tokens_total=1657193\n",
      "[CHECKPOINT] processed=3300 ents=1714 stmts=2966 evs=8039 elapsed=34549s tokens_total=1657487\n",
      "[CHECKPOINT] processed=3325 ents=1723 stmts=2983 evs=8058 elapsed=34723s tokens_total=1660775\n",
      "[CHECKPOINT] processed=3350 ents=1723 stmts=2984 evs=8080 elapsed=34849s tokens_total=1666498\n",
      "[CHECKPOINT] processed=3375 ents=1725 stmts=2987 evs=8089 elapsed=35125s tokens_total=1678253\n",
      "[CHECKPOINT] processed=3400 ents=1727 stmts=2992 evs=8121 elapsed=35402s tokens_total=1683111\n",
      "[CHECKPOINT] processed=3421 ents=1727 stmts=2996 evs=8137 elapsed=35715s tokens_total=1703430\n",
      "[CHECKPOINT] processed=3425 ents=1727 stmts=2996 evs=8137 elapsed=35787s tokens_total=1708740\n",
      "[CHECKPOINT] processed=3450 ents=1727 stmts=2998 evs=8144 elapsed=35967s tokens_total=1714150\n",
      "[CHECKPOINT] processed=3475 ents=1728 stmts=3002 evs=8169 elapsed=36184s tokens_total=1718041\n",
      "[CHECKPOINT] processed=3500 ents=1728 stmts=3013 evs=8213 elapsed=36456s tokens_total=1724950\n",
      "[CHECKPOINT] processed=3520 ents=1732 stmts=3028 evs=8306 elapsed=36765s tokens_total=1736613\n",
      "[CHECKPOINT] processed=3525 ents=1732 stmts=3030 evs=8329 elapsed=36836s tokens_total=1739648\n",
      "[CHECKPOINT] processed=3550 ents=1735 stmts=3036 evs=8351 elapsed=37031s tokens_total=1743339\n",
      "[CHECKPOINT] processed=3575 ents=1736 stmts=3038 evs=8395 elapsed=37283s tokens_total=1757209\n",
      "[CHECKPOINT] processed=3595 ents=1740 stmts=3047 evs=8490 elapsed=37585s tokens_total=1768266\n",
      "[CHECKPOINT] processed=3600 ents=1740 stmts=3050 evs=8536 elapsed=37660s tokens_total=1770350\n",
      "[CHECKPOINT] processed=3625 ents=1740 stmts=3055 evs=8575 elapsed=37856s tokens_total=1774813\n",
      "[CHECKPOINT] processed=3650 ents=1745 stmts=3065 evs=8595 elapsed=38054s tokens_total=1777939\n",
      "[CHECKPOINT] processed=3668 ents=1755 stmts=3093 evs=8627 elapsed=38365s tokens_total=1782797\n",
      "[CHECKPOINT] processed=3675 ents=1757 stmts=3101 evs=8662 elapsed=38454s tokens_total=1785235\n",
      "[CHECKPOINT] processed=3693 ents=1791 stmts=3166 evs=8792 elapsed=38761s tokens_total=1794874\n",
      "[CHECKPOINT] processed=3700 ents=1801 stmts=3184 evs=8823 elapsed=38868s tokens_total=1798460\n",
      "[CHECKPOINT] processed=3719 ents=1826 stmts=3232 evs=8940 elapsed=39171s tokens_total=1808264\n",
      "[CHECKPOINT] processed=3725 ents=1833 stmts=3249 evs=8969 elapsed=39274s tokens_total=1811583\n",
      "[CHECKPOINT] processed=3744 ents=1864 stmts=3309 evs=9107 elapsed=39580s tokens_total=1821143\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mrun_extraction\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43myears\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m2024\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m2025\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_web\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_pdf_tables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_pdf_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_records\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m               \u001b[49m\u001b[38;5;66;43;03m# set None for full run\u001b[39;49;00m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mGENAI_MODEL\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# \"gemini-2.5-flash-lite\" by default\u001b[39;49;00m\n\u001b[0;32m      8\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 883\u001b[0m, in \u001b[0;36mrun_extraction\u001b[1;34m(years, include_web, include_pdf_tables, include_pdf_text, max_records, output_name, model_name)\u001b[0m\n\u001b[0;32m    881\u001b[0m         batch \u001b[38;5;241m=\u001b[39m extractor\u001b[38;5;241m.\u001b[39mextract_table(table_markdown\u001b[38;5;241m=\u001b[39mrec[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmarkdown_table\u001b[39m\u001b[38;5;124m\"\u001b[39m], doc_ctx\u001b[38;5;241m=\u001b[39mdoc_ctx, meta\u001b[38;5;241m=\u001b[39mmeta)\n\u001b[0;32m    882\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 883\u001b[0m         batch \u001b[38;5;241m=\u001b[39m \u001b[43mextractor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdoc_ctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoc_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmeta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    884\u001b[0m     store\u001b[38;5;241m.\u001b[39mmerge_batch(batch)\n\u001b[0;32m    885\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "Cell \u001b[1;32mIn[4], line 515\u001b[0m, in \u001b[0;36mGraphExtractionAgent.extract_text\u001b[1;34m(self, text, doc_ctx, meta)\u001b[0m\n\u001b[0;32m    513\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstats[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequests\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    514\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 515\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_text\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvars_payload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    516\u001b[0m     pm, cm, tt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_record_usage(resp, vars_payload)\n\u001b[0;32m    517\u001b[0m     raw \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_safe_json_loads(resp\u001b[38;5;241m.\u001b[39mcontent)\n",
      "File \u001b[1;32mc:\\Users\\shada\\anaconda3\\envs\\rag_env\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3047\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   3045\u001b[0m                 input_ \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, input_, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   3046\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3047\u001b[0m                 input_ \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, input_, config)\n\u001b[0;32m   3048\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[0;32m   3049\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\shada\\anaconda3\\envs\\rag_env\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:378\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    366\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m    367\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    368\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    373\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    374\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[0;32m    375\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    376\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[0;32m    377\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatGeneration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m--> 378\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    379\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    380\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    381\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    382\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    383\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    384\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    385\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    387\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    388\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[1;32mc:\\Users\\shada\\anaconda3\\envs\\rag_env\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:963\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    954\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m    955\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    956\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    960\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    961\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    962\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 963\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\shada\\anaconda3\\envs\\rag_env\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:782\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    779\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[0;32m    780\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    781\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m--> 782\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    783\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    784\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    785\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    786\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    787\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    788\u001b[0m         )\n\u001b[0;32m    789\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    790\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[1;32mc:\\Users\\shada\\anaconda3\\envs\\rag_env\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1028\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m   1026\u001b[0m     result \u001b[38;5;241m=\u001b[39m generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[0;32m   1027\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1028\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1029\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m   1030\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1031\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1032\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\shada\\anaconda3\\envs\\rag_env\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:961\u001b[0m, in \u001b[0;36mChatGoogleGenerativeAI._generate\u001b[1;34m(self, messages, stop, run_manager, tools, functions, safety_settings, tool_config, generation_config, cached_content, tool_choice, **kwargs)\u001b[0m\n\u001b[0;32m    935\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate\u001b[39m(\n\u001b[0;32m    936\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    937\u001b[0m     messages: List[BaseMessage],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    949\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatResult:\n\u001b[0;32m    950\u001b[0m     request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_request(\n\u001b[0;32m    951\u001b[0m         messages,\n\u001b[0;32m    952\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    959\u001b[0m         tool_choice\u001b[38;5;241m=\u001b[39mtool_choice,\n\u001b[0;32m    960\u001b[0m     )\n\u001b[1;32m--> 961\u001b[0m     response: GenerateContentResponse \u001b[38;5;241m=\u001b[39m \u001b[43m_chat_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    962\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    963\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    964\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    965\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    966\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    967\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _response_to_result(response)\n",
      "File \u001b[1;32mc:\\Users\\shada\\anaconda3\\envs\\rag_env\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:196\u001b[0m, in \u001b[0;36m_chat_with_retry\u001b[1;34m(generation_method, **kwargs)\u001b[0m\n\u001b[0;32m    193\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    194\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m--> 196\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_chat_with_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\shada\\anaconda3\\envs\\rag_env\\Lib\\site-packages\\tenacity\\__init__.py:338\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    336\u001b[0m copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    337\u001b[0m wrapped_f\u001b[38;5;241m.\u001b[39mstatistics \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mstatistics  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m--> 338\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\shada\\anaconda3\\envs\\rag_env\\Lib\\site-packages\\tenacity\\__init__.py:477\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    475\u001b[0m retry_state \u001b[38;5;241m=\u001b[39m RetryCallState(retry_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, fn\u001b[38;5;241m=\u001b[39mfn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m    476\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 477\u001b[0m     do \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    479\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\shada\\anaconda3\\envs\\rag_env\\Lib\\site-packages\\tenacity\\__init__.py:378\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[1;34m(self, retry_state)\u001b[0m\n\u001b[0;32m    376\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_state\u001b[38;5;241m.\u001b[39mactions:\n\u001b[1;32m--> 378\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\shada\\anaconda3\\envs\\rag_env\\Lib\\site-packages\\tenacity\\__init__.py:400\u001b[0m, in \u001b[0;36mBaseRetrying._post_retry_check_actions.<locals>.<lambda>\u001b[1;34m(rs)\u001b[0m\n\u001b[0;32m    398\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_post_retry_check_actions\u001b[39m(\u001b[38;5;28mself\u001b[39m, retry_state: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetryCallState\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    399\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_state\u001b[38;5;241m.\u001b[39mis_explicit_retry \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_state\u001b[38;5;241m.\u001b[39mretry_run_result):\n\u001b[1;32m--> 400\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_action_func(\u001b[38;5;28;01mlambda\u001b[39;00m rs: \u001b[43mrs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutcome\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    401\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    403\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\shada\\anaconda3\\envs\\rag_env\\Lib\\concurrent\\futures\\_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[1;32mc:\\Users\\shada\\anaconda3\\envs\\rag_env\\Lib\\concurrent\\futures\\_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\shada\\anaconda3\\envs\\rag_env\\Lib\\site-packages\\tenacity\\__init__.py:480\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 480\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    481\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[0;32m    482\u001b[0m         retry_state\u001b[38;5;241m.\u001b[39mset_exception(sys\u001b[38;5;241m.\u001b[39mexc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\shada\\anaconda3\\envs\\rag_env\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:178\u001b[0m, in \u001b[0;36m_chat_with_retry.<locals>._chat_with_retry\u001b[1;34m(**kwargs)\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_chat_with_retry\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    177\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 178\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgeneration_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    179\u001b[0m     \u001b[38;5;66;03m# Do not retry for these errors.\u001b[39;00m\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m google\u001b[38;5;241m.\u001b[39mapi_core\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mFailedPrecondition \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[1;32mc:\\Users\\shada\\anaconda3\\envs\\rag_env\\Lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\generative_service\\client.py:835\u001b[0m, in \u001b[0;36mGenerativeServiceClient.generate_content\u001b[1;34m(self, request, model, contents, retry, timeout, metadata)\u001b[0m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_universe_domain()\n\u001b[0;32m    834\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[1;32m--> 835\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    836\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    837\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    838\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    839\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    840\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    842\u001b[0m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[0;32m    843\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\shada\\anaconda3\\envs\\rag_env\\Lib\\site-packages\\google\\api_core\\gapic_v1\\method.py:131\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[1;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    129\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m compression\n\u001b[1;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\shada\\anaconda3\\envs\\rag_env\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:294\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    290\u001b[0m target \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    291\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[0;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[0;32m    293\u001b[0m )\n\u001b[1;32m--> 294\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    299\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    300\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\shada\\anaconda3\\envs\\rag_env\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:147\u001b[0m, in \u001b[0;36mretry_target\u001b[1;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 147\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    148\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misawaitable(result):\n\u001b[0;32m    149\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(_ASYNC_RETRY_WARNING)\n",
      "File \u001b[1;32mc:\\Users\\shada\\anaconda3\\envs\\rag_env\\Lib\\site-packages\\google\\api_core\\timeout.py:130\u001b[0m, in \u001b[0;36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    126\u001b[0m         remaining_timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout\n\u001b[0;32m    128\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m remaining_timeout\n\u001b[1;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\shada\\anaconda3\\envs\\rag_env\\Lib\\site-packages\\google\\api_core\\grpc_helpers.py:76\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(callable_)\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21merror_remapped_callable\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 76\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcallable_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     78\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\shada\\anaconda3\\envs\\rag_env\\Lib\\site-packages\\grpc\\_interceptor.py:277\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.__call__\u001b[1;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[0;32m    268\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[0;32m    269\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    270\u001b[0m     request: Any,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    275\u001b[0m     compression: Optional[grpc\u001b[38;5;241m.\u001b[39mCompression] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    276\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m--> 277\u001b[0m     response, ignored_call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    278\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    279\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    280\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    281\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    282\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwait_for_ready\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    283\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    284\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\shada\\anaconda3\\envs\\rag_env\\Lib\\site-packages\\grpc\\_interceptor.py:329\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable._with_call\u001b[1;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _FailureOutcome(exception, sys\u001b[38;5;241m.\u001b[39mexc_info()[\u001b[38;5;241m2\u001b[39m])\n\u001b[1;32m--> 329\u001b[0m call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interceptor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintercept_unary_unary\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontinuation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclient_call_details\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m call\u001b[38;5;241m.\u001b[39mresult(), call\n",
      "File \u001b[1;32mc:\\Users\\shada\\anaconda3\\envs\\rag_env\\Lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\generative_service\\transports\\grpc.py:79\u001b[0m, in \u001b[0;36m_LoggingClientInterceptor.intercept_unary_unary\u001b[1;34m(self, continuation, client_call_details, request)\u001b[0m\n\u001b[0;32m     64\u001b[0m     grpc_request \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     65\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpayload\u001b[39m\u001b[38;5;124m\"\u001b[39m: request_payload,\n\u001b[0;32m     66\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequestMethod\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrpc\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     67\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(request_metadata),\n\u001b[0;32m     68\u001b[0m     }\n\u001b[0;32m     69\u001b[0m     _LOGGER\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[0;32m     70\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSending request for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclient_call_details\u001b[38;5;241m.\u001b[39mmethod\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     71\u001b[0m         extra\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     76\u001b[0m         },\n\u001b[0;32m     77\u001b[0m     )\n\u001b[1;32m---> 79\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mcontinuation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclient_call_details\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logging_enabled:  \u001b[38;5;66;03m# pragma: NO COVER\u001b[39;00m\n\u001b[0;32m     81\u001b[0m     response_metadata \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mtrailing_metadata()\n",
      "File \u001b[1;32mc:\\Users\\shada\\anaconda3\\envs\\rag_env\\Lib\\site-packages\\grpc\\_interceptor.py:315\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable._with_call.<locals>.continuation\u001b[1;34m(new_details, request)\u001b[0m\n\u001b[0;32m    306\u001b[0m (\n\u001b[0;32m    307\u001b[0m     new_method,\n\u001b[0;32m    308\u001b[0m     new_timeout,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    312\u001b[0m     new_compression,\n\u001b[0;32m    313\u001b[0m ) \u001b[38;5;241m=\u001b[39m _unwrap_client_call_details(new_details, client_call_details)\n\u001b[0;32m    314\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 315\u001b[0m     response, call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_thunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_method\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwith_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_credentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_wait_for_ready\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_compression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _UnaryOutcome(response, call)\n\u001b[0;32m    324\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m rpc_error:\n",
      "File \u001b[1;32mc:\\Users\\shada\\anaconda3\\envs\\rag_env\\Lib\\site-packages\\grpc\\_channel.py:1195\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.with_call\u001b[1;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[0;32m   1183\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwith_call\u001b[39m(\n\u001b[0;32m   1184\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1185\u001b[0m     request: Any,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1190\u001b[0m     compression: Optional[grpc\u001b[38;5;241m.\u001b[39mCompression] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1191\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Any, grpc\u001b[38;5;241m.\u001b[39mCall]:\n\u001b[0;32m   1192\u001b[0m     (\n\u001b[0;32m   1193\u001b[0m         state,\n\u001b[0;32m   1194\u001b[0m         call,\n\u001b[1;32m-> 1195\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_blocking\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompression\u001b[49m\n\u001b[0;32m   1197\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1198\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _end_unary_response_blocking(state, call, \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\shada\\anaconda3\\envs\\rag_env\\Lib\\site-packages\\grpc\\_channel.py:1162\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable._blocking\u001b[1;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[0;32m   1145\u001b[0m state\u001b[38;5;241m.\u001b[39mtarget \u001b[38;5;241m=\u001b[39m _common\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_target)\n\u001b[0;32m   1146\u001b[0m call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_channel\u001b[38;5;241m.\u001b[39msegregated_call(\n\u001b[0;32m   1147\u001b[0m     cygrpc\u001b[38;5;241m.\u001b[39mPropagationConstants\u001b[38;5;241m.\u001b[39mGRPC_PROPAGATE_DEFAULTS,\n\u001b[0;32m   1148\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_method,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1160\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_registered_call_handle,\n\u001b[0;32m   1161\u001b[0m )\n\u001b[1;32m-> 1162\u001b[0m event \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1163\u001b[0m _handle_event(event, state, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_deserializer)\n\u001b[0;32m   1164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m state, call\n",
      "File \u001b[1;32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:388\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc.SegregatedCall.next_event\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:211\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._next_call_event\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:205\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._next_call_event\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi:97\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._latent_event\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi:80\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._internal_latent_event\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi:61\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._next\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run_extraction(\n",
    "    years=[\"2024\", \"2025\"],\n",
    "    include_web=True,\n",
    "    include_pdf_tables=True,\n",
    "    include_pdf_text=True,\n",
    "    max_records=None,               # set None for full run\n",
    "    model_name=GENAI_MODEL        # \"gemini-2.5-flash-lite\" by default\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e70bf5e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INIT] store entities=3156 statements=7292 evidences=61697\n",
      "[CHECKPOINT] processed=25 ents=3156 stmts=7292 evs=61697 elapsed=283s tokens_total=4705\n",
      "[CHECKPOINT] processed=45 ents=3156 stmts=7292 evs=61697 elapsed=586s tokens_total=10732\n",
      "[CHECKPOINT] processed=50 ents=3156 stmts=7292 evs=61697 elapsed=677s tokens_total=13503\n",
      "[CHECKPOINT] processed=69 ents=3156 stmts=7292 evs=61697 elapsed=982s tokens_total=24710\n",
      "[CHECKPOINT] processed=75 ents=3156 stmts=7292 evs=61697 elapsed=1089s tokens_total=28352\n",
      "[CHECKPOINT] processed=94 ents=3156 stmts=7292 evs=61697 elapsed=1401s tokens_total=38124\n",
      "[CHECKPOINT] processed=100 ents=3156 stmts=7292 evs=61697 elapsed=1470s tokens_total=40697\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mrun_extraction\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43myears\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m2025\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_web\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;66;43;03m# <— turn off web crawling\u001b[39;49;00m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_pdf_tables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_pdf_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_records\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mGENAI_MODEL\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 894\u001b[0m, in \u001b[0;36mrun_extraction\u001b[1;34m(years, include_web, include_pdf_tables, include_pdf_text, max_records, output_name, model_name)\u001b[0m\n\u001b[0;32m    892\u001b[0m         batch \u001b[38;5;241m=\u001b[39m extractor\u001b[38;5;241m.\u001b[39mextract_table(table_markdown\u001b[38;5;241m=\u001b[39mrec[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmarkdown_table\u001b[39m\u001b[38;5;124m\"\u001b[39m], doc_ctx\u001b[38;5;241m=\u001b[39mdoc_ctx, meta\u001b[38;5;241m=\u001b[39mmeta)\n\u001b[0;32m    893\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 894\u001b[0m         batch \u001b[38;5;241m=\u001b[39m \u001b[43mextractor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdoc_ctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoc_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmeta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    895\u001b[0m     store\u001b[38;5;241m.\u001b[39mmerge_batch(batch)\n\u001b[0;32m    896\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "Cell \u001b[1;32mIn[1], line 512\u001b[0m, in \u001b[0;36mGraphExtractionAgent.extract_text\u001b[1;34m(self, text, doc_ctx, meta)\u001b[0m\n\u001b[0;32m    510\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose:\n\u001b[0;32m    511\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[RUN] text   doc=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdoc_ctx\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdoc_id\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m chunk=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmeta\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchunk_id\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m page=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmeta\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpage\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m words=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text\u001b[38;5;241m.\u001b[39msplit())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 512\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_enforce_rate_limit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    513\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstats[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequests\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    514\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn[1], line 246\u001b[0m, in \u001b[0;36mGraphExtractionAgent._enforce_rate_limit\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose:\n\u001b[0;32m    245\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[RATE] sleeping \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwait\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms (base+\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjitter\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms jitter)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 246\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(wait)\n\u001b[0;32m    247\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_request_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run_extraction(\n",
    "    years=[\"2025\"],\n",
    "    include_web=False,          # <— turn off web crawling\n",
    "    include_pdf_tables=True,\n",
    "    include_pdf_text=True,\n",
    "    max_records=None,\n",
    "    model_name=GENAI_MODEL\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6805a0",
   "metadata": {},
   "source": [
    "# KG rebuild with LLMGraphTransformer (table-aware, budgeted)\n",
    "\n",
    "This section filters ~50–70 useful web pages by title, chunks to ~9k tokens, extracts triples from text and tables using LLMGraphTransformer, and writes to Neo4j with provenance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa768e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config: limits, env, and paths\n",
    "\n",
    "import os, json, re, math\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from typing import List, Dict, Any, Iterable, Tuple\n",
    "\n",
    "\n",
    "\n",
    "# Limits\n",
    "\n",
    "MAX_PAGES = 70  # cap at 70\n",
    "\n",
    "MIN_PAGES = 50  # try to keep >=50 if available\n",
    "\n",
    "MAX_TOKENS_PER_CHUNK = 9000  # ~9k tokens budget per batch\n",
    "\n",
    "OVERLAP_TOKENS = 400  # conservative overlap for context continuity\n",
    "\n",
    "\n",
    "\n",
    "# Data sources\n",
    "\n",
    "ROOT = Path(r\"i:\\My Drive\\M. Tech AI ML\\AIML SEM 4\\Dissertation\\Project\")\n",
    "\n",
    "WEB_DOCS_JSON = ROOT/\"output\"/\"verizon_production_web_documents.json\"\n",
    "\n",
    "DOC_LING_DIR = ROOT/\"output\"/\"extraction_results\"\n",
    "\n",
    "\n",
    "\n",
    "# Neo4j conn (env-driven)\n",
    "\n",
    "NEO4J_URI = os.getenv(\"NEO4J_URI\", \"bolt://localhost:7687\")\n",
    "\n",
    "NEO4J_USER = os.getenv(\"NEO4J_USER\", \"neo4j\")\n",
    "\n",
    "NEO4J_PASSWORD = os.getenv(\"NEO4J_PASSWORD\", \"password\")\n",
    "\n",
    "NEO4J_DATABASE = os.getenv(\"NEO4J_DATABASE\", \"neo4j\")\n",
    "\n",
    "\n",
    "\n",
    "# Model config\n",
    "\n",
    "GENAI_MODEL = os.getenv(\"GENAI_MODEL\", \"gemini-1.5-pro\")\n",
    "\n",
    "EMBED_MODEL = os.getenv(\"EMBED_MODEL\", \"text-embedding-004\")\n",
    "\n",
    "\n",
    "\n",
    "# Basic token estimator using tiktoken-like fallback\n",
    "\n",
    "def rough_token_count(text: str) -> int:\n",
    "\n",
    "    if not text: return 0\n",
    "\n",
    "    # heuristic: ~4 chars/token, clamp\n",
    "\n",
    "    return max(1, math.ceil(len(text) / 4))\n",
    "\n",
    "\n",
    "\n",
    "def select_useful_pages(docs: List[Dict[str, Any]],\n",
    "\n",
    "                        keep_terms=(\"investor\",\"financial\",\"quarter\",\"annual\",\"10-k\",\"10-q\",\"earnings\",\"webcast\",\"presentation\",\"report\",\"capex\",\"opex\",\"revenue\",\"cash flow\",\"guidance\",\"forecast\",\"board\",\"governance\")) -> List[Dict[str, Any]]:\n",
    "\n",
    "    \"\"\"Score by title heuristics; prefer investor/financial content and long-form pages.\"\"\"\n",
    "\n",
    "    scored = []\n",
    "\n",
    "    for d in docs:\n",
    "\n",
    "        url = d.get(\"url\",\"\")\n",
    "\n",
    "        content = d.get(\"content\",\"\") or \"\"\n",
    "\n",
    "        ttl = url.split(\"/\")[-1].replace(\"-\",\" \").lower()\n",
    "\n",
    "        score = 0\n",
    "\n",
    "        # title and path hints\n",
    "\n",
    "        for t in keep_terms:\n",
    "\n",
    "            if t in ttl:\n",
    "\n",
    "                score += 8\n",
    "\n",
    "        if any(seg in url for seg in [\"/investors/\",\"quarterly-reports\",\"financial\",\"earnings\",\"conference\",\"webcast\",\"annual\"]):\n",
    "\n",
    "            score += 10\n",
    "\n",
    "        # content length as a proxy for substance\n",
    "\n",
    "        score += min(10, len(content)//2000)\n",
    "\n",
    "        # de-prioritize T&C, privacy, generic parenting pages\n",
    "\n",
    "        if any(x in url for x in [\"terms-conditions\",\"privacy\",\"parenting\",\"international/privacy\",\"our-company/our-culture\"]):\n",
    "\n",
    "            score -= 12\n",
    "\n",
    "        scored.append((score, d))\n",
    "\n",
    "    scored.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "    top = [d for _, d in scored if _ > 0]\n",
    "\n",
    "    return top[:MAX_PAGES]\n",
    "\n",
    "\n",
    "\n",
    "def chunk_text_to_budget(text: str, max_tokens=MAX_TOKENS_PER_CHUNK, overlap=OVERLAP_TOKENS) -> List[str]:\n",
    "\n",
    "    if not text: return []\n",
    "\n",
    "    tokens = rough_token_count(text)\n",
    "\n",
    "    if tokens <= max_tokens:\n",
    "\n",
    "        return [text]\n",
    "\n",
    "    # naive sliding window by character proportional to tokens\n",
    "\n",
    "    chars_per_token = max(1, len(text)//tokens)\n",
    "\n",
    "    max_chars = max_tokens * chars_per_token\n",
    "\n",
    "    overlap_chars = overlap * chars_per_token\n",
    "\n",
    "    chunks = []\n",
    "\n",
    "    start = 0\n",
    "\n",
    "    while start < len(text):\n",
    "\n",
    "        end = min(len(text), start + max_chars)\n",
    "\n",
    "        chunk = text[start:end]\n",
    "\n",
    "        chunks.append(chunk)\n",
    "\n",
    "        if end >= len(text): break\n",
    "\n",
    "        start = end - overlap_chars\n",
    "\n",
    "        if start < 0: start = 0\n",
    "\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3fbccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load web documents and select 50–70 useful pages by title/URL heuristics\n",
    "\n",
    "with open(WEB_DOCS_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "\n",
    "    web_blob = json.load(f)\n",
    "\n",
    "docs = web_blob.get(\"documents\", [])\n",
    "\n",
    "print(f\"Total web docs: {len(docs)}\")\n",
    "\n",
    "selected = select_useful_pages(docs)\n",
    "\n",
    "if len(selected) < MIN_PAGES:\n",
    "\n",
    "    # fallback: relax filters and take top MIN_PAGES by content length\n",
    "\n",
    "    docs_sorted = sorted(docs, key=lambda d: len(d.get(\"content\",\"\")), reverse=True)\n",
    "\n",
    "    selected = docs_sorted[:MIN_PAGES]\n",
    "\n",
    "print(f\"Selected pages: {len(selected)} (target 50–70)\")\n",
    "\n",
    "for i, d in enumerate(selected[:5], 1):\n",
    "\n",
    "    print(i, d.get(\"url\",\"\"), len(d.get(\"content\",\"\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129fae7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract triples from text and tables\n",
    "\n",
    "from langchain_community.graphs.graph_document import GraphDocument\n",
    "\n",
    "from langchain_community.graphs.neo4j_graph import Neo4jGraph\n",
    "\n",
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "from docling.document_converter import DocumentConverter\n",
    "\n",
    "from docling.models.document import Table as DLTable\n",
    "\n",
    "\n",
    "\n",
    "# Initialize LLM and graph\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=GENAI_MODEL, temperature=0.2)\n",
    "\n",
    "graph = Neo4jGraph(url=NEO4J_URI, username=NEO4J_USER, password=NEO4J_PASSWORD)\n",
    "\n",
    "\n",
    "\n",
    "# Optional: rebuild graph (wipe only Verizon domain) – comment out if you want to append\n",
    "\n",
    "REBUILD = True\n",
    "\n",
    "if REBUILD:\n",
    "\n",
    "    with graph._driver.session(database=NEO4J_DATABASE) as s:\n",
    "\n",
    "        s.run(\"MATCH (n) DETACH DELETE n\")\n",
    "\n",
    "        print(\"Neo4j cleared.\")\n",
    "\n",
    "\n",
    "\n",
    "# Transformer configured to be table-aware by feeding serialized tables as text prompts per chunk\n",
    "\n",
    "kg_transformer = LLMGraphTransformer(llm=llm)\n",
    "\n",
    "\n",
    "\n",
    "def serialize_table(tbl: DLTable) -> str:\n",
    "\n",
    "    # produce a simple markdown-like table string\n",
    "\n",
    "    try:\n",
    "\n",
    "        headers = [c.text or \"\" for c in (tbl.columns or [])]\n",
    "\n",
    "        rows = []\n",
    "\n",
    "        for r in (tbl.rows or []):\n",
    "\n",
    "            rows.append([c.text or \"\" for c in r.cells])\n",
    "\n",
    "        lines = []\n",
    "\n",
    "        if headers:\n",
    "\n",
    "            lines.append(\" | \".join(headers))\n",
    "\n",
    "            lines.append(\" | \".join([\"---\"]*len(headers)))\n",
    "\n",
    "        for r in rows:\n",
    "\n",
    "            lines.append(\" | \".join(r))\n",
    "\n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "    except Exception:\n",
    "\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "\n",
    "def iter_pages(selected_docs: List[Dict[str,Any]]):\n",
    "\n",
    "    for d in selected_docs:\n",
    "\n",
    "        yield d.get(\"url\",\"\"), d.get(\"content\",\"\")\n",
    "\n",
    "\n",
    "\n",
    "converter = DocumentConverter()\n",
    "\n",
    "\n",
    "\n",
    "all_graph_docs: List[GraphDocument] = []\n",
    "\n",
    "batch_tokens = 0\n",
    "\n",
    "batch_chunks: List[Tuple[str,str]] = []  # (url, chunk_text)\n",
    "\n",
    "\n",
    "\n",
    "def flush_batch(batch: List[Tuple[str,str]]):\n",
    "\n",
    "    global all_graph_docs\n",
    "\n",
    "    if not batch: return\n",
    "\n",
    "    inputs = []\n",
    "\n",
    "    for url, text in batch:\n",
    "\n",
    "        # Try to parse tables if file exists in docling results (optional)\n",
    "\n",
    "        # For web HTML, we already have plain text; we attach as-is\n",
    "\n",
    "        inputs.append(text)\n",
    "\n",
    "    gdocs = kg_transformer.transform(texts=inputs)\n",
    "\n",
    "    # Attach provenance (source URL) into GraphDocument metadata\n",
    "\n",
    "    for (url, _), gd in zip(batch, gdocs):\n",
    "\n",
    "        if not hasattr(gd, \"metadata\") or gd.metadata is None:\n",
    "\n",
    "            gd.metadata = {}\n",
    "\n",
    "        gd.metadata[\"source_url\"] = url\n",
    "\n",
    "    all_graph_docs.extend(gdocs)\n",
    "\n",
    "\n",
    "\n",
    "for url, content in iter_pages(selected):\n",
    "\n",
    "    # chunk by token budget\n",
    "\n",
    "    chunks = chunk_text_to_budget(content, MAX_TOKENS_PER_CHUNK, OVERLAP_TOKENS)\n",
    "\n",
    "    for ch in chunks:\n",
    "\n",
    "        tks = rough_token_count(ch)\n",
    "\n",
    "        if batch_tokens + tks > MAX_TOKENS_PER_CHUNK and batch_chunks:\n",
    "\n",
    "            flush_batch(batch_chunks)\n",
    "\n",
    "            batch_chunks = []\n",
    "\n",
    "            batch_tokens = 0\n",
    "\n",
    "        batch_chunks.append((url, ch))\n",
    "\n",
    "        batch_tokens += tks\n",
    "\n",
    "\n",
    "\n",
    "# flush remaining\n",
    "\n",
    "flush_batch(batch_chunks)\n",
    "\n",
    "print(f\"Graph docs extracted: {len(all_graph_docs)}\")\n",
    "\n",
    "\n",
    "\n",
    "# Write to Neo4j with constraints\n",
    "\n",
    "graph.add_graph_documents(all_graph_docs, baseEntityLabel=True, include_source=True)\n",
    "\n",
    "print(\"Write complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e96b52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick validation: counts and sample\n",
    "\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "\n",
    "\n",
    "driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))\n",
    "\n",
    "with driver.session(database=NEO4J_DATABASE) as s:\n",
    "\n",
    "    c = s.run(\"MATCH (n) RETURN count(n) AS c\").single()[\"c\"]\n",
    "\n",
    "    r = s.run(\"MATCH ()-[r]->() RETURN count(r) AS c\").single()[\"c\"]\n",
    "\n",
    "    print(\"Nodes:\", c, \"Relationships:\", r)\n",
    "\n",
    "    ex = s.run(\"MATCH (a)-[r]->(b) RETURN a,r,b LIMIT 3\").data()\n",
    "\n",
    "    for i, row in enumerate(ex, 1):\n",
    "\n",
    "        a = row['a'].get('name') or row['a'].get('id') or row['a'].get('label')\n",
    "\n",
    "        b = row['b'].get('name') or row['b'].get('id') or row['b'].get('label')\n",
    "\n",
    "        print(i, row['r'].type, '::', a, '->', b)\n",
    "\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fd2c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Ingest tables from Docling JSON outputs (PDF parsing)\n",
    "\n",
    "import glob\n",
    "\n",
    "\n",
    "\n",
    "def load_docling_tables(docling_path: str) -> List[Tuple[str, str]]:\n",
    "\n",
    "    \"\"\"Return list of (provenance, serialized_table_text).\"\"\"\n",
    "\n",
    "    with open(docling_path, \"r\", encoding=\"utf-8\") as f:\n",
    "\n",
    "        blob = json.load(f)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # docling JSON schema: pages -> blocks (type: table) OR content.tables depending on version\n",
    "\n",
    "    tables = []\n",
    "\n",
    "    if isinstance(blob, dict):\n",
    "\n",
    "        # newer schema\n",
    "\n",
    "        if \"tables\" in blob:\n",
    "\n",
    "            tables = blob[\"tables\"]\n",
    "\n",
    "        elif \"pages\" in blob:\n",
    "\n",
    "            for p in blob[\"pages\"]:\n",
    "\n",
    "                for b in p.get(\"blocks\", []):\n",
    "\n",
    "                    if b.get(\"type\") == \"table\":\n",
    "\n",
    "                        tables.append(b)\n",
    "\n",
    "    for idx, t in enumerate(tables):\n",
    "\n",
    "        # Attempt to read cells\n",
    "\n",
    "        md = []\n",
    "\n",
    "        headers = [c.get(\"text\",\"\") for c in t.get(\"header\", [])] if isinstance(t.get(\"header\"), list) else []\n",
    "\n",
    "        if headers:\n",
    "\n",
    "            md.append(\" | \".join(headers))\n",
    "\n",
    "            md.append(\" | \".join([\"---\"]*len(headers)))\n",
    "\n",
    "        for row in t.get(\"rows\", []):\n",
    "\n",
    "            cells = [c.get(\"text\",\"\") for c in row]\n",
    "\n",
    "            md.append(\" | \".join(cells))\n",
    "\n",
    "        if not md and \"markdown\" in t:\n",
    "\n",
    "            md = [t[\"markdown\"]]\n",
    "\n",
    "        txt = \"\\n\".join(md)\n",
    "\n",
    "        if txt.strip():\n",
    "\n",
    "            prov = f\"docling:{Path(docling_path).name}#table{idx}\"\n",
    "\n",
    "            results.append((prov, txt))\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "# Collect docling files\n",
    "\n",
    "docling_files = glob.glob(str((ROOT/\"output\").joinpath(\"*.docling.json\")))\n",
    "\n",
    "print(\"Docling JSON files:\", len(docling_files))\n",
    "\n",
    "\n",
    "\n",
    "table_graph_docs: List[GraphDocument] = []\n",
    "\n",
    "batch_tokens = 0\n",
    "\n",
    "batch_chunks: List[Tuple[str,str]] = []\n",
    "\n",
    "\n",
    "\n",
    "def flush_table_batch(batch: List[Tuple[str,str]]):\n",
    "\n",
    "    global table_graph_docs\n",
    "\n",
    "    if not batch: return\n",
    "\n",
    "    texts = [t for _, t in batch]\n",
    "\n",
    "    gdocs = kg_transformer.transform(texts=texts)\n",
    "\n",
    "    for (prov, _), gd in zip(batch, gdocs):\n",
    "\n",
    "        if not hasattr(gd, \"metadata\") or gd.metadata is None:\n",
    "\n",
    "            gd.metadata = {}\n",
    "\n",
    "        gd.metadata[\"source_url\"] = prov\n",
    "\n",
    "        gd.metadata[\"source_type\"] = \"table\"\n",
    "\n",
    "    table_graph_docs.extend(gdocs)\n",
    "\n",
    "\n",
    "\n",
    "for fp in docling_files:\n",
    "\n",
    "    pairs = load_docling_tables(fp)\n",
    "\n",
    "    for prov, txt in pairs:\n",
    "\n",
    "        tks = rough_token_count(txt)\n",
    "\n",
    "        if batch_tokens + tks > MAX_TOKENS_PER_CHUNK and batch_chunks:\n",
    "\n",
    "            flush_table_batch(batch_chunks)\n",
    "\n",
    "            batch_chunks = []\n",
    "\n",
    "            batch_tokens = 0\n",
    "\n",
    "        batch_chunks.append((prov, txt))\n",
    "\n",
    "        batch_tokens += tks\n",
    "\n",
    "\n",
    "\n",
    "flush_table_batch(batch_chunks)\n",
    "\n",
    "print(f\"Table graph docs extracted: {len(table_graph_docs)}\")\n",
    "\n",
    "\n",
    "\n",
    "if table_graph_docs:\n",
    "\n",
    "    graph.add_graph_documents(table_graph_docs, baseEntityLabel=True, include_source=True)\n",
    "\n",
    "    print(\"Table triples written to Neo4j.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
